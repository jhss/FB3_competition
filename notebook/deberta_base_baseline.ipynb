{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Reference: https://www.kaggle.com/code/kojimar/fb3-single-pytorch-model-train"
      ],
      "metadata": {
        "id": "ujOI-zli8Rlv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lM0CH5Tr_QKM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497c5e58-cc12-4851-fc1b-d2515ed46ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "/content/drive/MyDrive/kaggle/notebook\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd drive/MyDrive/kaggle/notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VvALF3IfS1Ee"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers==4.20.1\n",
        "!pip install tokenizers==0.12.1\n",
        "!pip install sentencepiece==0.1.97"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WhmJrRr1_MvV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AutoModel, AutoTokenizer, AutoConfig\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "os.system('pip install iterative-stratification==0.1.7')\n",
        "os.environ['TOKENIZERS_PARALLELISM']='true'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uXo99OVW_Mvb"
      },
      "outputs": [],
      "source": [
        "class CFG:\n",
        "    model_name  = \"microsoft/deberta-v3-base\"\n",
        "    \n",
        "    base        = \"../input/feedback-prize-english-language-learning/\"\n",
        "    train       = base + \"train.csv\"\n",
        "    test        = base + \"test.csv\"\n",
        "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    num_targets = 6\n",
        "    \n",
        "    # Backbone\n",
        "    max_length  = 512\n",
        "    hidden_dims = 768\n",
        "    output_hidden_states = True\n",
        "    pool = 'mean'\n",
        "    reinit   = True\n",
        "    reinit_n = 1\n",
        "    \n",
        "    # loss\n",
        "    loss = 'L1smooth'\n",
        "    apex = True\n",
        "    max_norm = 1000\n",
        "    \n",
        "    # optimizer\n",
        "    default_lr   = 1e-5\n",
        "    head_lr      = 1e-4\n",
        "    adam_eps     = 1e-6\n",
        "    lr_decay     = 0.9\n",
        "    weight_decay = 0.01\n",
        "    \n",
        "    # scheduler\n",
        "    scheduler = 'lambda'\n",
        "    lr_sch_decay_steps = 1600\n",
        "    num_warmup_steps = 0.0\n",
        "    num_cycles = 0.5\n",
        "    \n",
        "    # CV\n",
        "    n_folds = 5\n",
        "    seed   = 42\n",
        "    epochs = 5\n",
        "    \n",
        "    # Loader\n",
        "    batch_size  = 4\n",
        "    num_workers = 2\n",
        "    print_freq = 20\n",
        "\n",
        "    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "82eXqEicr0r6"
      },
      "outputs": [],
      "source": [
        "if len(CFG.model_name.split(\"/\")) == 2:\n",
        "   CFG.identifier = f\"{CFG.str_now}-{CFG.model_name.split('/')[1]}\"\n",
        "else:\n",
        "   CFG.identifier = f\"{CFG.str_now}-{CFG.model_name}\"\n",
        "\n",
        "CFG.OUTPUT_DIR = f'./{CFG.identifier}/'\n",
        "CFG.log_file   = CFG.OUTPUT_DIR + 'train'\n",
        "os.makedirs(CFG.OUTPUT_DIR, exist_ok = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "EBJTWKqf_Mvc"
      },
      "outputs": [],
      "source": [
        "df_train = pd.read_csv(CFG.train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2bNvgiep_Mve"
      },
      "outputs": [],
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fEyYoSId_Mvh"
      },
      "outputs": [],
      "source": [
        "fold = MultilabelStratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "df   = df_train.copy()\n",
        "y    = pd.get_dummies(data = df[CFG.target_cols], columns = CFG.target_cols)\n",
        "\n",
        "for idx, (train_idx, valid_idx) in enumerate(fold.split(df_train, y)):\n",
        "    df_train.loc[valid_idx, 'fold'] = idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vP0hQ3g3_Mvj"
      },
      "outputs": [],
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "             \n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        \n",
        "    def update(self, val, n = 1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{int(m)}m {int(s)}s'\n",
        "        \n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    \n",
        "    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})'\n",
        "        \n",
        "def prepare_inputs(texts):\n",
        "    inputs = []\n",
        "    for text in texts:\n",
        "        tokens = tokenizer.encode_plus(text,\n",
        "                                       add_special_tokens = True,\n",
        "                                       max_length = CFG.max_length,\n",
        "                                       pad_to_max_length = True,\n",
        "                                       truncation = True,\n",
        "                                       return_attention_mask = True\n",
        "                                       )\n",
        "        \n",
        "        for key, vals in tokens.items():\n",
        "            tokens[key] = torch.tensor(vals, dtype = torch.long)\n",
        "        inputs.append(tokens)\n",
        "    \n",
        "    return inputs\n",
        "\n",
        "class MeanPooling(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(MeanPooling, self).__init__()\n",
        "\n",
        "      def forward(self, inputs, attention_masks):\n",
        "          #print(\"inputs.shape: \", inputs.shape, \" masks.shape: \", attention_masks.shape)\n",
        "          attention_masks = attention_masks.unsqueeze(-1).float()\n",
        "          inputs   = torch.sum(inputs*attention_masks, dim = 1)\n",
        "          num_masks = torch.sum(attention_masks == 1.0, dim = 1)\n",
        "          torch.clamp(num_masks, min = 1e-9)\n",
        "          inputs   = (inputs / num_masks)\n",
        "\n",
        "          return inputs\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "      def __init__(self, hidden_size):\n",
        "          super(AttentionPooling, self).__init__()\n",
        "          self.attention = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
        "                                         nn.GELU(),\n",
        "                                         nn.Linear(hidden_size, 1)\n",
        "                                         )\n",
        "\n",
        "      def forward(self, hidden_state, attention_mask):\n",
        "          weights = self.attention(hidden_state)\n",
        "          weights[attention_mask == 0] = float(\"-inf\")\n",
        "          weights = torch.softmax(weights, dim = 1)\n",
        "          context = torch.sum(hidden_state * weights, dim = 1)\n",
        "\n",
        "          return context\n",
        "\n",
        "def MCRMSE(labels, preds):\n",
        "    scores = []\n",
        "    num_targets = labels.shape[1]\n",
        "    \n",
        "    for i in range(num_targets):\n",
        "        pred  = preds[:,i]\n",
        "        label = labels[:,i]\n",
        "        score = mean_squared_error(label, pred, squared = False)\n",
        "        scores.append(score)\n",
        "    \n",
        "    mcrmse = np.mean(scores)\n",
        "    \n",
        "    return mcrmse, scores \n",
        "\n",
        "def get_score(labels, preds):\n",
        "    return MCRMSE(labels, preds)\n",
        "\n",
        "def get_logger(log_file = CFG.log_file):\n",
        "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
        "    logger = getLogger(__name__)\n",
        "    logger.setLevel(INFO)\n",
        "    handler1 = StreamHandler()\n",
        "    handler1.setFormatter(Formatter('%(message)s'))\n",
        "    handler2 = FileHandler(filename = f'{log_file}.log')\n",
        "    handler2.setFormatter(Formatter('%(message)s'))\n",
        "    logger.addHandler(handler1)\n",
        "    logger.addHandler(handler2)\n",
        "    logger.propagate = False\n",
        "    \n",
        "    return logger\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "32saDLj6bVrW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81577936-7b5e-4015-e0fa-0c561a806c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "OUTPUT_DIR: ./20221213-000822-deberta-v3-base/\n"
          ]
        }
      ],
      "source": [
        "LOGGER = get_logger()\n",
        "LOGGER.info(f'OUTPUT_DIR: {CFG.OUTPUT_DIR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tMKFr56B_Mvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383376a1-81b9-49d0-d098-31f8bba3620c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "max_len: 1428\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
        "#tokenizer.add_tokens([\"\\n\"], special_tokens=True)\n",
        "\n",
        "CFG.max_len = 1429\n",
        "LOGGER.info(f'max_len: {CFG.max_len}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1bMw-Mip_Mvl"
      },
      "outputs": [],
      "source": [
        "class FB3Dataset(Dataset):\n",
        "    def __init__(self, df_train):\n",
        "        self.inputs = prepare_inputs(df_train['full_text'])\n",
        "        self.labels = torch.tensor(df_train[CFG.target_cols].values, dtype = torch.float)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "lvyMXacu_Mvn"
      },
      "outputs": [],
      "source": [
        "class FB3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.config     = AutoConfig.from_pretrained(cfg.model_name, output_hidden_states = cfg.output_hidden_states)\n",
        "        self.config.hidden_dropout = 0.\n",
        "        self.config.hidden_dropout_prob = 0.\n",
        "        self.config.attention_dropout = 0.\n",
        "        self.config.attention_probs_dropout_prob = 0.\n",
        "\n",
        "        self.model      = AutoModel.from_pretrained(cfg.model_name, \n",
        "                                                    config = self.config)\n",
        "        self.cfg        = cfg\n",
        "\n",
        "        LOGGER.info(self.cfg)\n",
        "\n",
        "        self.pool       = MeanPooling()\n",
        "        self.classifier = nn.Linear(cfg.hidden_dims, cfg.num_targets)\n",
        "        self._init_weights(self.classifier)\n",
        "\n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "        feature = self.pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        " \n",
        "        return feature\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        module.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "\n",
        "        if module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    def reinitialize_layer(self, layer_num):\n",
        "        for layer in self.model.encoder.layer[-layer_num:].modules():\n",
        "            if isinstance(layer, nn.Linear):\n",
        "               layer.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "\n",
        "               if layer.bias is not None:\n",
        "                  layer.bias.data.zero_()\n",
        "\n",
        "            elif isinstance(layer, nn.Embedding):\n",
        "               layer.weight.data.normal_(mean = 0.0, std = 0.02)\n",
        "\n",
        "               if layer.padding_idx is not None:\n",
        "                  layer.weight.data[layer.padding_idx].zero_()\n",
        "\n",
        "            elif isinstance(layer, nn.LayerNorm):\n",
        "                layer.bias.data.zero_()\n",
        "                layer.weight.data.fill_(1.0)\n",
        "          \n",
        "    def forward(self, inputs):\n",
        "\n",
        "        feature = self.feature(inputs)\n",
        "        outputs = self.classifier(feature)\n",
        "        \n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qElgZ6d-_Mvo"
      },
      "outputs": [],
      "source": [
        "def train_fn(model, criterion, optimizer, scheduler, train_loader, epoch, cfg):\n",
        "    losses = AverageMeter()\n",
        "    total_steps = len(train_loader)\n",
        "    start = end = time.time()\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.apex)\n",
        "    \n",
        "    for step, (inputs, labels) in enumerate(train_loader):\n",
        "        #print(\"inputs.shape: \", inputs['input_ids'].shape)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled = cfg.apex):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "        losses.update(loss.item(), labels.shape[0])\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_norm)\n",
        "          \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "        \n",
        "        end = time.time()\n",
        "        \n",
        "        if (step + 1) % cfg.print_freq == 0:\n",
        "            print(\"[{0}][{1}/{2}] \"\n",
        "                  \"Elapsed: {remain:s} \"\n",
        "                  \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
        "                  \"Grad: {grad_norm} \"\n",
        "                  \"LR: {lr:.8f} \"\n",
        "                  .format(epoch, step, total_steps, remain = timeSince(start, float(step+1)/total_steps),\n",
        "                          loss = losses,\n",
        "                          grad_norm = grad_norm,\n",
        "                          lr = scheduler.get_lr()[0]\n",
        "                          )\n",
        "                 )\n",
        "    \n",
        "    return losses.avg\n",
        "\n",
        "def valid_fn(model, criterion, valid_loader, epoch, cfg):\n",
        "    losses = AverageMeter()\n",
        "    predictions = []\n",
        "    total_steps = len(valid_loader) - 1\n",
        "    model.eval()\n",
        "    start = end = time.time()\n",
        "    \n",
        "    for step, (inputs, labels) in enumerate(valid_loader):\n",
        "        \n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = model(inputs)\n",
        "        \n",
        "        loss = criterion(preds, labels)\n",
        "        losses.update(loss.item(), labels.shape[0])\n",
        "        \n",
        "        predictions.append(preds.detach().cpu().numpy())\n",
        "        \n",
        "        if (step + 1) % cfg.print_freq == 0 or step == total_steps:\n",
        "            print(\"[{0}][{1}/{2}] \"\n",
        "                  \"Elapsed: {remain:s} \"\n",
        "                  \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
        "                  .format(epoch+1, step, total_steps, remain = timeSince(start, float(step+1)/total_steps),\n",
        "                          loss = losses,\n",
        "                          )\n",
        "                 )\n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis = 0)\n",
        "    \n",
        "    return losses.avg, predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WtvABLSm_Mvp"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def train_loop(folds, fold, cfg):\n",
        "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
        "    train_folds  = folds[folds['fold'] != fold]\n",
        "    valid_folds  = folds[folds['fold'] == fold]\n",
        "    valid_labels = valid_folds[cfg.target_cols].values\n",
        "    \n",
        "    train_dataset = FB3Dataset(train_folds)\n",
        "    valid_dataset = FB3Dataset(valid_folds)\n",
        "    \n",
        "    train_loader  = DataLoader(train_dataset, \n",
        "                               batch_size  = cfg.batch_size,\n",
        "                               shuffle = True,\n",
        "                               pin_memory = True,\n",
        "                               num_workers = cfg.num_workers)\n",
        "    \n",
        "    valid_loader  = DataLoader(valid_dataset, \n",
        "                               batch_size = cfg.batch_size,\n",
        "                               shuffle = False,\n",
        "                               pin_memory = True,\n",
        "                               num_workers = cfg.num_workers)\n",
        "    \n",
        "    def get_optimizer(cfg, model):\n",
        "        \n",
        "        all_parameters = list(model.named_parameters())\n",
        "        used_name_parameters = set()\n",
        "        params = []\n",
        "        no_wd = ['word_embeddings', 'bias', 'LayerNorm.weight']\n",
        "\n",
        "        head   = [(n, p) for n, p in model.named_parameters() if 'model' not in n]\n",
        "        for n, _ in head: used_name_parameters.add(n)\n",
        "  \n",
        "        params.append({\"params\": [p for n, p in head if not any(no in n for no in no_wd)], \"weight_decay\": 0.01, \"lr\": cfg.head_lr})\n",
        "        params.append({\"params\": [p for n, p in head if any(no in n for no in no_wd)], \"weight_decay\": 0.0, \"lr\": cfg.head_lr})\n",
        "        \n",
        "        lr = cfg.default_lr\n",
        "        decay = 0.9\n",
        "\n",
        "        layer_list = [model.model.embeddings] + list(model.model.encoder.layer)\n",
        "        layer_list.reverse()\n",
        "\n",
        "        params.append(\n",
        "            {\n",
        "                \"params\": list(model.model.encoder.rel_embeddings.parameters()),\n",
        "                \"lr\": lr,\n",
        "                \"weight_decay\": 0.0\n",
        "            }\n",
        "        )\n",
        "\n",
        "        params.append(\n",
        "            {\n",
        "                \"params\": list(model.model.encoder.LayerNorm.parameters()),\n",
        "                \"lr\": lr,\n",
        "                \"weight_decay\": 0.0\n",
        "            }\n",
        "        )\n",
        "      \n",
        "        for i, layer in enumerate(layer_list):\n",
        "            \n",
        "            parameters = [(n, p) for n, p in layer.named_parameters()]\n",
        "            for n, _ in parameters: used_name_parameters.add(n)\n",
        "\n",
        "            params.append({\"params\": [p for n, p in parameters if not any(no in n for no in no_wd)], 'weight_decay': 0.01, \"lr\": lr})\n",
        "            params.append({\"params\": [p for n, p in parameters if any(no in n for no in no_wd)], 'weight_decay': 0.0, \"lr\": lr})\n",
        "\n",
        "            lr = lr * decay\n",
        "\n",
        "        optimizer = torch.optim.AdamW(params, eps = cfg.adam_eps)\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
        "        warmup_start = int(cfg.epochs * num_train_steps)\n",
        "        print(\"warmup start: \", warmup_start)\n",
        "\n",
        "        def decay_func(step):\n",
        "            return 0.3 ** (float(step) / float(cfg.lr_sch_decay_steps))\n",
        "\n",
        "        if cfg.scheduler == 'linear':\n",
        "           scheduler = transformers.get_scheduler(cfg.scheduler, optimizer, warmup_start, num_train_steps)\n",
        "        elif cfg.scheduler == 'cosine':\n",
        "           scheduler = get_cosine_schedule_with_warmup(\n",
        "               optimizer,\n",
        "               num_warmup_steps = cfg.num_warmup_steps,\n",
        "               num_training_steps = num_train_steps,\n",
        "               num_cycles = cfg.num_cycles\n",
        "           )\n",
        "        elif cfg.scheduler == 'lambda':\n",
        "           scheduler = LambdaLR(optimizer, lr_lambda = decay_func)\n",
        "\n",
        "        return scheduler\n",
        "    \n",
        "    model = FB3Model(cfg).to(device)\n",
        "    if CFG.reinit:\n",
        "       model.reinitialize_layer(CFG.reinit_n)\n",
        "    criterion = nn.SmoothL1Loss(reduction = 'mean')\n",
        "    optimizer = get_optimizer(cfg, model)\n",
        "    num_train_steps = int(len(train_folds) / cfg.batch_size * cfg.epochs)\n",
        "    print(\"[DEBUG] num_train_steps: \", num_train_steps, \" len train loader: \", len(train_loader))\n",
        "    scheduler = get_scheduler(cfg, optimizer, num_train_steps)\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_val_loss = np.inf\n",
        "    best_train_loss = np.inf\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        \n",
        "        start = time.time()\n",
        "        \n",
        "        avg_loss = train_fn(model, criterion, optimizer, scheduler, train_loader, epoch, cfg)\n",
        "        \n",
        "        avg_val_loss, preds = valid_fn(model, criterion, valid_loader, epoch, cfg)\n",
        "        score, scores = get_score(preds, valid_labels)\n",
        "        \n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        LOGGER.info(f\"Epoch: {epoch+1} - avg_train_loss = {avg_loss:.4f} avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.4f}s\")\n",
        "        LOGGER.info(f\"Epoch: {epoch+1} - Score: {score:.4f}, Scores: {scores}\")\n",
        "\n",
        "        if best_score > score:\n",
        "           best_score = score\n",
        "           best_train_loss = avg_loss\n",
        "           bset_val_loss = avg_val_loss\n",
        "           LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n",
        "\n",
        "           torch.save({'model': model.state_dict(),\n",
        "                       'predictions': preds},\n",
        "                      CFG.OUTPUT_DIR + f\"{CFG.model_name.replace('/', '-')}_fold{fold}_best.pth\")\n",
        "    \n",
        "    predictions = torch.load(CFG.OUTPUT_DIR + f\"{CFG.model_name.replace('/', '-')}_fold{fold}_best.pth\", \n",
        "                             map_location = torch.device('cpu'))['predictions']\n",
        "    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return best_train_loss, best_val_loss, valid_folds, best_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9jx_34Ji_Mvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b0aa823-5127-4053-ca63-b13e6e2126df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "========== fold: 0 training ==========\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<class '__main__.CFG'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] num_train_steps:  3912  len train loader:  783\n",
            "warmup start:  19560\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/783] Elapsed: 0m 7s (remain 4m 53s) Loss: 0.3765(1.5168) Grad: 246857.6875 LR: 0.00009851 \n",
            "[0][39/783] Elapsed: 0m 14s (remain 4m 24s) Loss: 0.2607(0.9170) Grad: 88933.359375 LR: 0.00009703 \n",
            "[0][59/783] Elapsed: 0m 20s (remain 4m 10s) Loss: 0.1563(0.6692) Grad: 186682.390625 LR: 0.00009559 \n",
            "[0][79/783] Elapsed: 0m 27s (remain 4m 0s) Loss: 0.0779(0.5441) Grad: 141349.421875 LR: 0.00009416 \n",
            "[0][99/783] Elapsed: 0m 33s (remain 3m 51s) Loss: 0.0964(0.4679) Grad: 80614.7578125 LR: 0.00009275 \n",
            "[0][119/783] Elapsed: 0m 40s (remain 3m 44s) Loss: 0.2041(0.4099) Grad: 165196.90625 LR: 0.00009137 \n",
            "[0][139/783] Elapsed: 0m 47s (remain 3m 37s) Loss: 0.1572(0.3700) Grad: 154415.484375 LR: 0.00009000 \n",
            "[0][159/783] Elapsed: 0m 53s (remain 3m 30s) Loss: 0.1772(0.3404) Grad: 227742.5 LR: 0.00008866 \n",
            "[0][179/783] Elapsed: 1m 0s (remain 3m 22s) Loss: 0.0742(0.3158) Grad: 98222.4296875 LR: 0.00008733 \n",
            "[0][199/783] Elapsed: 1m 7s (remain 3m 15s) Loss: 0.1501(0.2959) Grad: 117917.921875 LR: 0.00008603 \n",
            "[0][219/783] Elapsed: 1m 13s (remain 3m 8s) Loss: 0.0886(0.2787) Grad: 111222.71875 LR: 0.00008474 \n",
            "[0][239/783] Elapsed: 1m 20s (remain 3m 1s) Loss: 0.0617(0.2654) Grad: 73660.765625 LR: 0.00008348 \n",
            "[0][259/783] Elapsed: 1m 26s (remain 2m 54s) Loss: 0.1797(0.2547) Grad: 249540.125 LR: 0.00008223 \n",
            "[0][279/783] Elapsed: 1m 33s (remain 2m 47s) Loss: 0.0746(0.2457) Grad: 104170.953125 LR: 0.00008100 \n",
            "[0][299/783] Elapsed: 1m 40s (remain 2m 41s) Loss: 0.0662(0.2379) Grad: 59005.37890625 LR: 0.00007979 \n",
            "[0][319/783] Elapsed: 1m 46s (remain 2m 34s) Loss: 0.0710(0.2305) Grad: 47553.79296875 LR: 0.00007860 \n",
            "[0][339/783] Elapsed: 1m 53s (remain 2m 27s) Loss: 0.0637(0.2238) Grad: 62815.62890625 LR: 0.00007743 \n",
            "[0][359/783] Elapsed: 1m 59s (remain 2m 20s) Loss: 0.0985(0.2177) Grad: 112017.3046875 LR: 0.00007627 \n",
            "[0][379/783] Elapsed: 2m 6s (remain 2m 14s) Loss: 0.1801(0.2119) Grad: 168376.890625 LR: 0.00007513 \n",
            "[0][399/783] Elapsed: 2m 13s (remain 2m 7s) Loss: 0.1334(0.2072) Grad: 140390.34375 LR: 0.00007401 \n",
            "[0][419/783] Elapsed: 2m 19s (remain 2m 0s) Loss: 0.1153(0.2023) Grad: 157155.546875 LR: 0.00007290 \n",
            "[0][439/783] Elapsed: 2m 26s (remain 1m 54s) Loss: 0.0924(0.1984) Grad: 117949.3359375 LR: 0.00007181 \n",
            "[0][459/783] Elapsed: 2m 32s (remain 1m 47s) Loss: 0.1642(0.1952) Grad: 131059.6171875 LR: 0.00007074 \n",
            "[0][479/783] Elapsed: 2m 39s (remain 1m 40s) Loss: 0.0872(0.1920) Grad: 130851.7421875 LR: 0.00006968 \n",
            "[0][499/783] Elapsed: 2m 46s (remain 1m 33s) Loss: 0.0882(0.1885) Grad: 76220.2265625 LR: 0.00006864 \n",
            "[0][519/783] Elapsed: 2m 52s (remain 1m 27s) Loss: 0.2076(0.1858) Grad: 117829.5703125 LR: 0.00006762 \n",
            "[0][539/783] Elapsed: 2m 59s (remain 1m 20s) Loss: 0.0605(0.1842) Grad: 81275.96875 LR: 0.00006661 \n",
            "[0][559/783] Elapsed: 3m 5s (remain 1m 14s) Loss: 0.1847(0.1819) Grad: 155735.671875 LR: 0.00006561 \n",
            "[0][579/783] Elapsed: 3m 12s (remain 1m 7s) Loss: 0.0884(0.1795) Grad: 69662.171875 LR: 0.00006463 \n",
            "[0][599/783] Elapsed: 3m 19s (remain 1m 0s) Loss: 0.1308(0.1770) Grad: 131082.96875 LR: 0.00006367 \n",
            "[0][619/783] Elapsed: 3m 25s (remain 0m 54s) Loss: 0.1150(0.1750) Grad: 140315.078125 LR: 0.00006272 \n",
            "[0][639/783] Elapsed: 3m 32s (remain 0m 47s) Loss: 0.1044(0.1732) Grad: 151488.1875 LR: 0.00006178 \n",
            "[0][659/783] Elapsed: 3m 38s (remain 0m 40s) Loss: 0.1650(0.1719) Grad: 209242.203125 LR: 0.00006086 \n",
            "[0][679/783] Elapsed: 3m 45s (remain 0m 34s) Loss: 0.1092(0.1699) Grad: 169458.28125 LR: 0.00005995 \n",
            "[0][699/783] Elapsed: 3m 51s (remain 0m 27s) Loss: 0.1104(0.1685) Grad: 87984.671875 LR: 0.00005905 \n",
            "[0][719/783] Elapsed: 3m 58s (remain 0m 20s) Loss: 0.0857(0.1668) Grad: 125477.7734375 LR: 0.00005817 \n",
            "[0][739/783] Elapsed: 4m 5s (remain 0m 14s) Loss: 0.1210(0.1655) Grad: 102693.9453125 LR: 0.00005730 \n",
            "[0][759/783] Elapsed: 4m 11s (remain 0m 7s) Loss: 0.1823(0.1645) Grad: 151622.828125 LR: 0.00005645 \n",
            "[0][779/783] Elapsed: 4m 18s (remain 0m 0s) Loss: 0.2320(0.1632) Grad: 170645.140625 LR: 0.00005560 \n",
            "[1][19/195] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0626(0.0952) \n",
            "[1][39/195] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1104(0.1033) \n",
            "[1][59/195] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1385(0.1119) \n",
            "[1][79/195] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0909(0.1071) \n",
            "[1][99/195] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1068(0.1052) \n",
            "[1][119/195] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1235(0.1057) \n",
            "[1][139/195] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0693(0.1062) \n",
            "[1][159/195] Elapsed: 0m 34s (remain 0m 7s) Loss: 0.0552(0.1052) \n",
            "[1][179/195] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1966(0.1078) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1 - avg_train_loss = 0.1631 avg_val_loss: 0.1074 time: 300.7663s\n",
            "Epoch: 1 - Score: 0.4651, Scores: [0.48553763825112545, 0.46286388668305734, 0.43294210874559136, 0.46273514523289316, 0.47782467958336794, 0.46845511347539165]\n",
            "Epoch 1 - Save Best Score: 0.4651 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][195/195] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0707(0.1074) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/783] Elapsed: 0m 6s (remain 4m 15s) Loss: 0.1168(0.1105) Grad: 262935.25 LR: 0.00005465 \n",
            "[1][39/783] Elapsed: 0m 13s (remain 4m 6s) Loss: 0.0476(0.1062) Grad: 182202.75 LR: 0.00005383 \n",
            "[1][59/783] Elapsed: 0m 19s (remain 4m 0s) Loss: 0.0518(0.1079) Grad: 100504.7578125 LR: 0.00005303 \n",
            "[1][79/783] Elapsed: 0m 26s (remain 3m 53s) Loss: 0.1218(0.1056) Grad: 189165.421875 LR: 0.00005224 \n",
            "[1][99/783] Elapsed: 0m 33s (remain 3m 46s) Loss: 0.0493(0.1031) Grad: 157897.578125 LR: 0.00005146 \n",
            "[1][119/783] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.0883(0.1027) Grad: 157243.03125 LR: 0.00005069 \n",
            "[1][139/783] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.1516(0.1030) Grad: 522948.0625 LR: 0.00004993 \n",
            "[1][159/783] Elapsed: 0m 52s (remain 3m 26s) Loss: 0.0484(0.1035) Grad: 152926.53125 LR: 0.00004918 \n",
            "[1][179/783] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.1407(0.1035) Grad: 444762.1875 LR: 0.00004845 \n",
            "[1][199/783] Elapsed: 1m 6s (remain 3m 12s) Loss: 0.0367(0.1036) Grad: 163598.59375 LR: 0.00004773 \n",
            "[1][219/783] Elapsed: 1m 12s (remain 3m 6s) Loss: 0.1524(0.1044) Grad: 389001.46875 LR: 0.00004701 \n",
            "[1][239/783] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.1106(0.1045) Grad: 157209.359375 LR: 0.00004631 \n",
            "[1][259/783] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0727(0.1033) Grad: 161853.0625 LR: 0.00004562 \n",
            "[1][279/783] Elapsed: 1m 32s (remain 2m 46s) Loss: 0.0868(0.1036) Grad: 170856.65625 LR: 0.00004494 \n",
            "[1][299/783] Elapsed: 1m 39s (remain 2m 39s) Loss: 0.1446(0.1024) Grad: 274457.78125 LR: 0.00004427 \n",
            "[1][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.1870(0.1036) Grad: 200922.84375 LR: 0.00004361 \n",
            "[1][339/783] Elapsed: 1m 52s (remain 2m 26s) Loss: 0.1366(0.1036) Grad: 146475.40625 LR: 0.00004295 \n",
            "[1][359/783] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.0918(0.1034) Grad: 241191.65625 LR: 0.00004231 \n",
            "[1][379/783] Elapsed: 2m 5s (remain 2m 12s) Loss: 0.1309(0.1038) Grad: 268153.75 LR: 0.00004168 \n",
            "[1][399/783] Elapsed: 2m 12s (remain 2m 6s) Loss: 0.0914(0.1036) Grad: 198127.0 LR: 0.00004106 \n",
            "[1][419/783] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0930(0.1034) Grad: 183578.390625 LR: 0.00004044 \n",
            "[1][439/783] Elapsed: 2m 25s (remain 1m 53s) Loss: 0.1218(0.1033) Grad: 399256.71875 LR: 0.00003984 \n",
            "[1][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.0797(0.1035) Grad: 137431.890625 LR: 0.00003925 \n",
            "[1][479/783] Elapsed: 2m 38s (remain 1m 39s) Loss: 0.0564(0.1032) Grad: 148574.25 LR: 0.00003866 \n",
            "[1][499/783] Elapsed: 2m 45s (remain 1m 33s) Loss: 0.1264(0.1027) Grad: 269257.84375 LR: 0.00003808 \n",
            "[1][519/783] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.0709(0.1034) Grad: 133691.3125 LR: 0.00003751 \n",
            "[1][539/783] Elapsed: 2m 58s (remain 1m 20s) Loss: 0.0877(0.1040) Grad: 105266.21875 LR: 0.00003695 \n",
            "[1][559/783] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.1176(0.1035) Grad: 152029.046875 LR: 0.00003640 \n",
            "[1][579/783] Elapsed: 3m 11s (remain 1m 6s) Loss: 0.1085(0.1040) Grad: 106998.2109375 LR: 0.00003586 \n",
            "[1][599/783] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.0808(0.1036) Grad: 116909.4140625 LR: 0.00003532 \n",
            "[1][619/783] Elapsed: 3m 24s (remain 0m 53s) Loss: 0.1618(0.1038) Grad: 77565.421875 LR: 0.00003479 \n",
            "[1][639/783] Elapsed: 3m 31s (remain 0m 47s) Loss: 0.0734(0.1037) Grad: 82850.9140625 LR: 0.00003427 \n",
            "[1][659/783] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.1452(0.1038) Grad: 89654.6953125 LR: 0.00003376 \n",
            "[1][679/783] Elapsed: 3m 44s (remain 0m 33s) Loss: 0.1563(0.1039) Grad: 112185.4453125 LR: 0.00003326 \n",
            "[1][699/783] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.0667(0.1045) Grad: 111269.5390625 LR: 0.00003276 \n",
            "[1][719/783] Elapsed: 3m 57s (remain 0m 20s) Loss: 0.0922(0.1046) Grad: 205627.015625 LR: 0.00003227 \n",
            "[1][739/783] Elapsed: 4m 4s (remain 0m 14s) Loss: 0.0929(0.1046) Grad: 96256.6953125 LR: 0.00003179 \n",
            "[1][759/783] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0867(0.1051) Grad: 125143.46875 LR: 0.00003131 \n",
            "[1][779/783] Elapsed: 4m 17s (remain 0m 0s) Loss: 0.1184(0.1050) Grad: 89752.0078125 LR: 0.00003085 \n",
            "[2][19/195] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0564(0.0910) \n",
            "[2][39/195] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1088(0.0986) \n",
            "[2][59/195] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1653(0.1071) \n",
            "[2][79/195] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0701(0.1032) \n",
            "[2][99/195] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0837(0.1009) \n",
            "[2][119/195] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1198(0.1016) \n",
            "[2][139/195] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0638(0.1023) \n",
            "[2][159/195] Elapsed: 0m 34s (remain 0m 7s) Loss: 0.0645(0.1015) \n",
            "[2][179/195] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1840(0.1040) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2 - avg_train_loss = 0.1051 avg_val_loss: 0.1036 time: 299.7450s\n",
            "Epoch: 2 - Score: 0.4563, Scores: [0.47959054715540494, 0.4576673340095501, 0.4225161352271177, 0.4540978044273171, 0.47833399622320216, 0.4457512265344438]\n",
            "Epoch 2 - Save Best Score: 0.4563 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][195/195] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0528(0.1036) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/783] Elapsed: 0m 6s (remain 4m 15s) Loss: 0.0967(0.0965) Grad: 182077.71875 LR: 0.00003032 \n",
            "[2][39/783] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.1183(0.1027) Grad: 245811.015625 LR: 0.00002986 \n",
            "[2][59/783] Elapsed: 0m 19s (remain 4m 0s) Loss: 0.0952(0.1001) Grad: 250169.78125 LR: 0.00002942 \n",
            "[2][79/783] Elapsed: 0m 26s (remain 3m 53s) Loss: 0.1269(0.0993) Grad: 252557.671875 LR: 0.00002898 \n",
            "[2][99/783] Elapsed: 0m 33s (remain 3m 46s) Loss: 0.0619(0.0991) Grad: 156488.6875 LR: 0.00002855 \n",
            "[2][119/783] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.1486(0.1015) Grad: 499653.625 LR: 0.00002812 \n",
            "[2][139/783] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.0590(0.1003) Grad: 169920.75 LR: 0.00002770 \n",
            "[2][159/783] Elapsed: 0m 52s (remain 3m 26s) Loss: 0.1014(0.1000) Grad: 293314.5625 LR: 0.00002729 \n",
            "[2][179/783] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.0479(0.0994) Grad: 151032.078125 LR: 0.00002688 \n",
            "[2][199/783] Elapsed: 1m 6s (remain 3m 12s) Loss: 0.0788(0.0989) Grad: 341305.96875 LR: 0.00002648 \n",
            "[2][219/783] Elapsed: 1m 12s (remain 3m 6s) Loss: 0.2097(0.0994) Grad: 203902.03125 LR: 0.00002608 \n",
            "[2][239/783] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.0533(0.0999) Grad: 161291.3125 LR: 0.00002569 \n",
            "[2][259/783] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0844(0.1001) Grad: 186536.515625 LR: 0.00002531 \n",
            "[2][279/783] Elapsed: 1m 32s (remain 2m 46s) Loss: 0.0873(0.1005) Grad: 164192.75 LR: 0.00002493 \n",
            "[2][299/783] Elapsed: 1m 39s (remain 2m 39s) Loss: 0.0285(0.1010) Grad: 128279.328125 LR: 0.00002456 \n",
            "[2][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0685(0.1009) Grad: 183806.796875 LR: 0.00002419 \n",
            "[2][339/783] Elapsed: 1m 52s (remain 2m 26s) Loss: 0.0900(0.1003) Grad: 341159.25 LR: 0.00002383 \n",
            "[2][359/783] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.1737(0.1002) Grad: 167521.15625 LR: 0.00002347 \n",
            "[2][379/783] Elapsed: 2m 5s (remain 2m 13s) Loss: 0.0978(0.1007) Grad: 309600.0625 LR: 0.00002312 \n",
            "[2][399/783] Elapsed: 2m 12s (remain 2m 6s) Loss: 0.0567(0.1010) Grad: 205496.171875 LR: 0.00002278 \n",
            "[2][419/783] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0453(0.1003) Grad: 190433.65625 LR: 0.00002244 \n",
            "[2][439/783] Elapsed: 2m 25s (remain 1m 53s) Loss: 0.0523(0.1005) Grad: 201735.828125 LR: 0.00002210 \n",
            "[2][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1059(0.1002) Grad: 322900.34375 LR: 0.00002177 \n",
            "[2][479/783] Elapsed: 2m 38s (remain 1m 39s) Loss: 0.0556(0.1000) Grad: 182385.703125 LR: 0.00002145 \n",
            "[2][499/783] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.0895(0.0996) Grad: 205418.8125 LR: 0.00002113 \n",
            "[2][519/783] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.1454(0.0999) Grad: 158303.546875 LR: 0.00002081 \n",
            "[2][539/783] Elapsed: 2m 58s (remain 1m 20s) Loss: 0.0749(0.0995) Grad: 212638.78125 LR: 0.00002050 \n",
            "[2][559/783] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0811(0.0997) Grad: 195472.9375 LR: 0.00002019 \n",
            "[2][579/783] Elapsed: 3m 11s (remain 1m 6s) Loss: 0.0846(0.0997) Grad: 326668.59375 LR: 0.00001989 \n",
            "[2][599/783] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.0898(0.0996) Grad: 194548.484375 LR: 0.00001960 \n",
            "[2][619/783] Elapsed: 3m 24s (remain 0m 53s) Loss: 0.1220(0.0999) Grad: 203777.03125 LR: 0.00001930 \n",
            "[2][639/783] Elapsed: 3m 31s (remain 0m 47s) Loss: 0.0593(0.1001) Grad: 214626.65625 LR: 0.00001901 \n",
            "[2][659/783] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.1182(0.0998) Grad: 298710.8125 LR: 0.00001873 \n",
            "[2][679/783] Elapsed: 3m 44s (remain 0m 33s) Loss: 0.0951(0.0997) Grad: 262603.34375 LR: 0.00001845 \n",
            "[2][699/783] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.1100(0.0994) Grad: 285819.71875 LR: 0.00001817 \n",
            "[2][719/783] Elapsed: 3m 57s (remain 0m 20s) Loss: 0.0629(0.0990) Grad: 126313.3203125 LR: 0.00001790 \n",
            "[2][739/783] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.0765(0.0992) Grad: 212043.0625 LR: 0.00001764 \n",
            "[2][759/783] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0733(0.0994) Grad: 184177.8125 LR: 0.00001737 \n",
            "[2][779/783] Elapsed: 4m 17s (remain 0m 0s) Loss: 0.1207(0.0994) Grad: 183757.5625 LR: 0.00001711 \n",
            "[3][19/195] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0557(0.0913) \n",
            "[3][39/195] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1003(0.0990) \n",
            "[3][59/195] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1535(0.1068) \n",
            "[3][79/195] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0766(0.1027) \n",
            "[3][99/195] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0940(0.1000) \n",
            "[3][119/195] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1123(0.1006) \n",
            "[3][139/195] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0719(0.1014) \n",
            "[3][159/195] Elapsed: 0m 34s (remain 0m 7s) Loss: 0.0587(0.1007) \n",
            "[3][179/195] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1855(0.1030) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3 - avg_train_loss = 0.0994 avg_val_loss: 0.1024 time: 299.6775s\n",
            "Epoch: 3 - Score: 0.4536, Scores: [0.47988349236460764, 0.456898197846496, 0.41880411861646316, 0.4546648135030823, 0.4690435416606141, 0.4423078494213756]\n",
            "Epoch 3 - Save Best Score: 0.4536 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][195/195] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0521(0.1024) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/783] Elapsed: 0m 6s (remain 4m 15s) Loss: 0.0736(0.0904) Grad: 192430.859375 LR: 0.00001682 \n",
            "[3][39/783] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.0591(0.0875) Grad: 187311.453125 LR: 0.00001657 \n",
            "[3][59/783] Elapsed: 0m 19s (remain 4m 0s) Loss: 0.1073(0.0934) Grad: 313251.125 LR: 0.00001632 \n",
            "[3][79/783] Elapsed: 0m 26s (remain 3m 53s) Loss: 0.0766(0.0976) Grad: 182752.765625 LR: 0.00001608 \n",
            "[3][99/783] Elapsed: 0m 33s (remain 3m 46s) Loss: 0.1534(0.0980) Grad: 430150.59375 LR: 0.00001584 \n",
            "[3][119/783] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.0874(0.0970) Grad: 315770.53125 LR: 0.00001560 \n",
            "[3][139/783] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.0554(0.0949) Grad: 136960.0 LR: 0.00001537 \n",
            "[3][159/783] Elapsed: 0m 52s (remain 3m 26s) Loss: 0.1938(0.0961) Grad: 176613.53125 LR: 0.00001514 \n",
            "[3][179/783] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.0690(0.0963) Grad: 174154.421875 LR: 0.00001491 \n",
            "[3][199/783] Elapsed: 1m 6s (remain 3m 12s) Loss: 0.0802(0.0964) Grad: 163898.6875 LR: 0.00001469 \n",
            "[3][219/783] Elapsed: 1m 12s (remain 3m 6s) Loss: 0.0815(0.0967) Grad: 129016.4609375 LR: 0.00001447 \n",
            "[3][239/783] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.1445(0.0962) Grad: 362240.625 LR: 0.00001425 \n",
            "[3][259/783] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0833(0.0962) Grad: 173389.046875 LR: 0.00001404 \n",
            "[3][279/783] Elapsed: 1m 32s (remain 2m 46s) Loss: 0.1148(0.0962) Grad: 366826.71875 LR: 0.00001383 \n",
            "[3][299/783] Elapsed: 1m 39s (remain 2m 39s) Loss: 0.1030(0.0969) Grad: 208907.4375 LR: 0.00001362 \n",
            "[3][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0738(0.0967) Grad: 144366.109375 LR: 0.00001342 \n",
            "[3][339/783] Elapsed: 1m 52s (remain 2m 26s) Loss: 0.0863(0.0967) Grad: 254696.671875 LR: 0.00001322 \n",
            "[3][359/783] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.0799(0.0969) Grad: 114859.1171875 LR: 0.00001302 \n",
            "[3][379/783] Elapsed: 2m 5s (remain 2m 12s) Loss: 0.1472(0.0972) Grad: 207250.4375 LR: 0.00001283 \n",
            "[3][399/783] Elapsed: 2m 11s (remain 2m 6s) Loss: 0.0620(0.0971) Grad: 126295.265625 LR: 0.00001264 \n",
            "[3][419/783] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.2029(0.0970) Grad: 281103.4375 LR: 0.00001245 \n",
            "[3][439/783] Elapsed: 2m 25s (remain 1m 53s) Loss: 0.0467(0.0972) Grad: 131902.640625 LR: 0.00001226 \n",
            "[3][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1421(0.0974) Grad: 241720.4375 LR: 0.00001208 \n",
            "[3][479/783] Elapsed: 2m 38s (remain 1m 39s) Loss: 0.2134(0.0977) Grad: 244546.65625 LR: 0.00001190 \n",
            "[3][499/783] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.0695(0.0973) Grad: 178565.4375 LR: 0.00001172 \n",
            "[3][519/783] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.0858(0.0970) Grad: 263922.21875 LR: 0.00001155 \n",
            "[3][539/783] Elapsed: 2m 58s (remain 1m 20s) Loss: 0.0822(0.0968) Grad: 176698.4375 LR: 0.00001137 \n",
            "[3][559/783] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0598(0.0967) Grad: 153539.046875 LR: 0.00001120 \n",
            "[3][579/783] Elapsed: 3m 11s (remain 1m 6s) Loss: 0.0960(0.0966) Grad: 229367.546875 LR: 0.00001104 \n",
            "[3][599/783] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.1887(0.0968) Grad: 351900.28125 LR: 0.00001087 \n",
            "[3][619/783] Elapsed: 3m 24s (remain 0m 53s) Loss: 0.1111(0.0965) Grad: 198448.84375 LR: 0.00001071 \n",
            "[3][639/783] Elapsed: 3m 30s (remain 0m 47s) Loss: 0.0519(0.0966) Grad: 148189.109375 LR: 0.00001055 \n",
            "[3][659/783] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.0996(0.0970) Grad: 220251.4375 LR: 0.00001039 \n",
            "[3][679/783] Elapsed: 3m 44s (remain 0m 33s) Loss: 0.0745(0.0966) Grad: 238841.125 LR: 0.00001024 \n",
            "[3][699/783] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.1056(0.0962) Grad: 265147.75 LR: 0.00001008 \n",
            "[3][719/783] Elapsed: 3m 57s (remain 0m 20s) Loss: 0.1128(0.0963) Grad: 458245.25 LR: 0.00000993 \n",
            "[3][739/783] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.0922(0.0962) Grad: 198803.03125 LR: 0.00000978 \n",
            "[3][759/783] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0632(0.0961) Grad: 149790.046875 LR: 0.00000964 \n",
            "[3][779/783] Elapsed: 4m 17s (remain 0m 0s) Loss: 0.0622(0.0963) Grad: 244408.921875 LR: 0.00000949 \n",
            "[4][19/195] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0583(0.0898) \n",
            "[4][39/195] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1056(0.0979) \n",
            "[4][59/195] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1566(0.1061) \n",
            "[4][79/195] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0710(0.1023) \n",
            "[4][99/195] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0985(0.0995) \n",
            "[4][119/195] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1000(0.1003) \n",
            "[4][139/195] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0685(0.1011) \n",
            "[4][159/195] Elapsed: 0m 34s (remain 0m 7s) Loss: 0.0637(0.1003) \n",
            "[4][179/195] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1750(0.1025) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4 - avg_train_loss = 0.0964 avg_val_loss: 0.1018 time: 299.6495s\n",
            "Epoch: 4 - Score: 0.4524, Scores: [0.47640881263829066, 0.45776162460157954, 0.41868910358923933, 0.452380555922179, 0.4684776153382562, 0.44044009373976073]\n",
            "Epoch 4 - Save Best Score: 0.4524 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][195/195] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0497(0.1018) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/783] Elapsed: 0m 6s (remain 4m 16s) Loss: 0.1713(0.1073) Grad: 461288.875 LR: 0.00000933 \n",
            "[4][39/783] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.0435(0.0990) Grad: 178609.609375 LR: 0.00000919 \n",
            "[4][59/783] Elapsed: 0m 19s (remain 4m 0s) Loss: 0.0456(0.1013) Grad: 125355.609375 LR: 0.00000905 \n",
            "[4][79/783] Elapsed: 0m 26s (remain 3m 53s) Loss: 0.0667(0.1006) Grad: 285112.96875 LR: 0.00000892 \n",
            "[4][99/783] Elapsed: 0m 33s (remain 3m 46s) Loss: 0.0486(0.0977) Grad: 133471.875 LR: 0.00000879 \n",
            "[4][119/783] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.0942(0.0972) Grad: 368474.25 LR: 0.00000865 \n",
            "[4][139/783] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.1282(0.0992) Grad: 161010.375 LR: 0.00000853 \n",
            "[4][159/783] Elapsed: 0m 52s (remain 3m 26s) Loss: 0.1457(0.0978) Grad: 254464.921875 LR: 0.00000840 \n",
            "[4][179/783] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.1563(0.0965) Grad: 447202.09375 LR: 0.00000827 \n",
            "[4][199/783] Elapsed: 1m 6s (remain 3m 12s) Loss: 0.1197(0.0965) Grad: 215566.671875 LR: 0.00000815 \n",
            "[4][219/783] Elapsed: 1m 12s (remain 3m 6s) Loss: 0.0501(0.0969) Grad: 255158.84375 LR: 0.00000803 \n",
            "[4][239/783] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.0844(0.0952) Grad: 222023.921875 LR: 0.00000791 \n",
            "[4][259/783] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0952(0.0955) Grad: 203284.234375 LR: 0.00000779 \n",
            "[4][279/783] Elapsed: 1m 32s (remain 2m 46s) Loss: 0.1745(0.0955) Grad: 255118.921875 LR: 0.00000767 \n",
            "[4][299/783] Elapsed: 1m 39s (remain 2m 39s) Loss: 0.0675(0.0959) Grad: 183683.5 LR: 0.00000756 \n",
            "[4][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.1357(0.0956) Grad: 358217.0 LR: 0.00000745 \n",
            "[4][339/783] Elapsed: 1m 52s (remain 2m 26s) Loss: 0.0806(0.0955) Grad: 159630.375 LR: 0.00000733 \n",
            "[4][359/783] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.0955(0.0956) Grad: 315542.78125 LR: 0.00000722 \n",
            "[4][379/783] Elapsed: 2m 5s (remain 2m 13s) Loss: 0.0521(0.0952) Grad: 173210.1875 LR: 0.00000712 \n",
            "[4][399/783] Elapsed: 2m 12s (remain 2m 6s) Loss: 0.0629(0.0951) Grad: 141345.890625 LR: 0.00000701 \n",
            "[4][419/783] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0340(0.0948) Grad: 133951.09375 LR: 0.00000691 \n",
            "[4][439/783] Elapsed: 2m 25s (remain 1m 53s) Loss: 0.0826(0.0945) Grad: 293902.21875 LR: 0.00000680 \n",
            "[4][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1065(0.0944) Grad: 341220.6875 LR: 0.00000670 \n",
            "[4][479/783] Elapsed: 2m 38s (remain 1m 39s) Loss: 0.0835(0.0945) Grad: 199400.6875 LR: 0.00000660 \n",
            "[4][499/783] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.0572(0.0944) Grad: 125638.609375 LR: 0.00000650 \n",
            "[4][519/783] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.0813(0.0947) Grad: 135570.59375 LR: 0.00000641 \n",
            "[4][539/783] Elapsed: 2m 58s (remain 1m 20s) Loss: 0.1576(0.0947) Grad: 378022.9375 LR: 0.00000631 \n",
            "[4][559/783] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.1241(0.0947) Grad: 153262.9375 LR: 0.00000622 \n",
            "[4][579/783] Elapsed: 3m 11s (remain 1m 6s) Loss: 0.0983(0.0947) Grad: 147836.515625 LR: 0.00000612 \n",
            "[4][599/783] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.0580(0.0950) Grad: 102362.3515625 LR: 0.00000603 \n",
            "[4][619/783] Elapsed: 3m 24s (remain 0m 53s) Loss: 0.1768(0.0949) Grad: 206098.75 LR: 0.00000594 \n",
            "[4][639/783] Elapsed: 3m 31s (remain 0m 47s) Loss: 0.0740(0.0946) Grad: 128807.6953125 LR: 0.00000585 \n",
            "[4][659/783] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.0464(0.0943) Grad: 211062.265625 LR: 0.00000576 \n",
            "[4][679/783] Elapsed: 3m 44s (remain 0m 33s) Loss: 0.1726(0.0944) Grad: 234298.640625 LR: 0.00000568 \n",
            "[4][699/783] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.0846(0.0946) Grad: 245576.46875 LR: 0.00000559 \n",
            "[4][719/783] Elapsed: 3m 57s (remain 0m 20s) Loss: 0.0636(0.0947) Grad: 229348.65625 LR: 0.00000551 \n",
            "[4][739/783] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.1317(0.0948) Grad: 450093.96875 LR: 0.00000543 \n",
            "[4][759/783] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0984(0.0949) Grad: 182318.0 LR: 0.00000535 \n",
            "[4][779/783] Elapsed: 4m 17s (remain 0m 0s) Loss: 0.0845(0.0949) Grad: 164144.40625 LR: 0.00000527 \n",
            "[5][19/195] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0572(0.0896) \n",
            "[5][39/195] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1038(0.0976) \n",
            "[5][59/195] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1546(0.1056) \n",
            "[5][79/195] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0736(0.1017) \n",
            "[5][99/195] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0984(0.0991) \n",
            "[5][119/195] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1045(0.0999) \n",
            "[5][139/195] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0688(0.1006) \n",
            "[5][159/195] Elapsed: 0m 34s (remain 0m 7s) Loss: 0.0619(0.0999) \n",
            "[5][179/195] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1722(0.1020) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5 - avg_train_loss = 0.0949 avg_val_loss: 0.1014 time: 299.6372s\n",
            "Epoch: 5 - Score: 0.4513, Scores: [0.47486252547268554, 0.4562591933219615, 0.41928850554311603, 0.4507207335035314, 0.46708362500722833, 0.4396209436226126]\n",
            "Epoch 5 - Save Best Score: 0.4513 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5][195/195] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0508(0.1014) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = igetitem(value, i)\n",
            "========== fold: 0 result ==========\n",
            "Score: 0.4513  Scores: [0.47486252547268554, 0.4562591933219615, 0.41928850554311603, 0.4507207335035314, 0.46708362500722833, 0.4396209436226126]\n",
            "========== fold: 1 training ==========\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<class '__main__.CFG'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] num_train_steps:  3905  len train loader:  781\n",
            "warmup start:  19525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/781] Elapsed: 0m 6s (remain 4m 13s) Loss: 1.2630(1.7638) Grad: 313708.8125 LR: 0.00009851 \n",
            "[0][39/781] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.1302(1.0261) Grad: 117226.03125 LR: 0.00009703 \n",
            "[0][59/781] Elapsed: 0m 19s (remain 3m 57s) Loss: 0.1133(0.7463) Grad: 77318.34375 LR: 0.00009559 \n",
            "[0][79/781] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.1897(0.5965) Grad: 111742.7109375 LR: 0.00009416 \n",
            "[0][99/781] Elapsed: 0m 33s (remain 3m 44s) Loss: 0.1449(0.5019) Grad: 170520.1875 LR: 0.00009275 \n",
            "[0][119/781] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.1044(0.4413) Grad: 63116.296875 LR: 0.00009137 \n",
            "[0][139/781] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.0739(0.3984) Grad: 90030.2578125 LR: 0.00009000 \n",
            "[0][159/781] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.1339(0.3655) Grad: 119022.3671875 LR: 0.00008866 \n",
            "[0][179/781] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.1199(0.3394) Grad: 90602.6015625 LR: 0.00008733 \n",
            "[0][199/781] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.1030(0.3182) Grad: 98825.1171875 LR: 0.00008603 \n",
            "[0][219/781] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.1305(0.2987) Grad: 133164.140625 LR: 0.00008474 \n",
            "[0][239/781] Elapsed: 1m 18s (remain 2m 57s) Loss: 0.0835(0.2855) Grad: 92074.3984375 LR: 0.00008348 \n",
            "[0][259/781] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1318(0.2734) Grad: 168738.546875 LR: 0.00008223 \n",
            "[0][279/781] Elapsed: 1m 32s (remain 2m 44s) Loss: 0.0563(0.2633) Grad: 93353.5234375 LR: 0.00008100 \n",
            "[0][299/781] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.1943(0.2530) Grad: 146373.546875 LR: 0.00007979 \n",
            "[0][319/781] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.2621(0.2451) Grad: 302166.75 LR: 0.00007860 \n",
            "[0][339/781] Elapsed: 1m 51s (remain 2m 24s) Loss: 0.0882(0.2377) Grad: 183967.078125 LR: 0.00007743 \n",
            "[0][359/781] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.1941(0.2313) Grad: 197062.1875 LR: 0.00007627 \n",
            "[0][379/781] Elapsed: 2m 4s (remain 2m 11s) Loss: 0.0957(0.2253) Grad: 82272.5625 LR: 0.00007513 \n",
            "[0][399/781] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.2483(0.2206) Grad: 121999.609375 LR: 0.00007401 \n",
            "[0][419/781] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.1865(0.2158) Grad: 142329.625 LR: 0.00007290 \n",
            "[0][439/781] Elapsed: 2m 24s (remain 1m 51s) Loss: 0.1159(0.2115) Grad: 148924.203125 LR: 0.00007181 \n",
            "[0][459/781] Elapsed: 2m 31s (remain 1m 45s) Loss: 0.2083(0.2081) Grad: 143498.328125 LR: 0.00007074 \n",
            "[0][479/781] Elapsed: 2m 37s (remain 1m 38s) Loss: 0.1557(0.2045) Grad: 193179.6875 LR: 0.00006968 \n",
            "[0][499/781] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.1027(0.2008) Grad: 85842.7265625 LR: 0.00006864 \n",
            "[0][519/781] Elapsed: 2m 50s (remain 1m 25s) Loss: 0.1469(0.1978) Grad: 186757.625 LR: 0.00006762 \n",
            "[0][539/781] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0891(0.1952) Grad: 93035.5 LR: 0.00006661 \n",
            "[0][559/781] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.0889(0.1922) Grad: 59753.78125 LR: 0.00006561 \n",
            "[0][579/781] Elapsed: 3m 10s (remain 1m 5s) Loss: 0.0958(0.1895) Grad: 97366.953125 LR: 0.00006463 \n",
            "[0][599/781] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.1427(0.1867) Grad: 108891.8671875 LR: 0.00006367 \n",
            "[0][619/781] Elapsed: 3m 23s (remain 0m 52s) Loss: 0.1956(0.1843) Grad: 89914.515625 LR: 0.00006272 \n",
            "[0][639/781] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.0963(0.1818) Grad: 111504.890625 LR: 0.00006178 \n",
            "[0][659/781] Elapsed: 3m 36s (remain 0m 39s) Loss: 0.1138(0.1801) Grad: 107179.46875 LR: 0.00006086 \n",
            "[0][679/781] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.1293(0.1778) Grad: 209528.96875 LR: 0.00005995 \n",
            "[0][699/781] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.1236(0.1759) Grad: 77520.078125 LR: 0.00005905 \n",
            "[0][719/781] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1527(0.1742) Grad: 121582.8046875 LR: 0.00005817 \n",
            "[0][739/781] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.1435(0.1723) Grad: 148502.8125 LR: 0.00005730 \n",
            "[0][759/781] Elapsed: 4m 9s (remain 0m 6s) Loss: 0.0902(0.1710) Grad: 105457.53125 LR: 0.00005645 \n",
            "[0][779/781] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.0665(0.1698) Grad: 111911.1640625 LR: 0.00005560 \n",
            "[1][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.1137(0.1105) \n",
            "[1][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0690(0.1031) \n",
            "[1][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1022(0.1027) \n",
            "[1][79/196] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1151(0.1008) \n",
            "[1][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1513(0.1063) \n",
            "[1][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.2041(0.1063) \n",
            "[1][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0878(0.1057) \n",
            "[1][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1654(0.1071) \n",
            "[1][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1096(0.1067) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1 - avg_train_loss = 0.1697 avg_val_loss: 0.1060 time: 297.9898s\n",
            "Epoch: 1 - Score: 0.4617, Scores: [0.48580829367790657, 0.4323346213225305, 0.45937869810183185, 0.46092950899927326, 0.47410185252611525, 0.4577047211419799]\n",
            "Epoch 1 - Save Best Score: 0.4617 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0871(0.1060) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/781] Elapsed: 0m 6s (remain 4m 12s) Loss: 0.0736(0.0984) Grad: 69701.5625 LR: 0.00005473 \n",
            "[1][39/781] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0791(0.1020) Grad: 65164.17578125 LR: 0.00005391 \n",
            "[1][59/781] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.0904(0.1043) Grad: 114009.03125 LR: 0.00005311 \n",
            "[1][79/781] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.0785(0.1038) Grad: 108404.0703125 LR: 0.00005231 \n",
            "[1][99/781] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.0655(0.1007) Grad: 57650.828125 LR: 0.00005153 \n",
            "[1][119/781] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.1576(0.1022) Grad: 104791.9765625 LR: 0.00005076 \n",
            "[1][139/781] Elapsed: 0m 46s (remain 3m 30s) Loss: 0.0730(0.1008) Grad: 105765.296875 LR: 0.00005001 \n",
            "[1][159/781] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.1208(0.1002) Grad: 210340.734375 LR: 0.00004926 \n",
            "[1][179/781] Elapsed: 0m 59s (remain 3m 17s) Loss: 0.0928(0.1009) Grad: 86361.015625 LR: 0.00004852 \n",
            "[1][199/781] Elapsed: 1m 5s (remain 3m 10s) Loss: 0.0742(0.1012) Grad: 62239.95703125 LR: 0.00004780 \n",
            "[1][219/781] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.1578(0.1024) Grad: 123693.6796875 LR: 0.00004708 \n",
            "[1][239/781] Elapsed: 1m 18s (remain 2m 57s) Loss: 0.1271(0.1024) Grad: 167021.546875 LR: 0.00004638 \n",
            "[1][259/781] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1119(0.1024) Grad: 126257.7109375 LR: 0.00004569 \n",
            "[1][279/781] Elapsed: 1m 31s (remain 2m 44s) Loss: 0.1392(0.1022) Grad: 133408.484375 LR: 0.00004501 \n",
            "[1][299/781] Elapsed: 1m 38s (remain 2m 37s) Loss: 0.1230(0.1029) Grad: 54069.64453125 LR: 0.00004433 \n",
            "[1][319/781] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.1203(0.1031) Grad: 90580.2890625 LR: 0.00004367 \n",
            "[1][339/781] Elapsed: 1m 51s (remain 2m 24s) Loss: 0.1214(0.1040) Grad: 74949.1171875 LR: 0.00004302 \n",
            "[1][359/781] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0960(0.1042) Grad: 76740.703125 LR: 0.00004238 \n",
            "[1][379/781] Elapsed: 2m 4s (remain 2m 11s) Loss: 0.0754(0.1042) Grad: 58532.25 LR: 0.00004174 \n",
            "[1][399/781] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0653(0.1046) Grad: 88405.734375 LR: 0.00004112 \n",
            "[1][419/781] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.1061(0.1042) Grad: 157548.265625 LR: 0.00004051 \n",
            "[1][439/781] Elapsed: 2m 24s (remain 1m 51s) Loss: 0.0936(0.1038) Grad: 94731.5625 LR: 0.00003990 \n",
            "[1][459/781] Elapsed: 2m 30s (remain 1m 45s) Loss: 0.2357(0.1044) Grad: 176519.8125 LR: 0.00003930 \n",
            "[1][479/781] Elapsed: 2m 37s (remain 1m 38s) Loss: 0.0813(0.1042) Grad: 94686.5390625 LR: 0.00003872 \n",
            "[1][499/781] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0751(0.1039) Grad: 80817.0 LR: 0.00003814 \n",
            "[1][519/781] Elapsed: 2m 50s (remain 1m 25s) Loss: 0.1192(0.1042) Grad: 86681.7109375 LR: 0.00003757 \n",
            "[1][539/781] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0741(0.1043) Grad: 91142.5625 LR: 0.00003701 \n",
            "[1][559/781] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.1217(0.1041) Grad: 81691.6875 LR: 0.00003646 \n",
            "[1][579/781] Elapsed: 3m 10s (remain 1m 5s) Loss: 0.1106(0.1046) Grad: 160432.9375 LR: 0.00003591 \n",
            "[1][599/781] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.1584(0.1050) Grad: 191026.046875 LR: 0.00003537 \n",
            "[1][619/781] Elapsed: 3m 23s (remain 0m 52s) Loss: 0.0932(0.1045) Grad: 71892.5625 LR: 0.00003485 \n",
            "[1][639/781] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.0843(0.1048) Grad: 102287.5390625 LR: 0.00003433 \n",
            "[1][659/781] Elapsed: 3m 36s (remain 0m 39s) Loss: 0.0922(0.1050) Grad: 110629.8203125 LR: 0.00003381 \n",
            "[1][679/781] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.2275(0.1052) Grad: 214760.734375 LR: 0.00003331 \n",
            "[1][699/781] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.1843(0.1053) Grad: 282151.125 LR: 0.00003281 \n",
            "[1][719/781] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1332(0.1055) Grad: 120261.90625 LR: 0.00003232 \n",
            "[1][739/781] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.0605(0.1053) Grad: 55392.05078125 LR: 0.00003184 \n",
            "[1][759/781] Elapsed: 4m 9s (remain 0m 6s) Loss: 0.0773(0.1048) Grad: 73706.0234375 LR: 0.00003136 \n",
            "[1][779/781] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.1834(0.1050) Grad: 118010.890625 LR: 0.00003089 \n",
            "[2][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.1266(0.1191) \n",
            "[2][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0672(0.1129) \n",
            "[2][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1389(0.1070) \n",
            "[2][79/196] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1208(0.1055) \n",
            "[2][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1544(0.1074) \n",
            "[2][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1404(0.1065) \n",
            "[2][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0928(0.1057) \n",
            "[2][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1554(0.1067) \n",
            "[2][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1039(0.1078) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2 - avg_train_loss = 0.1050 avg_val_loss: 0.1072 time: 297.7937s\n",
            "Epoch: 2 - Score: 0.4634, Scores: [0.48680434513929766, 0.44718165982252017, 0.425854656271397, 0.4492660048552112, 0.5305862891019537, 0.44054315927742455]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0617(0.1072) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/781] Elapsed: 0m 6s (remain 4m 9s) Loss: 0.0823(0.0860) Grad: 85592.6328125 LR: 0.00003041 \n",
            "[2][39/781] Elapsed: 0m 13s (remain 4m 2s) Loss: 0.1045(0.1055) Grad: 144852.40625 LR: 0.00002995 \n",
            "[2][59/781] Elapsed: 0m 19s (remain 3m 56s) Loss: 0.0678(0.0983) Grad: 89756.9453125 LR: 0.00002951 \n",
            "[2][79/781] Elapsed: 0m 26s (remain 3m 49s) Loss: 0.0782(0.0971) Grad: 61842.3125 LR: 0.00002907 \n",
            "[2][99/781] Elapsed: 0m 32s (remain 3m 43s) Loss: 0.0707(0.0980) Grad: 103962.0546875 LR: 0.00002863 \n",
            "[2][119/781] Elapsed: 0m 39s (remain 3m 36s) Loss: 0.1133(0.0992) Grad: 129055.6796875 LR: 0.00002820 \n",
            "[2][139/781] Elapsed: 0m 45s (remain 3m 30s) Loss: 0.1922(0.1003) Grad: 289774.15625 LR: 0.00002778 \n",
            "[2][159/781] Elapsed: 0m 52s (remain 3m 23s) Loss: 0.1165(0.0994) Grad: 248712.734375 LR: 0.00002737 \n",
            "[2][179/781] Elapsed: 0m 58s (remain 3m 16s) Loss: 0.0927(0.0996) Grad: 77442.484375 LR: 0.00002696 \n",
            "[2][199/781] Elapsed: 1m 5s (remain 3m 10s) Loss: 0.0504(0.0997) Grad: 50791.35546875 LR: 0.00002656 \n",
            "[2][219/781] Elapsed: 1m 12s (remain 3m 3s) Loss: 0.0863(0.0992) Grad: 171298.15625 LR: 0.00002616 \n",
            "[2][239/781] Elapsed: 1m 18s (remain 2m 57s) Loss: 0.1410(0.1000) Grad: 125979.0625 LR: 0.00002577 \n",
            "[2][259/781] Elapsed: 1m 25s (remain 2m 50s) Loss: 0.0730(0.0999) Grad: 70440.7734375 LR: 0.00002538 \n",
            "[2][279/781] Elapsed: 1m 31s (remain 2m 44s) Loss: 0.1022(0.0986) Grad: 101245.96875 LR: 0.00002501 \n",
            "[2][299/781] Elapsed: 1m 38s (remain 2m 37s) Loss: 0.0964(0.0989) Grad: 114632.2734375 LR: 0.00002463 \n",
            "[2][319/781] Elapsed: 1m 44s (remain 2m 31s) Loss: 0.0790(0.0990) Grad: 122321.4296875 LR: 0.00002426 \n",
            "[2][339/781] Elapsed: 1m 51s (remain 2m 24s) Loss: 0.1109(0.0990) Grad: 194679.0625 LR: 0.00002390 \n",
            "[2][359/781] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0716(0.0984) Grad: 100335.7578125 LR: 0.00002354 \n",
            "[2][379/781] Elapsed: 2m 4s (remain 2m 11s) Loss: 0.0717(0.0985) Grad: 97341.2421875 LR: 0.00002319 \n",
            "[2][399/781] Elapsed: 2m 11s (remain 2m 4s) Loss: 0.0592(0.0980) Grad: 66883.875 LR: 0.00002285 \n",
            "[2][419/781] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.0964(0.0979) Grad: 57276.3359375 LR: 0.00002251 \n",
            "[2][439/781] Elapsed: 2m 24s (remain 1m 51s) Loss: 0.0902(0.0983) Grad: 87636.578125 LR: 0.00002217 \n",
            "[2][459/781] Elapsed: 2m 30s (remain 1m 45s) Loss: 0.0671(0.0978) Grad: 101582.484375 LR: 0.00002184 \n",
            "[2][479/781] Elapsed: 2m 37s (remain 1m 38s) Loss: 0.1293(0.0978) Grad: 90824.78125 LR: 0.00002151 \n",
            "[2][499/781] Elapsed: 2m 43s (remain 1m 32s) Loss: 0.0883(0.0978) Grad: 74202.921875 LR: 0.00002119 \n",
            "[2][519/781] Elapsed: 2m 50s (remain 1m 25s) Loss: 0.1382(0.0981) Grad: 124278.0390625 LR: 0.00002087 \n",
            "[2][539/781] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0528(0.0979) Grad: 73570.9140625 LR: 0.00002056 \n",
            "[2][559/781] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.1066(0.0980) Grad: 196511.40625 LR: 0.00002025 \n",
            "[2][579/781] Elapsed: 3m 10s (remain 1m 5s) Loss: 0.0756(0.0984) Grad: 80317.59375 LR: 0.00001995 \n",
            "[2][599/781] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.1049(0.0985) Grad: 99867.6171875 LR: 0.00001965 \n",
            "[2][619/781] Elapsed: 3m 23s (remain 0m 52s) Loss: 0.0560(0.0984) Grad: 68842.953125 LR: 0.00001936 \n",
            "[2][639/781] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.0777(0.0986) Grad: 76096.484375 LR: 0.00001907 \n",
            "[2][659/781] Elapsed: 3m 36s (remain 0m 39s) Loss: 0.1202(0.0987) Grad: 230315.828125 LR: 0.00001879 \n",
            "[2][679/781] Elapsed: 3m 42s (remain 0m 33s) Loss: 0.1104(0.0987) Grad: 73400.21875 LR: 0.00001851 \n",
            "[2][699/781] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.0907(0.0981) Grad: 176905.234375 LR: 0.00001823 \n",
            "[2][719/781] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1010(0.0983) Grad: 79293.9921875 LR: 0.00001796 \n",
            "[2][739/781] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.0924(0.0983) Grad: 126886.46875 LR: 0.00001769 \n",
            "[2][759/781] Elapsed: 4m 9s (remain 0m 6s) Loss: 0.0951(0.0986) Grad: 83922.2734375 LR: 0.00001742 \n",
            "[2][779/781] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.0998(0.0988) Grad: 166061.34375 LR: 0.00001716 \n",
            "[3][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0980(0.1008) \n",
            "[3][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0440(0.0958) \n",
            "[3][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0889(0.0964) \n",
            "[3][79/196] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1096(0.0945) \n",
            "[3][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1551(0.0992) \n",
            "[3][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1715(0.0992) \n",
            "[3][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0673(0.0986) \n",
            "[3][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1551(0.1000) \n",
            "[3][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.0929(0.0996) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3 - avg_train_loss = 0.0987 avg_val_loss: 0.0990 time: 297.7434s\n",
            "Epoch: 3 - Score: 0.4456, Scores: [0.4790586452255965, 0.43055562430017624, 0.42132747486038635, 0.4449015651666129, 0.46431622975409964, 0.4333692833062872]\n",
            "Epoch 3 - Save Best Score: 0.4456 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0706(0.0990) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/781] Elapsed: 0m 6s (remain 4m 15s) Loss: 0.1152(0.0922) Grad: 327664.375 LR: 0.00001690 \n",
            "[3][39/781] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0616(0.0950) Grad: 66711.359375 LR: 0.00001664 \n",
            "[3][59/781] Elapsed: 0m 19s (remain 3m 57s) Loss: 0.0600(0.0928) Grad: 80202.484375 LR: 0.00001639 \n",
            "[3][79/781] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.0757(0.0910) Grad: 95514.890625 LR: 0.00001615 \n",
            "[3][99/781] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.1236(0.0923) Grad: 119882.7890625 LR: 0.00001591 \n",
            "[3][119/781] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.0654(0.0931) Grad: 150375.25 LR: 0.00001567 \n",
            "[3][139/781] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.1172(0.0917) Grad: 102906.328125 LR: 0.00001544 \n",
            "[3][159/781] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.2946(0.0927) Grad: 175897.296875 LR: 0.00001521 \n",
            "[3][179/781] Elapsed: 0m 59s (remain 3m 17s) Loss: 0.1075(0.0933) Grad: 102878.9296875 LR: 0.00001498 \n",
            "[3][199/781] Elapsed: 1m 5s (remain 3m 10s) Loss: 0.1521(0.0942) Grad: 160999.96875 LR: 0.00001476 \n",
            "[3][219/781] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0872(0.0942) Grad: 122524.6484375 LR: 0.00001453 \n",
            "[3][239/781] Elapsed: 1m 18s (remain 2m 57s) Loss: 0.0578(0.0940) Grad: 64528.9296875 LR: 0.00001432 \n",
            "[3][259/781] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.0778(0.0938) Grad: 87142.3671875 LR: 0.00001410 \n",
            "[3][279/781] Elapsed: 1m 31s (remain 2m 44s) Loss: 0.0603(0.0946) Grad: 69602.4921875 LR: 0.00001389 \n",
            "[3][299/781] Elapsed: 1m 38s (remain 2m 37s) Loss: 0.0917(0.0946) Grad: 225297.78125 LR: 0.00001369 \n",
            "[3][319/781] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.0410(0.0949) Grad: 56176.87109375 LR: 0.00001348 \n",
            "[3][339/781] Elapsed: 1m 51s (remain 2m 24s) Loss: 0.0601(0.0940) Grad: 85643.546875 LR: 0.00001328 \n",
            "[3][359/781] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0609(0.0940) Grad: 104196.453125 LR: 0.00001308 \n",
            "[3][379/781] Elapsed: 2m 4s (remain 2m 11s) Loss: 0.0662(0.0937) Grad: 79719.53125 LR: 0.00001289 \n",
            "[3][399/781] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0671(0.0940) Grad: 76356.234375 LR: 0.00001269 \n",
            "[3][419/781] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.0998(0.0947) Grad: 94926.265625 LR: 0.00001250 \n",
            "[3][439/781] Elapsed: 2m 24s (remain 1m 51s) Loss: 0.0768(0.0946) Grad: 147861.59375 LR: 0.00001232 \n",
            "[3][459/781] Elapsed: 2m 30s (remain 1m 45s) Loss: 0.0621(0.0942) Grad: 72271.96875 LR: 0.00001213 \n",
            "[3][479/781] Elapsed: 2m 37s (remain 1m 38s) Loss: 0.0799(0.0946) Grad: 60220.765625 LR: 0.00001195 \n",
            "[3][499/781] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.1389(0.0946) Grad: 165151.875 LR: 0.00001177 \n",
            "[3][519/781] Elapsed: 2m 50s (remain 1m 25s) Loss: 0.1218(0.0950) Grad: 106610.078125 LR: 0.00001160 \n",
            "[3][539/781] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0666(0.0951) Grad: 85164.96875 LR: 0.00001142 \n",
            "[3][559/781] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.1330(0.0952) Grad: 68603.1953125 LR: 0.00001125 \n",
            "[3][579/781] Elapsed: 3m 10s (remain 1m 5s) Loss: 0.0620(0.0948) Grad: 60678.49609375 LR: 0.00001109 \n",
            "[3][599/781] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.1453(0.0946) Grad: 109928.203125 LR: 0.00001092 \n",
            "[3][619/781] Elapsed: 3m 23s (remain 0m 52s) Loss: 0.1222(0.0947) Grad: 106552.1875 LR: 0.00001076 \n",
            "[3][639/781] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.1970(0.0945) Grad: 126087.484375 LR: 0.00001060 \n",
            "[3][659/781] Elapsed: 3m 36s (remain 0m 39s) Loss: 0.0732(0.0946) Grad: 144253.921875 LR: 0.00001044 \n",
            "[3][679/781] Elapsed: 3m 42s (remain 0m 33s) Loss: 0.1082(0.0946) Grad: 131109.140625 LR: 0.00001028 \n",
            "[3][699/781] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.0988(0.0945) Grad: 216450.6875 LR: 0.00001013 \n",
            "[3][719/781] Elapsed: 3m 56s (remain 0m 19s) Loss: 0.0762(0.0947) Grad: 93126.71875 LR: 0.00000998 \n",
            "[3][739/781] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.0634(0.0946) Grad: 64324.21875 LR: 0.00000983 \n",
            "[3][759/781] Elapsed: 4m 9s (remain 0m 6s) Loss: 0.1079(0.0947) Grad: 143706.46875 LR: 0.00000968 \n",
            "[3][779/781] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.1028(0.0946) Grad: 123874.7265625 LR: 0.00000954 \n",
            "[4][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0993(0.1037) \n",
            "[4][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0475(0.0989) \n",
            "[4][59/196] Elapsed: 0m 12s (remain 0m 29s) Loss: 0.1072(0.0971) \n",
            "[4][79/196] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.1069(0.0953) \n",
            "[4][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1516(0.0984) \n",
            "[4][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1419(0.0980) \n",
            "[4][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0721(0.0973) \n",
            "[4][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1552(0.0985) \n",
            "[4][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.0928(0.0988) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4 - avg_train_loss = 0.0946 avg_val_loss: 0.0983 time: 297.7130s\n",
            "Epoch: 4 - Score: 0.4441, Scores: [0.4736707829906585, 0.42938793687023075, 0.4202417231161282, 0.44035164741174343, 0.4667670276135549, 0.4341776676975357]\n",
            "Epoch 4 - Save Best Score: 0.4441 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0614(0.0983) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/781] Elapsed: 0m 6s (remain 4m 11s) Loss: 0.0829(0.1037) Grad: 90863.09375 LR: 0.00000939 \n",
            "[4][39/781] Elapsed: 0m 13s (remain 4m 3s) Loss: 0.0479(0.1060) Grad: 49258.28515625 LR: 0.00000925 \n",
            "[4][59/781] Elapsed: 0m 19s (remain 3m 57s) Loss: 0.0521(0.0972) Grad: 118525.171875 LR: 0.00000911 \n",
            "[4][79/781] Elapsed: 0m 26s (remain 3m 50s) Loss: 0.0981(0.0964) Grad: 159583.515625 LR: 0.00000897 \n",
            "[4][99/781] Elapsed: 0m 32s (remain 3m 43s) Loss: 0.0740(0.0971) Grad: 65310.47265625 LR: 0.00000884 \n",
            "[4][119/781] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.0663(0.0970) Grad: 76519.796875 LR: 0.00000871 \n",
            "[4][139/781] Elapsed: 0m 46s (remain 3m 30s) Loss: 0.1329(0.0960) Grad: 201872.25 LR: 0.00000858 \n",
            "[4][159/781] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.0401(0.0962) Grad: 48993.49609375 LR: 0.00000845 \n",
            "[4][179/781] Elapsed: 0m 59s (remain 3m 17s) Loss: 0.0941(0.0964) Grad: 150804.921875 LR: 0.00000832 \n",
            "[4][199/781] Elapsed: 1m 5s (remain 3m 10s) Loss: 0.0612(0.0951) Grad: 114337.953125 LR: 0.00000820 \n",
            "[4][219/781] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0829(0.0948) Grad: 102145.0390625 LR: 0.00000808 \n",
            "[4][239/781] Elapsed: 1m 18s (remain 2m 57s) Loss: 0.0861(0.0954) Grad: 109126.109375 LR: 0.00000796 \n",
            "[4][259/781] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.0885(0.0954) Grad: 190009.828125 LR: 0.00000784 \n",
            "[4][279/781] Elapsed: 1m 31s (remain 2m 44s) Loss: 0.0897(0.0952) Grad: 134605.34375 LR: 0.00000772 \n",
            "[4][299/781] Elapsed: 1m 38s (remain 2m 37s) Loss: 0.0496(0.0944) Grad: 49200.09765625 LR: 0.00000760 \n",
            "[4][319/781] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.0771(0.0956) Grad: 89973.265625 LR: 0.00000749 \n",
            "[4][339/781] Elapsed: 1m 51s (remain 2m 24s) Loss: 0.1331(0.0953) Grad: 262735.78125 LR: 0.00000738 \n",
            "[4][359/781] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0911(0.0950) Grad: 137451.28125 LR: 0.00000727 \n",
            "[4][379/781] Elapsed: 2m 4s (remain 2m 11s) Loss: 0.1174(0.0946) Grad: 87315.359375 LR: 0.00000716 \n",
            "[4][399/781] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0549(0.0945) Grad: 81915.21875 LR: 0.00000705 \n",
            "[4][419/781] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.0856(0.0940) Grad: 100069.7890625 LR: 0.00000695 \n",
            "[4][439/781] Elapsed: 2m 24s (remain 1m 51s) Loss: 0.0869(0.0938) Grad: 67928.5234375 LR: 0.00000684 \n",
            "[4][459/781] Elapsed: 2m 30s (remain 1m 45s) Loss: 0.0846(0.0937) Grad: 98824.390625 LR: 0.00000674 \n",
            "[4][479/781] Elapsed: 2m 37s (remain 1m 38s) Loss: 0.0685(0.0935) Grad: 66643.7734375 LR: 0.00000664 \n",
            "[4][499/781] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0696(0.0935) Grad: 74365.6015625 LR: 0.00000654 \n",
            "[4][519/781] Elapsed: 2m 50s (remain 1m 25s) Loss: 0.0578(0.0932) Grad: 62064.6015625 LR: 0.00000644 \n",
            "[4][539/781] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0887(0.0937) Grad: 145848.453125 LR: 0.00000635 \n",
            "[4][559/781] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.0534(0.0933) Grad: 59554.7421875 LR: 0.00000625 \n",
            "[4][579/781] Elapsed: 3m 10s (remain 1m 5s) Loss: 0.1013(0.0933) Grad: 91901.84375 LR: 0.00000616 \n",
            "[4][599/781] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.0592(0.0931) Grad: 116626.9609375 LR: 0.00000607 \n",
            "[4][619/781] Elapsed: 3m 23s (remain 0m 52s) Loss: 0.1296(0.0930) Grad: 162186.640625 LR: 0.00000598 \n",
            "[4][639/781] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.0837(0.0933) Grad: 64218.6015625 LR: 0.00000589 \n",
            "[4][659/781] Elapsed: 3m 36s (remain 0m 39s) Loss: 0.0965(0.0931) Grad: 234081.96875 LR: 0.00000580 \n",
            "[4][679/781] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0535(0.0928) Grad: 88459.984375 LR: 0.00000571 \n",
            "[4][699/781] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.1266(0.0931) Grad: 95051.4921875 LR: 0.00000563 \n",
            "[4][719/781] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.0505(0.0927) Grad: 93171.3828125 LR: 0.00000554 \n",
            "[4][739/781] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.0746(0.0930) Grad: 89839.078125 LR: 0.00000546 \n",
            "[4][759/781] Elapsed: 4m 9s (remain 0m 6s) Loss: 0.0642(0.0930) Grad: 137925.125 LR: 0.00000538 \n",
            "[4][779/781] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.0676(0.0929) Grad: 95300.5234375 LR: 0.00000530 \n",
            "[5][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0993(0.1000) \n",
            "[5][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0443(0.0951) \n",
            "[5][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0947(0.0955) \n",
            "[5][79/196] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1052(0.0935) \n",
            "[5][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1530(0.0979) \n",
            "[5][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1558(0.0979) \n",
            "[5][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0663(0.0972) \n",
            "[5][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1567(0.0985) \n",
            "[5][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.0935(0.0983) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5 - avg_train_loss = 0.0929 avg_val_loss: 0.0978 time: 297.8464s\n",
            "Epoch: 5 - Score: 0.4430, Scores: [0.4734103256591635, 0.42552829638480516, 0.4182834201781216, 0.4423315380546638, 0.4627444445106863, 0.4357072294566703]\n",
            "Epoch 5 - Save Best Score: 0.4430 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0674(0.0978) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = igetitem(value, i)\n",
            "========== fold: 1 result ==========\n",
            "Score: 0.4430  Scores: [0.4734103256591635, 0.42552829638480516, 0.4182834201781216, 0.4423315380546638, 0.4627444445106863, 0.4357072294566703]\n",
            "========== fold: 2 training ==========\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<class '__main__.CFG'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] num_train_steps:  3907  len train loader:  782\n",
            "warmup start:  19535\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/782] Elapsed: 0m 6s (remain 4m 14s) Loss: 0.2763(1.6272) Grad: 139481.015625 LR: 0.00009851 \n",
            "[0][39/782] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.1594(0.9347) Grad: 162874.875 LR: 0.00009703 \n",
            "[0][59/782] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.1380(0.6809) Grad: 126774.734375 LR: 0.00009559 \n",
            "[0][79/782] Elapsed: 0m 26s (remain 3m 52s) Loss: 0.1328(0.5568) Grad: 119748.4921875 LR: 0.00009416 \n",
            "[0][99/782] Elapsed: 0m 33s (remain 3m 45s) Loss: 0.1152(0.4767) Grad: 138125.171875 LR: 0.00009275 \n",
            "[0][119/782] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.0790(0.4177) Grad: 141898.109375 LR: 0.00009137 \n",
            "[0][139/782] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.1572(0.3735) Grad: 108409.9921875 LR: 0.00009000 \n",
            "[0][159/782] Elapsed: 0m 52s (remain 3m 25s) Loss: 0.0918(0.3425) Grad: 77075.0546875 LR: 0.00008866 \n",
            "[0][179/782] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.0949(0.3220) Grad: 102633.984375 LR: 0.00008733 \n",
            "[0][199/782] Elapsed: 1m 6s (remain 3m 12s) Loss: 0.0642(0.3017) Grad: 104605.8828125 LR: 0.00008603 \n",
            "[0][219/782] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.1002(0.2846) Grad: 192453.78125 LR: 0.00008474 \n",
            "[0][239/782] Elapsed: 1m 19s (remain 2m 58s) Loss: 0.0820(0.2709) Grad: 117613.6953125 LR: 0.00008348 \n",
            "[0][259/782] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.0968(0.2602) Grad: 122214.1015625 LR: 0.00008223 \n",
            "[0][279/782] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0992(0.2509) Grad: 91650.0546875 LR: 0.00008100 \n",
            "[0][299/782] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0952(0.2416) Grad: 129546.828125 LR: 0.00007979 \n",
            "[0][319/782] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0718(0.2328) Grad: 91843.3515625 LR: 0.00007860 \n",
            "[0][339/782] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.1397(0.2269) Grad: 87293.3046875 LR: 0.00007743 \n",
            "[0][359/782] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.2108(0.2205) Grad: 225976.109375 LR: 0.00007627 \n",
            "[0][379/782] Elapsed: 2m 5s (remain 2m 12s) Loss: 0.1107(0.2151) Grad: 193154.15625 LR: 0.00007513 \n",
            "[0][399/782] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.1707(0.2108) Grad: 95693.0 LR: 0.00007401 \n",
            "[0][419/782] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.1856(0.2065) Grad: 123842.3984375 LR: 0.00007290 \n",
            "[0][439/782] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.0957(0.2029) Grad: 143860.359375 LR: 0.00007181 \n",
            "[0][459/782] Elapsed: 2m 31s (remain 1m 45s) Loss: 0.0981(0.1998) Grad: 83530.8984375 LR: 0.00007074 \n",
            "[0][479/782] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0910(0.1964) Grad: 93870.21875 LR: 0.00006968 \n",
            "[0][499/782] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.1176(0.1936) Grad: 128531.484375 LR: 0.00006864 \n",
            "[0][519/782] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0793(0.1911) Grad: 76123.09375 LR: 0.00006762 \n",
            "[0][539/782] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0753(0.1882) Grad: 113832.4609375 LR: 0.00006661 \n",
            "[0][559/782] Elapsed: 3m 4s (remain 1m 12s) Loss: 0.1396(0.1854) Grad: 104995.1953125 LR: 0.00006561 \n",
            "[0][579/782] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0970(0.1830) Grad: 105521.8046875 LR: 0.00006463 \n",
            "[0][599/782] Elapsed: 3m 17s (remain 0m 59s) Loss: 0.1281(0.1804) Grad: 132622.890625 LR: 0.00006367 \n",
            "[0][619/782] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.1328(0.1785) Grad: 226239.125 LR: 0.00006272 \n",
            "[0][639/782] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.1475(0.1767) Grad: 166169.75 LR: 0.00006178 \n",
            "[0][659/782] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.2198(0.1753) Grad: 166073.796875 LR: 0.00006086 \n",
            "[0][679/782] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.1392(0.1735) Grad: 176972.140625 LR: 0.00005995 \n",
            "[0][699/782] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.1751(0.1715) Grad: 210770.953125 LR: 0.00005905 \n",
            "[0][719/782] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1471(0.1698) Grad: 88943.1328125 LR: 0.00005817 \n",
            "[0][739/782] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.1396(0.1679) Grad: 251492.671875 LR: 0.00005730 \n",
            "[0][759/782] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.0483(0.1666) Grad: 51344.62890625 LR: 0.00005645 \n",
            "[0][779/782] Elapsed: 4m 16s (remain 0m 0s) Loss: 0.1295(0.1655) Grad: 178931.40625 LR: 0.00005560 \n",
            "[1][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0825(0.1042) \n",
            "[1][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1248(0.1038) \n",
            "[1][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0965(0.1060) \n",
            "[1][79/196] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0958(0.1045) \n",
            "[1][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0495(0.1046) \n",
            "[1][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1416(0.1095) \n",
            "[1][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1197(0.1105) \n",
            "[1][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0897(0.1105) \n",
            "[1][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1412(0.1106) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1 - avg_train_loss = 0.1654 avg_val_loss: 0.1110 time: 298.3020s\n",
            "Epoch: 1 - Score: 0.4724, Scores: [0.5000862978257119, 0.48761364342271896, 0.4364122382913792, 0.45969248007341557, 0.46273176630682505, 0.4880066643466378]\n",
            "Epoch 1 - Save Best Score: 0.4724 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1111(0.1110) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/782] Elapsed: 0m 6s (remain 4m 14s) Loss: 0.0769(0.1150) Grad: 357628.96875 LR: 0.00005469 \n",
            "[1][39/782] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.0884(0.1144) Grad: 308793.8125 LR: 0.00005387 \n",
            "[1][59/782] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.1151(0.1127) Grad: 116407.8984375 LR: 0.00005307 \n",
            "[1][79/782] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.0897(0.1120) Grad: 308237.6875 LR: 0.00005228 \n",
            "[1][99/782] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.1243(0.1114) Grad: 368467.28125 LR: 0.00005149 \n",
            "[1][119/782] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.1760(0.1116) Grad: 385197.65625 LR: 0.00005073 \n",
            "[1][139/782] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.1045(0.1094) Grad: 209357.78125 LR: 0.00004997 \n",
            "[1][159/782] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.0846(0.1095) Grad: 248270.40625 LR: 0.00004922 \n",
            "[1][179/782] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.1336(0.1078) Grad: 318971.46875 LR: 0.00004849 \n",
            "[1][199/782] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.0830(0.1069) Grad: 154737.609375 LR: 0.00004776 \n",
            "[1][219/782] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0907(0.1068) Grad: 339471.53125 LR: 0.00004705 \n",
            "[1][239/782] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.0964(0.1067) Grad: 221198.078125 LR: 0.00004635 \n",
            "[1][259/782] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.0990(0.1064) Grad: 137698.078125 LR: 0.00004565 \n",
            "[1][279/782] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0863(0.1060) Grad: 236820.890625 LR: 0.00004497 \n",
            "[1][299/782] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0649(0.1051) Grad: 164026.21875 LR: 0.00004430 \n",
            "[1][319/782] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.1039(0.1045) Grad: 163447.34375 LR: 0.00004364 \n",
            "[1][339/782] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.1011(0.1045) Grad: 307138.3125 LR: 0.00004299 \n",
            "[1][359/782] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.1153(0.1051) Grad: 145998.796875 LR: 0.00004234 \n",
            "[1][379/782] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.1333(0.1052) Grad: 288386.84375 LR: 0.00004171 \n",
            "[1][399/782] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0625(0.1060) Grad: 161271.453125 LR: 0.00004109 \n",
            "[1][419/782] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.0876(0.1054) Grad: 259888.90625 LR: 0.00004048 \n",
            "[1][439/782] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.1384(0.1057) Grad: 281820.46875 LR: 0.00003987 \n",
            "[1][459/782] Elapsed: 2m 31s (remain 1m 45s) Loss: 0.1057(0.1058) Grad: 174282.578125 LR: 0.00003927 \n",
            "[1][479/782] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0622(0.1060) Grad: 161061.65625 LR: 0.00003869 \n",
            "[1][499/782] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0559(0.1057) Grad: 135432.484375 LR: 0.00003811 \n",
            "[1][519/782] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0791(0.1055) Grad: 197578.640625 LR: 0.00003754 \n",
            "[1][539/782] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0835(0.1056) Grad: 207264.53125 LR: 0.00003698 \n",
            "[1][559/782] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.1504(0.1053) Grad: 280028.75 LR: 0.00003643 \n",
            "[1][579/782] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.1083(0.1053) Grad: 275003.0625 LR: 0.00003588 \n",
            "[1][599/782] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.1082(0.1053) Grad: 138103.78125 LR: 0.00003535 \n",
            "[1][619/782] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.0748(0.1047) Grad: 203346.03125 LR: 0.00003482 \n",
            "[1][639/782] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.1475(0.1043) Grad: 233691.875 LR: 0.00003430 \n",
            "[1][659/782] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.0771(0.1041) Grad: 191901.09375 LR: 0.00003379 \n",
            "[1][679/782] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0820(0.1037) Grad: 281593.75 LR: 0.00003328 \n",
            "[1][699/782] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.1528(0.1039) Grad: 508111.5625 LR: 0.00003279 \n",
            "[1][719/782] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.0579(0.1033) Grad: 96934.109375 LR: 0.00003230 \n",
            "[1][739/782] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.0775(0.1034) Grad: 270118.84375 LR: 0.00003181 \n",
            "[1][759/782] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.1069(0.1036) Grad: 168846.609375 LR: 0.00003134 \n",
            "[1][779/782] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.0878(0.1039) Grad: 183604.953125 LR: 0.00003087 \n",
            "[2][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0776(0.1000) \n",
            "[2][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0934(0.0997) \n",
            "[2][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1030(0.1019) \n",
            "[2][79/196] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.0679(0.0998) \n",
            "[2][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0497(0.0996) \n",
            "[2][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1246(0.1022) \n",
            "[2][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1243(0.1026) \n",
            "[2][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0840(0.1025) \n",
            "[2][179/196] Elapsed: 0m 37s (remain 0m 3s) Loss: 0.1323(0.1023) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2 - avg_train_loss = 0.1039 avg_val_loss: 0.1029 time: 298.0412s\n",
            "Epoch: 2 - Score: 0.4543, Scores: [0.4900083865375496, 0.45070331378141276, 0.42236506296865556, 0.45445004293205954, 0.45352492803203986, 0.45495289515646203]\n",
            "Epoch 2 - Save Best Score: 0.4543 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1404(0.1029) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/782] Elapsed: 0m 6s (remain 4m 16s) Loss: 0.1759(0.1156) Grad: 190636.609375 LR: 0.00003036 \n",
            "[2][39/782] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.1160(0.1111) Grad: 394310.65625 LR: 0.00002991 \n",
            "[2][59/782] Elapsed: 0m 19s (remain 3m 59s) Loss: 0.0911(0.1076) Grad: 262988.65625 LR: 0.00002946 \n",
            "[2][79/782] Elapsed: 0m 26s (remain 3m 52s) Loss: 0.1639(0.1100) Grad: 489335.09375 LR: 0.00002902 \n",
            "[2][99/782] Elapsed: 0m 33s (remain 3m 45s) Loss: 0.1334(0.1061) Grad: 174252.4375 LR: 0.00002859 \n",
            "[2][119/782] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.0742(0.1064) Grad: 227706.171875 LR: 0.00002816 \n",
            "[2][139/782] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.0701(0.1054) Grad: 145591.9375 LR: 0.00002774 \n",
            "[2][159/782] Elapsed: 0m 52s (remain 3m 25s) Loss: 0.1010(0.1032) Grad: 151774.765625 LR: 0.00002733 \n",
            "[2][179/782] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.0944(0.1003) Grad: 213569.65625 LR: 0.00002692 \n",
            "[2][199/782] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.1055(0.1008) Grad: 108302.375 LR: 0.00002652 \n",
            "[2][219/782] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.1237(0.1011) Grad: 506822.96875 LR: 0.00002612 \n",
            "[2][239/782] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.1164(0.1001) Grad: 342804.25 LR: 0.00002573 \n",
            "[2][259/782] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.0674(0.1001) Grad: 171658.296875 LR: 0.00002535 \n",
            "[2][279/782] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0584(0.0995) Grad: 136135.234375 LR: 0.00002497 \n",
            "[2][299/782] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.1655(0.0990) Grad: 532993.3125 LR: 0.00002459 \n",
            "[2][319/782] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.1353(0.0986) Grad: 391423.96875 LR: 0.00002423 \n",
            "[2][339/782] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.0790(0.0989) Grad: 237493.421875 LR: 0.00002387 \n",
            "[2][359/782] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0574(0.0992) Grad: 125217.0390625 LR: 0.00002351 \n",
            "[2][379/782] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.1647(0.0989) Grad: 365381.9375 LR: 0.00002316 \n",
            "[2][399/782] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0451(0.0988) Grad: 64581.4296875 LR: 0.00002281 \n",
            "[2][419/782] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.0499(0.0982) Grad: 232965.15625 LR: 0.00002247 \n",
            "[2][439/782] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.0662(0.0987) Grad: 220260.03125 LR: 0.00002214 \n",
            "[2][459/782] Elapsed: 2m 31s (remain 1m 45s) Loss: 0.0668(0.0987) Grad: 191667.234375 LR: 0.00002181 \n",
            "[2][479/782] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0767(0.0989) Grad: 150701.828125 LR: 0.00002148 \n",
            "[2][499/782] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0776(0.0983) Grad: 206040.78125 LR: 0.00002116 \n",
            "[2][519/782] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0904(0.0981) Grad: 289777.21875 LR: 0.00002084 \n",
            "[2][539/782] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.1095(0.0977) Grad: 322223.9375 LR: 0.00002053 \n",
            "[2][559/782] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.0616(0.0980) Grad: 179836.53125 LR: 0.00002022 \n",
            "[2][579/782] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0839(0.0982) Grad: 176518.328125 LR: 0.00001992 \n",
            "[2][599/782] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.1099(0.0982) Grad: 148595.546875 LR: 0.00001962 \n",
            "[2][619/782] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.1470(0.0983) Grad: 205200.71875 LR: 0.00001933 \n",
            "[2][639/782] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.0654(0.0983) Grad: 236199.796875 LR: 0.00001904 \n",
            "[2][659/782] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.0936(0.0981) Grad: 331133.0625 LR: 0.00001876 \n",
            "[2][679/782] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.1616(0.0986) Grad: 165515.1875 LR: 0.00001848 \n",
            "[2][699/782] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.0967(0.0989) Grad: 200811.671875 LR: 0.00001820 \n",
            "[2][719/782] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1259(0.0991) Grad: 147183.921875 LR: 0.00001793 \n",
            "[2][739/782] Elapsed: 4m 2s (remain 0m 13s) Loss: 0.1151(0.0990) Grad: 135814.859375 LR: 0.00001766 \n",
            "[2][759/782] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.0506(0.0986) Grad: 131732.140625 LR: 0.00001740 \n",
            "[2][779/782] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.1346(0.0990) Grad: 353185.5 LR: 0.00001714 \n",
            "[3][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0880(0.0991) \n",
            "[3][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0962(0.0999) \n",
            "[3][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0971(0.1016) \n",
            "[3][79/196] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0756(0.0993) \n",
            "[3][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0551(0.0998) \n",
            "[3][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1156(0.1027) \n",
            "[3][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1227(0.1036) \n",
            "[3][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0896(0.1033) \n",
            "[3][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1284(0.1027) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3 - avg_train_loss = 0.0991 avg_val_loss: 0.1033 time: 298.1390s\n",
            "Epoch: 3 - Score: 0.4553, Scores: [0.4917058424104645, 0.46389898009901875, 0.4238800920005655, 0.45125950044803037, 0.4532839105008448, 0.44749010839546305]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1103(0.1033) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/782] Elapsed: 0m 6s (remain 4m 12s) Loss: 0.0605(0.0894) Grad: 145400.921875 LR: 0.00001686 \n",
            "[3][39/782] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0591(0.0894) Grad: 216544.984375 LR: 0.00001661 \n",
            "[3][59/782] Elapsed: 0m 19s (remain 3m 57s) Loss: 0.1292(0.0927) Grad: 153514.546875 LR: 0.00001636 \n",
            "[3][79/782] Elapsed: 0m 26s (remain 3m 50s) Loss: 0.0797(0.0969) Grad: 203419.421875 LR: 0.00001611 \n",
            "[3][99/782] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.0987(0.0963) Grad: 215018.734375 LR: 0.00001587 \n",
            "[3][119/782] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.0799(0.0963) Grad: 174714.234375 LR: 0.00001564 \n",
            "[3][139/782] Elapsed: 0m 45s (remain 3m 30s) Loss: 0.0781(0.0953) Grad: 299232.53125 LR: 0.00001540 \n",
            "[3][159/782] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.0553(0.0956) Grad: 98104.8125 LR: 0.00001517 \n",
            "[3][179/782] Elapsed: 0m 59s (remain 3m 17s) Loss: 0.1411(0.0963) Grad: 118730.90625 LR: 0.00001495 \n",
            "[3][199/782] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.1170(0.0968) Grad: 238887.53125 LR: 0.00001472 \n",
            "[3][219/782] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0896(0.0980) Grad: 253199.375 LR: 0.00001450 \n",
            "[3][239/782] Elapsed: 1m 18s (remain 2m 57s) Loss: 0.0472(0.0973) Grad: 98982.9375 LR: 0.00001429 \n",
            "[3][259/782] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1248(0.0971) Grad: 547862.3125 LR: 0.00001407 \n",
            "[3][279/782] Elapsed: 1m 31s (remain 2m 44s) Loss: 0.0838(0.0967) Grad: 312939.625 LR: 0.00001386 \n",
            "[3][299/782] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0483(0.0965) Grad: 112156.09375 LR: 0.00001365 \n",
            "[3][319/782] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.0712(0.0958) Grad: 180368.96875 LR: 0.00001345 \n",
            "[3][339/782] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.1490(0.0956) Grad: 209819.15625 LR: 0.00001325 \n",
            "[3][359/782] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.1521(0.0965) Grad: 261208.71875 LR: 0.00001305 \n",
            "[3][379/782] Elapsed: 2m 4s (remain 2m 11s) Loss: 0.1062(0.0970) Grad: 334581.25 LR: 0.00001286 \n",
            "[3][399/782] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.1011(0.0970) Grad: 194102.875 LR: 0.00001267 \n",
            "[3][419/782] Elapsed: 2m 17s (remain 1m 58s) Loss: 0.0475(0.0969) Grad: 127652.703125 LR: 0.00001248 \n",
            "[3][439/782] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.1133(0.0971) Grad: 113761.0859375 LR: 0.00001229 \n",
            "[3][459/782] Elapsed: 2m 30s (remain 1m 45s) Loss: 0.0634(0.0965) Grad: 181797.015625 LR: 0.00001211 \n",
            "[3][479/782] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0701(0.0964) Grad: 116437.7890625 LR: 0.00001193 \n",
            "[3][499/782] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.2315(0.0963) Grad: 472591.4375 LR: 0.00001175 \n",
            "[3][519/782] Elapsed: 2m 50s (remain 1m 25s) Loss: 0.0770(0.0961) Grad: 157651.609375 LR: 0.00001157 \n",
            "[3][539/782] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.1277(0.0965) Grad: 317392.0625 LR: 0.00001140 \n",
            "[3][559/782] Elapsed: 3m 3s (remain 1m 12s) Loss: 0.1587(0.0962) Grad: 393336.25 LR: 0.00001123 \n",
            "[3][579/782] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0541(0.0958) Grad: 129428.4609375 LR: 0.00001106 \n",
            "[3][599/782] Elapsed: 3m 16s (remain 0m 59s) Loss: 0.0994(0.0958) Grad: 258844.078125 LR: 0.00001090 \n",
            "[3][619/782] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.0823(0.0961) Grad: 172559.28125 LR: 0.00001073 \n",
            "[3][639/782] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.0448(0.0960) Grad: 143407.40625 LR: 0.00001057 \n",
            "[3][659/782] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.0940(0.0961) Grad: 192611.4375 LR: 0.00001041 \n",
            "[3][679/782] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0846(0.0960) Grad: 163013.9375 LR: 0.00001026 \n",
            "[3][699/782] Elapsed: 3m 49s (remain 0m 26s) Loss: 0.1070(0.0961) Grad: 213868.0 LR: 0.00001011 \n",
            "[3][719/782] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.2232(0.0962) Grad: 526902.5 LR: 0.00000995 \n",
            "[3][739/782] Elapsed: 4m 3s (remain 0m 13s) Loss: 0.1138(0.0962) Grad: 119663.921875 LR: 0.00000981 \n",
            "[3][759/782] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.0635(0.0964) Grad: 237727.25 LR: 0.00000966 \n",
            "[3][779/782] Elapsed: 4m 16s (remain 0m 0s) Loss: 0.1675(0.0965) Grad: 442566.8125 LR: 0.00000952 \n",
            "[4][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0612(0.0987) \n",
            "[4][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0802(0.0986) \n",
            "[4][59/196] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1032(0.1026) \n",
            "[4][79/196] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0598(0.1001) \n",
            "[4][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0432(0.0997) \n",
            "[4][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1106(0.1026) \n",
            "[4][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1404(0.1026) \n",
            "[4][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0853(0.1028) \n",
            "[4][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1476(0.1025) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4 - avg_train_loss = 0.0965 avg_val_loss: 0.1031 time: 298.3916s\n",
            "Epoch: 4 - Score: 0.4547, Scores: [0.4943872017772749, 0.4502623733334951, 0.42539271280034296, 0.4522721873940964, 0.4552923051960827, 0.45040881957766005]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1318(0.1031) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/782] Elapsed: 0m 6s (remain 4m 13s) Loss: 0.0997(0.0939) Grad: 241405.6875 LR: 0.00000936 \n",
            "[4][39/782] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.1038(0.1007) Grad: 205772.8125 LR: 0.00000922 \n",
            "[4][59/782] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.0722(0.1032) Grad: 254737.15625 LR: 0.00000908 \n",
            "[4][79/782] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.2643(0.1010) Grad: 417431.28125 LR: 0.00000895 \n",
            "[4][99/782] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.0770(0.0992) Grad: 162549.578125 LR: 0.00000881 \n",
            "[4][119/782] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.1249(0.0999) Grad: 194257.0625 LR: 0.00000868 \n",
            "[4][139/782] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.0807(0.0972) Grad: 302809.8125 LR: 0.00000855 \n",
            "[4][159/782] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.1122(0.0965) Grad: 170424.078125 LR: 0.00000842 \n",
            "[4][179/782] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.0419(0.0950) Grad: 182939.765625 LR: 0.00000830 \n",
            "[4][199/782] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.2080(0.0954) Grad: 377484.4375 LR: 0.00000817 \n",
            "[4][219/782] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0648(0.0956) Grad: 103260.5078125 LR: 0.00000805 \n",
            "[4][239/782] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.1089(0.0954) Grad: 332307.78125 LR: 0.00000793 \n",
            "[4][259/782] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1166(0.0943) Grad: 276749.6875 LR: 0.00000781 \n",
            "[4][279/782] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0946(0.0948) Grad: 305235.59375 LR: 0.00000770 \n",
            "[4][299/782] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0962(0.0946) Grad: 326171.5625 LR: 0.00000758 \n",
            "[4][319/782] Elapsed: 1m 45s (remain 2m 31s) Loss: 0.1345(0.0941) Grad: 253263.578125 LR: 0.00000747 \n",
            "[4][339/782] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.1234(0.0944) Grad: 181656.96875 LR: 0.00000736 \n",
            "[4][359/782] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.1103(0.0946) Grad: 199587.875 LR: 0.00000725 \n",
            "[4][379/782] Elapsed: 2m 5s (remain 2m 12s) Loss: 0.1350(0.0943) Grad: 226357.1875 LR: 0.00000714 \n",
            "[4][399/782] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0866(0.0944) Grad: 293076.9375 LR: 0.00000703 \n",
            "[4][419/782] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0893(0.0941) Grad: 167721.6875 LR: 0.00000693 \n",
            "[4][439/782] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.0831(0.0946) Grad: 176466.65625 LR: 0.00000682 \n",
            "[4][459/782] Elapsed: 2m 31s (remain 1m 45s) Loss: 0.1035(0.0949) Grad: 265937.40625 LR: 0.00000672 \n",
            "[4][479/782] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0615(0.0953) Grad: 150819.125 LR: 0.00000662 \n",
            "[4][499/782] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0861(0.0953) Grad: 300625.125 LR: 0.00000652 \n",
            "[4][519/782] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.0972(0.0951) Grad: 193258.296875 LR: 0.00000642 \n",
            "[4][539/782] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.1920(0.0949) Grad: 572051.9375 LR: 0.00000633 \n",
            "[4][559/782] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0494(0.0947) Grad: 120153.109375 LR: 0.00000623 \n",
            "[4][579/782] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.1434(0.0949) Grad: 319475.5625 LR: 0.00000614 \n",
            "[4][599/782] Elapsed: 3m 17s (remain 0m 59s) Loss: 0.0905(0.0945) Grad: 195653.296875 LR: 0.00000605 \n",
            "[4][619/782] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.1307(0.0944) Grad: 336026.84375 LR: 0.00000596 \n",
            "[4][639/782] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.0571(0.0948) Grad: 105803.6796875 LR: 0.00000587 \n",
            "[4][659/782] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.0852(0.0951) Grad: 150156.515625 LR: 0.00000578 \n",
            "[4][679/782] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0636(0.0949) Grad: 171921.078125 LR: 0.00000570 \n",
            "[4][699/782] Elapsed: 3m 50s (remain 0m 26s) Loss: 0.0686(0.0950) Grad: 182924.78125 LR: 0.00000561 \n",
            "[4][719/782] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.0905(0.0952) Grad: 226594.40625 LR: 0.00000553 \n",
            "[4][739/782] Elapsed: 4m 3s (remain 0m 13s) Loss: 0.0732(0.0949) Grad: 173462.53125 LR: 0.00000544 \n",
            "[4][759/782] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0610(0.0948) Grad: 166276.703125 LR: 0.00000536 \n",
            "[4][779/782] Elapsed: 4m 16s (remain 0m 0s) Loss: 0.0975(0.0947) Grad: 309682.125 LR: 0.00000528 \n",
            "[5][19/196] Elapsed: 0m 4s (remain 0m 38s) Loss: 0.0824(0.0976) \n",
            "[5][39/196] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0898(0.0990) \n",
            "[5][59/196] Elapsed: 0m 12s (remain 0m 29s) Loss: 0.0893(0.1012) \n",
            "[5][79/196] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0721(0.0992) \n",
            "[5][99/196] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.0516(0.0997) \n",
            "[5][119/196] Elapsed: 0m 25s (remain 0m 16s) Loss: 0.1219(0.1022) \n",
            "[5][139/196] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1236(0.1027) \n",
            "[5][159/196] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0872(0.1025) \n",
            "[5][179/196] Elapsed: 0m 38s (remain 0m 3s) Loss: 0.1290(0.1021) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5 - avg_train_loss = 0.0947 avg_val_loss: 0.1028 time: 298.9048s\n",
            "Epoch: 5 - Score: 0.4543, Scores: [0.4891071863534419, 0.4500509331251393, 0.42075863414852477, 0.46153468289346894, 0.45529486499908906, 0.4490171107808261]\n",
            "Epoch 5 - Save Best Score: 0.4543 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5][196/196] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1142(0.1028) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = igetitem(value, i)\n",
            "========== fold: 2 result ==========\n",
            "Score: 0.4543  Scores: [0.4891071863534419, 0.4500509331251393, 0.42075863414852477, 0.46153468289346894, 0.45529486499908906, 0.4490171107808261]\n",
            "========== fold: 3 training ==========\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<class '__main__.CFG'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] num_train_steps:  3913  len train loader:  783\n",
            "warmup start:  19565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/783] Elapsed: 0m 6s (remain 4m 14s) Loss: 0.7854(1.9731) Grad: 289419.5625 LR: 0.00009851 \n",
            "[0][39/783] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.4203(1.1510) Grad: 360795.125 LR: 0.00009703 \n",
            "[0][59/783] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.1294(0.8301) Grad: 109793.9140625 LR: 0.00009559 \n",
            "[0][79/783] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.1230(0.6610) Grad: 122384.921875 LR: 0.00009416 \n",
            "[0][99/783] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.1458(0.5595) Grad: 110559.9453125 LR: 0.00009275 \n",
            "[0][119/783] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.1158(0.4865) Grad: 188259.9375 LR: 0.00009137 \n",
            "[0][139/783] Elapsed: 0m 45s (remain 3m 30s) Loss: 0.2285(0.4338) Grad: 178789.296875 LR: 0.00009000 \n",
            "[0][159/783] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.0807(0.3959) Grad: 52241.5 LR: 0.00008866 \n",
            "[0][179/783] Elapsed: 0m 59s (remain 3m 17s) Loss: 0.1681(0.3654) Grad: 118276.34375 LR: 0.00008733 \n",
            "[0][199/783] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.1345(0.3401) Grad: 239503.15625 LR: 0.00008603 \n",
            "[0][219/783] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0921(0.3201) Grad: 112427.234375 LR: 0.00008474 \n",
            "[0][239/783] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.1089(0.3029) Grad: 73611.828125 LR: 0.00008348 \n",
            "[0][259/783] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.2168(0.2887) Grad: 223339.90625 LR: 0.00008223 \n",
            "[0][279/783] Elapsed: 1m 31s (remain 2m 44s) Loss: 0.1609(0.2767) Grad: 132239.421875 LR: 0.00008100 \n",
            "[0][299/783] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0880(0.2668) Grad: 94733.6171875 LR: 0.00007979 \n",
            "[0][319/783] Elapsed: 1m 44s (remain 2m 31s) Loss: 0.0897(0.2568) Grad: 67119.6796875 LR: 0.00007860 \n",
            "[0][339/783] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.1123(0.2494) Grad: 113305.15625 LR: 0.00007743 \n",
            "[0][359/783] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0538(0.2412) Grad: 88535.9921875 LR: 0.00007627 \n",
            "[0][379/783] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.1182(0.2349) Grad: 119606.2265625 LR: 0.00007513 \n",
            "[0][399/783] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.1080(0.2283) Grad: 103193.7734375 LR: 0.00007401 \n",
            "[0][419/783] Elapsed: 2m 17s (remain 1m 59s) Loss: 0.1511(0.2236) Grad: 114061.6640625 LR: 0.00007290 \n",
            "[0][439/783] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.0799(0.2181) Grad: 126503.3046875 LR: 0.00007181 \n",
            "[0][459/783] Elapsed: 2m 30s (remain 1m 45s) Loss: 0.0959(0.2142) Grad: 119878.828125 LR: 0.00007074 \n",
            "[0][479/783] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.2437(0.2113) Grad: 206330.25 LR: 0.00006968 \n",
            "[0][499/783] Elapsed: 2m 43s (remain 1m 32s) Loss: 0.0531(0.2073) Grad: 78526.7265625 LR: 0.00006864 \n",
            "[0][519/783] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.1379(0.2036) Grad: 154359.859375 LR: 0.00006762 \n",
            "[0][539/783] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0707(0.1995) Grad: 66588.5234375 LR: 0.00006661 \n",
            "[0][559/783] Elapsed: 3m 3s (remain 1m 13s) Loss: 0.0781(0.1969) Grad: 188519.515625 LR: 0.00006561 \n",
            "[0][579/783] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0918(0.1939) Grad: 110504.96875 LR: 0.00006463 \n",
            "[0][599/783] Elapsed: 3m 16s (remain 1m 0s) Loss: 0.1633(0.1913) Grad: 184930.796875 LR: 0.00006367 \n",
            "[0][619/783] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.2034(0.1891) Grad: 121127.28125 LR: 0.00006272 \n",
            "[0][639/783] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.0536(0.1866) Grad: 79571.953125 LR: 0.00006178 \n",
            "[0][659/783] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.0748(0.1843) Grad: 102195.8203125 LR: 0.00006086 \n",
            "[0][679/783] Elapsed: 3m 42s (remain 0m 33s) Loss: 0.0630(0.1819) Grad: 96159.0703125 LR: 0.00005995 \n",
            "[0][699/783] Elapsed: 3m 49s (remain 0m 27s) Loss: 0.1408(0.1796) Grad: 235221.640625 LR: 0.00005905 \n",
            "[0][719/783] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1750(0.1778) Grad: 251570.0 LR: 0.00005817 \n",
            "[0][739/783] Elapsed: 4m 2s (remain 0m 14s) Loss: 0.1135(0.1758) Grad: 133323.203125 LR: 0.00005730 \n",
            "[0][759/783] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.0477(0.1740) Grad: 46502.92578125 LR: 0.00005645 \n",
            "[0][779/783] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.1385(0.1726) Grad: 164812.375 LR: 0.00005560 \n",
            "[1][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0872(0.1245) \n",
            "[1][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.0776(0.1100) \n",
            "[1][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.1058(0.1144) \n",
            "[1][79/194] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1234(0.1125) \n",
            "[1][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1441(0.1144) \n",
            "[1][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.0929(0.1134) \n",
            "[1][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1773(0.1127) \n",
            "[1][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1424(0.1125) \n",
            "[1][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0812(0.1121) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1 - avg_train_loss = 0.1723 avg_val_loss: 0.1116 time: 298.0691s\n",
            "Epoch: 1 - Score: 0.4732, Scores: [0.5072817129384404, 0.46210065919791316, 0.4237463020105452, 0.4790552254104742, 0.4940056783751439, 0.47330054804732125]\n",
            "Epoch 1 - Save Best Score: 0.4732 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1217(0.1116) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/783] Elapsed: 0m 6s (remain 4m 16s) Loss: 0.1337(0.0923) Grad: 406471.875 LR: 0.00005465 \n",
            "[1][39/783] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.0615(0.0918) Grad: 155397.59375 LR: 0.00005383 \n",
            "[1][59/783] Elapsed: 0m 19s (remain 3m 59s) Loss: 0.1862(0.0953) Grad: 570134.1875 LR: 0.00005303 \n",
            "[1][79/783] Elapsed: 0m 26s (remain 3m 52s) Loss: 0.1171(0.0972) Grad: 134765.78125 LR: 0.00005224 \n",
            "[1][99/783] Elapsed: 0m 32s (remain 3m 45s) Loss: 0.1341(0.0946) Grad: 104220.296875 LR: 0.00005146 \n",
            "[1][119/783] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.1559(0.0988) Grad: 188759.484375 LR: 0.00005069 \n",
            "[1][139/783] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.1245(0.1021) Grad: 144619.40625 LR: 0.00004993 \n",
            "[1][159/783] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.0851(0.1019) Grad: 167935.859375 LR: 0.00004918 \n",
            "[1][179/783] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.1136(0.1020) Grad: 60241.53125 LR: 0.00004845 \n",
            "[1][199/783] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.0877(0.1015) Grad: 97674.953125 LR: 0.00004773 \n",
            "[1][219/783] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.0891(0.1008) Grad: 119096.3671875 LR: 0.00004701 \n",
            "[1][239/783] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.1522(0.1024) Grad: 146508.046875 LR: 0.00004631 \n",
            "[1][259/783] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1311(0.1025) Grad: 74498.53125 LR: 0.00004562 \n",
            "[1][279/783] Elapsed: 1m 31s (remain 2m 45s) Loss: 0.0780(0.1012) Grad: 100613.5703125 LR: 0.00004494 \n",
            "[1][299/783] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.2180(0.1013) Grad: 184220.609375 LR: 0.00004427 \n",
            "[1][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0758(0.1027) Grad: 89526.171875 LR: 0.00004361 \n",
            "[1][339/783] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.0819(0.1032) Grad: 75188.9921875 LR: 0.00004295 \n",
            "[1][359/783] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0677(0.1027) Grad: 112599.484375 LR: 0.00004231 \n",
            "[1][379/783] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.1663(0.1030) Grad: 148338.625 LR: 0.00004168 \n",
            "[1][399/783] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0663(0.1023) Grad: 80839.28125 LR: 0.00004106 \n",
            "[1][419/783] Elapsed: 2m 17s (remain 1m 59s) Loss: 0.0920(0.1029) Grad: 61941.78515625 LR: 0.00004044 \n",
            "[1][439/783] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.0834(0.1032) Grad: 105275.6171875 LR: 0.00003984 \n",
            "[1][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1296(0.1032) Grad: 207263.921875 LR: 0.00003925 \n",
            "[1][479/783] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0805(0.1031) Grad: 168225.4375 LR: 0.00003866 \n",
            "[1][499/783] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0891(0.1030) Grad: 151650.15625 LR: 0.00003808 \n",
            "[1][519/783] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0975(0.1039) Grad: 70117.703125 LR: 0.00003751 \n",
            "[1][539/783] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0496(0.1035) Grad: 59693.7578125 LR: 0.00003695 \n",
            "[1][559/783] Elapsed: 3m 3s (remain 1m 13s) Loss: 0.0859(0.1031) Grad: 127531.390625 LR: 0.00003640 \n",
            "[1][579/783] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0769(0.1035) Grad: 149931.328125 LR: 0.00003586 \n",
            "[1][599/783] Elapsed: 3m 16s (remain 1m 0s) Loss: 0.0677(0.1033) Grad: 135195.03125 LR: 0.00003532 \n",
            "[1][619/783] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.1315(0.1035) Grad: 170766.25 LR: 0.00003479 \n",
            "[1][639/783] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.0671(0.1038) Grad: 87141.6875 LR: 0.00003427 \n",
            "[1][659/783] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.0950(0.1033) Grad: 84184.3671875 LR: 0.00003376 \n",
            "[1][679/783] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.1287(0.1035) Grad: 162351.640625 LR: 0.00003326 \n",
            "[1][699/783] Elapsed: 3m 49s (remain 0m 27s) Loss: 0.1081(0.1036) Grad: 77229.7734375 LR: 0.00003276 \n",
            "[1][719/783] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1041(0.1033) Grad: 114426.6015625 LR: 0.00003227 \n",
            "[1][739/783] Elapsed: 4m 2s (remain 0m 14s) Loss: 0.0565(0.1033) Grad: 79226.375 LR: 0.00003179 \n",
            "[1][759/783] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.1519(0.1035) Grad: 283906.0 LR: 0.00003131 \n",
            "[1][779/783] Elapsed: 4m 16s (remain 0m 0s) Loss: 0.0828(0.1033) Grad: 129353.7265625 LR: 0.00003085 \n",
            "[2][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0826(0.1240) \n",
            "[2][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.0827(0.1106) \n",
            "[2][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0888(0.1144) \n",
            "[2][79/194] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1201(0.1121) \n",
            "[2][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1270(0.1131) \n",
            "[2][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.0816(0.1120) \n",
            "[2][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1703(0.1115) \n",
            "[2][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1393(0.1118) \n",
            "[2][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0697(0.1113) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2 - avg_train_loss = 0.1034 avg_val_loss: 0.1105 time: 298.2368s\n",
            "Epoch: 2 - Score: 0.4709, Scores: [0.5097393994866717, 0.4583003592146541, 0.42167372214148174, 0.47475991841713316, 0.4971775498916156, 0.4636462175937214]\n",
            "Epoch 2 - Save Best Score: 0.4709 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1193(0.1105) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/783] Elapsed: 0m 6s (remain 4m 15s) Loss: 0.0904(0.0877) Grad: 331471.6875 LR: 0.00003032 \n",
            "[2][39/783] Elapsed: 0m 13s (remain 4m 6s) Loss: 0.0811(0.0905) Grad: 139117.1875 LR: 0.00002986 \n",
            "[2][59/783] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.0311(0.0893) Grad: 105921.2890625 LR: 0.00002942 \n",
            "[2][79/783] Elapsed: 0m 26s (remain 3m 52s) Loss: 0.1118(0.0890) Grad: 233044.46875 LR: 0.00002898 \n",
            "[2][99/783] Elapsed: 0m 32s (remain 3m 45s) Loss: 0.1266(0.0893) Grad: 336077.03125 LR: 0.00002855 \n",
            "[2][119/783] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.0718(0.0901) Grad: 156779.109375 LR: 0.00002812 \n",
            "[2][139/783] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.0903(0.0917) Grad: 195690.0625 LR: 0.00002770 \n",
            "[2][159/783] Elapsed: 0m 52s (remain 3m 25s) Loss: 0.0870(0.0947) Grad: 336737.09375 LR: 0.00002729 \n",
            "[2][179/783] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.0592(0.0961) Grad: 124671.0390625 LR: 0.00002688 \n",
            "[2][199/783] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.0694(0.0954) Grad: 180973.984375 LR: 0.00002648 \n",
            "[2][219/783] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.1062(0.0952) Grad: 161162.84375 LR: 0.00002608 \n",
            "[2][239/783] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.0571(0.0948) Grad: 173913.890625 LR: 0.00002569 \n",
            "[2][259/783] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1134(0.0949) Grad: 181802.046875 LR: 0.00002531 \n",
            "[2][279/783] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0831(0.0948) Grad: 170475.34375 LR: 0.00002493 \n",
            "[2][299/783] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.1113(0.0954) Grad: 272511.5625 LR: 0.00002456 \n",
            "[2][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0680(0.0952) Grad: 185654.390625 LR: 0.00002419 \n",
            "[2][339/783] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.1053(0.0953) Grad: 455575.03125 LR: 0.00002383 \n",
            "[2][359/783] Elapsed: 1m 58s (remain 2m 18s) Loss: 0.0532(0.0957) Grad: 199464.5 LR: 0.00002347 \n",
            "[2][379/783] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.1356(0.0959) Grad: 160051.734375 LR: 0.00002312 \n",
            "[2][399/783] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0673(0.0950) Grad: 202197.5 LR: 0.00002278 \n",
            "[2][419/783] Elapsed: 2m 17s (remain 1m 59s) Loss: 0.0842(0.0950) Grad: 158126.640625 LR: 0.00002244 \n",
            "[2][439/783] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.1705(0.0955) Grad: 290084.15625 LR: 0.00002210 \n",
            "[2][459/783] Elapsed: 2m 30s (remain 1m 46s) Loss: 0.0546(0.0954) Grad: 186215.125 LR: 0.00002177 \n",
            "[2][479/783] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0997(0.0955) Grad: 328467.46875 LR: 0.00002145 \n",
            "[2][499/783] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0849(0.0954) Grad: 164243.265625 LR: 0.00002113 \n",
            "[2][519/783] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0554(0.0955) Grad: 133938.671875 LR: 0.00002081 \n",
            "[2][539/783] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0950(0.0963) Grad: 262735.75 LR: 0.00002050 \n",
            "[2][559/783] Elapsed: 3m 3s (remain 1m 13s) Loss: 0.0702(0.0964) Grad: 175787.140625 LR: 0.00002019 \n",
            "[2][579/783] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.1889(0.0967) Grad: 174188.375 LR: 0.00001989 \n",
            "[2][599/783] Elapsed: 3m 16s (remain 1m 0s) Loss: 0.0815(0.0968) Grad: 196311.5625 LR: 0.00001960 \n",
            "[2][619/783] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.0601(0.0966) Grad: 79371.78125 LR: 0.00001930 \n",
            "[2][639/783] Elapsed: 3m 29s (remain 0m 46s) Loss: 0.0978(0.0962) Grad: 158468.453125 LR: 0.00001901 \n",
            "[2][659/783] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.1062(0.0961) Grad: 296632.75 LR: 0.00001873 \n",
            "[2][679/783] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0628(0.0965) Grad: 233517.359375 LR: 0.00001845 \n",
            "[2][699/783] Elapsed: 3m 49s (remain 0m 27s) Loss: 0.0510(0.0964) Grad: 249390.875 LR: 0.00001817 \n",
            "[2][719/783] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.0830(0.0962) Grad: 229062.171875 LR: 0.00001790 \n",
            "[2][739/783] Elapsed: 4m 2s (remain 0m 14s) Loss: 0.0757(0.0962) Grad: 199493.671875 LR: 0.00001764 \n",
            "[2][759/783] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.1173(0.0962) Grad: 170545.734375 LR: 0.00001737 \n",
            "[2][779/783] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.1098(0.0959) Grad: 225557.84375 LR: 0.00001711 \n",
            "[3][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0747(0.1179) \n",
            "[3][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.0761(0.1077) \n",
            "[3][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0975(0.1119) \n",
            "[3][79/194] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.1170(0.1100) \n",
            "[3][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1446(0.1119) \n",
            "[3][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.0837(0.1110) \n",
            "[3][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1911(0.1106) \n",
            "[3][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1355(0.1108) \n",
            "[3][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0778(0.1105) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3 - avg_train_loss = 0.0959 avg_val_loss: 0.1100 time: 298.2288s\n",
            "Epoch: 3 - Score: 0.4698, Scores: [0.5018428062861521, 0.46051859801144474, 0.4207361539767949, 0.4803043873235634, 0.4910628834216279, 0.46456895700258427]\n",
            "Epoch 3 - Save Best Score: 0.4698 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1282(0.1100) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/783] Elapsed: 0m 6s (remain 4m 17s) Loss: 0.0840(0.0893) Grad: 258840.171875 LR: 0.00001682 \n",
            "[3][39/783] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.0463(0.0876) Grad: 227343.984375 LR: 0.00001657 \n",
            "[3][59/783] Elapsed: 0m 19s (remain 3m 59s) Loss: 0.1491(0.0894) Grad: 193837.609375 LR: 0.00001632 \n",
            "[3][79/783] Elapsed: 0m 26s (remain 3m 52s) Loss: 0.0485(0.0860) Grad: 114184.0625 LR: 0.00001608 \n",
            "[3][99/783] Elapsed: 0m 33s (remain 3m 45s) Loss: 0.0891(0.0852) Grad: 317885.28125 LR: 0.00001584 \n",
            "[3][119/783] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.0964(0.0865) Grad: 347556.03125 LR: 0.00001560 \n",
            "[3][139/783] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.0765(0.0875) Grad: 137640.484375 LR: 0.00001537 \n",
            "[3][159/783] Elapsed: 0m 52s (remain 3m 25s) Loss: 0.0997(0.0883) Grad: 144660.96875 LR: 0.00001514 \n",
            "[3][179/783] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.0431(0.0886) Grad: 76030.2109375 LR: 0.00001491 \n",
            "[3][199/783] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.0578(0.0879) Grad: 182684.453125 LR: 0.00001469 \n",
            "[3][219/783] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.0293(0.0893) Grad: 94216.7578125 LR: 0.00001447 \n",
            "[3][239/783] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.1292(0.0895) Grad: 486674.53125 LR: 0.00001425 \n",
            "[3][259/783] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.1207(0.0891) Grad: 459070.0625 LR: 0.00001404 \n",
            "[3][279/783] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.1418(0.0897) Grad: 238640.015625 LR: 0.00001383 \n",
            "[3][299/783] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0410(0.0896) Grad: 126019.796875 LR: 0.00001362 \n",
            "[3][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.1086(0.0894) Grad: 240630.15625 LR: 0.00001342 \n",
            "[3][339/783] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.0965(0.0899) Grad: 115409.109375 LR: 0.00001322 \n",
            "[3][359/783] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.0816(0.0903) Grad: 149628.46875 LR: 0.00001302 \n",
            "[3][379/783] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.0555(0.0898) Grad: 128942.2109375 LR: 0.00001283 \n",
            "[3][399/783] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0884(0.0898) Grad: 171238.015625 LR: 0.00001264 \n",
            "[3][419/783] Elapsed: 2m 17s (remain 1m 59s) Loss: 0.0628(0.0908) Grad: 285130.15625 LR: 0.00001245 \n",
            "[3][439/783] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.0891(0.0911) Grad: 194309.359375 LR: 0.00001226 \n",
            "[3][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1064(0.0911) Grad: 198521.453125 LR: 0.00001208 \n",
            "[3][479/783] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.1025(0.0911) Grad: 210182.515625 LR: 0.00001190 \n",
            "[3][499/783] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0915(0.0906) Grad: 164677.578125 LR: 0.00001172 \n",
            "[3][519/783] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0567(0.0907) Grad: 122385.2421875 LR: 0.00001155 \n",
            "[3][539/783] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.1404(0.0907) Grad: 279458.75 LR: 0.00001137 \n",
            "[3][559/783] Elapsed: 3m 3s (remain 1m 13s) Loss: 0.0766(0.0907) Grad: 180210.15625 LR: 0.00001120 \n",
            "[3][579/783] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.1397(0.0908) Grad: 231956.671875 LR: 0.00001104 \n",
            "[3][599/783] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.1124(0.0916) Grad: 190284.09375 LR: 0.00001087 \n",
            "[3][619/783] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.0893(0.0919) Grad: 164934.265625 LR: 0.00001071 \n",
            "[3][639/783] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.0947(0.0927) Grad: 206470.703125 LR: 0.00001055 \n",
            "[3][659/783] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.1745(0.0930) Grad: 379441.28125 LR: 0.00001039 \n",
            "[3][679/783] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0623(0.0933) Grad: 98730.9765625 LR: 0.00001024 \n",
            "[3][699/783] Elapsed: 3m 49s (remain 0m 27s) Loss: 0.0811(0.0932) Grad: 193189.78125 LR: 0.00001008 \n",
            "[3][719/783] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.0623(0.0934) Grad: 218630.484375 LR: 0.00000993 \n",
            "[3][739/783] Elapsed: 4m 2s (remain 0m 14s) Loss: 0.0436(0.0933) Grad: 196287.890625 LR: 0.00000978 \n",
            "[3][759/783] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.0853(0.0931) Grad: 135835.21875 LR: 0.00000964 \n",
            "[3][779/783] Elapsed: 4m 15s (remain 0m 0s) Loss: 0.1455(0.0935) Grad: 103009.9609375 LR: 0.00000949 \n",
            "[4][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0834(0.1215) \n",
            "[4][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.0847(0.1099) \n",
            "[4][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0891(0.1144) \n",
            "[4][79/194] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.1069(0.1121) \n",
            "[4][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1262(0.1132) \n",
            "[4][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.0802(0.1119) \n",
            "[4][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1724(0.1114) \n",
            "[4][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1390(0.1116) \n",
            "[4][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0665(0.1110) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4 - avg_train_loss = 0.0936 avg_val_loss: 0.1101 time: 298.3316s\n",
            "Epoch: 4 - Score: 0.4701, Scores: [0.5008543184146947, 0.4595366433153847, 0.4202019839327752, 0.47672153035588816, 0.49889343217902177, 0.46417296878183667]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1272(0.1101) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/783] Elapsed: 0m 6s (remain 4m 14s) Loss: 0.0687(0.1025) Grad: 180960.5 LR: 0.00000933 \n",
            "[4][39/783] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.1016(0.0935) Grad: 311858.78125 LR: 0.00000919 \n",
            "[4][59/783] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.0820(0.0917) Grad: 218155.21875 LR: 0.00000905 \n",
            "[4][79/783] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.0348(0.0904) Grad: 121595.4765625 LR: 0.00000892 \n",
            "[4][99/783] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.0770(0.0903) Grad: 264955.46875 LR: 0.00000879 \n",
            "[4][119/783] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.0658(0.0923) Grad: 93278.140625 LR: 0.00000865 \n",
            "[4][139/783] Elapsed: 0m 46s (remain 3m 31s) Loss: 0.1984(0.0940) Grad: 215305.890625 LR: 0.00000853 \n",
            "[4][159/783] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.0840(0.0946) Grad: 126004.7578125 LR: 0.00000840 \n",
            "[4][179/783] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.1420(0.0945) Grad: 163811.53125 LR: 0.00000827 \n",
            "[4][199/783] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.0714(0.0937) Grad: 327789.46875 LR: 0.00000815 \n",
            "[4][219/783] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.0795(0.0931) Grad: 190241.296875 LR: 0.00000803 \n",
            "[4][239/783] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.0722(0.0929) Grad: 188985.125 LR: 0.00000791 \n",
            "[4][259/783] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1683(0.0932) Grad: 359784.375 LR: 0.00000779 \n",
            "[4][279/783] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0820(0.0932) Grad: 175235.171875 LR: 0.00000767 \n",
            "[4][299/783] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0694(0.0929) Grad: 205981.375 LR: 0.00000756 \n",
            "[4][319/783] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0700(0.0932) Grad: 112978.921875 LR: 0.00000745 \n",
            "[4][339/783] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.0895(0.0936) Grad: 196288.046875 LR: 0.00000733 \n",
            "[4][359/783] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.0789(0.0937) Grad: 206390.984375 LR: 0.00000722 \n",
            "[4][379/783] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.0718(0.0935) Grad: 230961.515625 LR: 0.00000712 \n",
            "[4][399/783] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.0514(0.0934) Grad: 100751.7265625 LR: 0.00000701 \n",
            "[4][419/783] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0762(0.0934) Grad: 353200.5 LR: 0.00000691 \n",
            "[4][439/783] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.1071(0.0933) Grad: 338845.96875 LR: 0.00000680 \n",
            "[4][459/783] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1031(0.0935) Grad: 201444.640625 LR: 0.00000670 \n",
            "[4][479/783] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0841(0.0931) Grad: 171309.453125 LR: 0.00000660 \n",
            "[4][499/783] Elapsed: 2m 44s (remain 1m 32s) Loss: 0.0625(0.0928) Grad: 158952.390625 LR: 0.00000650 \n",
            "[4][519/783] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.0459(0.0929) Grad: 181585.40625 LR: 0.00000641 \n",
            "[4][539/783] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.0806(0.0929) Grad: 198052.625 LR: 0.00000631 \n",
            "[4][559/783] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0967(0.0932) Grad: 238468.34375 LR: 0.00000622 \n",
            "[4][579/783] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0667(0.0931) Grad: 153546.75 LR: 0.00000612 \n",
            "[4][599/783] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.1381(0.0931) Grad: 386058.9375 LR: 0.00000603 \n",
            "[4][619/783] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.0965(0.0926) Grad: 191398.296875 LR: 0.00000594 \n",
            "[4][639/783] Elapsed: 3m 30s (remain 0m 46s) Loss: 0.1069(0.0925) Grad: 218452.390625 LR: 0.00000585 \n",
            "[4][659/783] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.0697(0.0927) Grad: 104321.046875 LR: 0.00000576 \n",
            "[4][679/783] Elapsed: 3m 43s (remain 0m 33s) Loss: 0.0655(0.0926) Grad: 120403.5390625 LR: 0.00000568 \n",
            "[4][699/783] Elapsed: 3m 49s (remain 0m 27s) Loss: 0.0911(0.0924) Grad: 147054.390625 LR: 0.00000559 \n",
            "[4][719/783] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.0820(0.0921) Grad: 281738.9375 LR: 0.00000551 \n",
            "[4][739/783] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.2424(0.0923) Grad: 528256.5625 LR: 0.00000543 \n",
            "[4][759/783] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.1256(0.0922) Grad: 151334.46875 LR: 0.00000535 \n",
            "[4][779/783] Elapsed: 4m 16s (remain 0m 0s) Loss: 0.1102(0.0922) Grad: 345341.8125 LR: 0.00000527 \n",
            "[5][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0842(0.1206) \n",
            "[5][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.0803(0.1084) \n",
            "[5][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0897(0.1128) \n",
            "[5][79/194] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.1138(0.1103) \n",
            "[5][99/194] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1246(0.1116) \n",
            "[5][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.0821(0.1106) \n",
            "[5][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1727(0.1100) \n",
            "[5][159/194] Elapsed: 0m 34s (remain 0m 7s) Loss: 0.1324(0.1102) \n",
            "[5][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0623(0.1096) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5 - avg_train_loss = 0.0921 avg_val_loss: 0.1089 time: 298.7101s\n",
            "Epoch: 5 - Score: 0.4674, Scores: [0.4983256182987699, 0.4586314146066606, 0.41808478689213147, 0.47783451718474224, 0.489226655992209, 0.46228337125976426]\n",
            "Epoch 5 - Save Best Score: 0.4674 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.1290(0.1089) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = igetitem(value, i)\n",
            "========== fold: 3 result ==========\n",
            "Score: 0.4674  Scores: [0.4983256182987699, 0.4586314146066606, 0.41808478689213147, 0.47783451718474224, 0.489226655992209, 0.46228337125976426]\n",
            "========== fold: 4 training ==========\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.dense.weight', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.weight', 'lm_predictions.lm_head.dense.weight']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "<class '__main__.CFG'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[DEBUG] num_train_steps:  3916  len train loader:  784\n",
            "warmup start:  19580\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/784] Elapsed: 0m 6s (remain 4m 13s) Loss: 0.9150(1.8874) Grad: 306832.59375 LR: 0.00009851 \n",
            "[0][39/784] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.1798(1.0772) Grad: 103197.2265625 LR: 0.00009703 \n",
            "[0][59/784] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.2238(0.7794) Grad: 113743.96875 LR: 0.00009559 \n",
            "[0][79/784] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.1607(0.6291) Grad: 53743.42578125 LR: 0.00009416 \n",
            "[0][99/784] Elapsed: 0m 32s (remain 3m 44s) Loss: 0.1921(0.5336) Grad: 177327.703125 LR: 0.00009275 \n",
            "[0][119/784] Elapsed: 0m 39s (remain 3m 37s) Loss: 0.1657(0.4686) Grad: 193106.0 LR: 0.00009137 \n",
            "[0][139/784] Elapsed: 0m 45s (remain 3m 31s) Loss: 0.1172(0.4208) Grad: 107261.2421875 LR: 0.00009000 \n",
            "[0][159/784] Elapsed: 0m 52s (remain 3m 24s) Loss: 0.2447(0.3834) Grad: 138078.109375 LR: 0.00008866 \n",
            "[0][179/784] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.1039(0.3538) Grad: 73933.2734375 LR: 0.00008733 \n",
            "[0][199/784] Elapsed: 1m 5s (remain 3m 11s) Loss: 0.0937(0.3318) Grad: 143227.546875 LR: 0.00008603 \n",
            "[0][219/784] Elapsed: 1m 12s (remain 3m 4s) Loss: 0.1674(0.3119) Grad: 263782.40625 LR: 0.00008474 \n",
            "[0][239/784] Elapsed: 1m 18s (remain 2m 58s) Loss: 0.0863(0.2953) Grad: 129855.5078125 LR: 0.00008348 \n",
            "[0][259/784] Elapsed: 1m 25s (remain 2m 51s) Loss: 0.1087(0.2818) Grad: 135443.734375 LR: 0.00008223 \n",
            "[0][279/784] Elapsed: 1m 31s (remain 2m 45s) Loss: 0.0959(0.2703) Grad: 88170.53125 LR: 0.00008100 \n",
            "[0][299/784] Elapsed: 1m 38s (remain 2m 38s) Loss: 0.0904(0.2593) Grad: 159321.234375 LR: 0.00007979 \n",
            "[0][319/784] Elapsed: 1m 44s (remain 2m 32s) Loss: 0.1583(0.2520) Grad: 77966.2734375 LR: 0.00007860 \n",
            "[0][339/784] Elapsed: 1m 51s (remain 2m 25s) Loss: 0.0994(0.2441) Grad: 168256.40625 LR: 0.00007743 \n",
            "[0][359/784] Elapsed: 1m 57s (remain 2m 18s) Loss: 0.1154(0.2363) Grad: 88316.140625 LR: 0.00007627 \n",
            "[0][379/784] Elapsed: 2m 4s (remain 2m 12s) Loss: 0.0980(0.2297) Grad: 100802.2265625 LR: 0.00007513 \n",
            "[0][399/784] Elapsed: 2m 11s (remain 2m 5s) Loss: 0.1094(0.2244) Grad: 127246.0 LR: 0.00007401 \n",
            "[0][419/784] Elapsed: 2m 17s (remain 1m 59s) Loss: 0.0614(0.2187) Grad: 102399.953125 LR: 0.00007290 \n",
            "[0][439/784] Elapsed: 2m 24s (remain 1m 52s) Loss: 0.1793(0.2140) Grad: 113203.7265625 LR: 0.00007181 \n",
            "[0][459/784] Elapsed: 2m 30s (remain 1m 46s) Loss: 0.1565(0.2097) Grad: 182771.703125 LR: 0.00007074 \n",
            "[0][479/784] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.1208(0.2059) Grad: 75091.4296875 LR: 0.00006968 \n",
            "[0][499/784] Elapsed: 2m 43s (remain 1m 33s) Loss: 0.0801(0.2024) Grad: 144435.125 LR: 0.00006864 \n",
            "[0][519/784] Elapsed: 2m 50s (remain 1m 26s) Loss: 0.2515(0.1992) Grad: 198209.46875 LR: 0.00006762 \n",
            "[0][539/784] Elapsed: 2m 57s (remain 1m 19s) Loss: 0.1270(0.1962) Grad: 97898.8125 LR: 0.00006661 \n",
            "[0][559/784] Elapsed: 3m 3s (remain 1m 13s) Loss: 0.1423(0.1932) Grad: 158620.5625 LR: 0.00006561 \n",
            "[0][579/784] Elapsed: 3m 10s (remain 1m 6s) Loss: 0.0891(0.1900) Grad: 145842.9375 LR: 0.00006463 \n",
            "[0][599/784] Elapsed: 3m 16s (remain 1m 0s) Loss: 0.2048(0.1875) Grad: 207469.96875 LR: 0.00006367 \n",
            "[0][619/784] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.1290(0.1850) Grad: 201816.390625 LR: 0.00006272 \n",
            "[0][639/784] Elapsed: 3m 29s (remain 0m 47s) Loss: 0.0741(0.1830) Grad: 140644.21875 LR: 0.00006178 \n",
            "[0][659/784] Elapsed: 3m 36s (remain 0m 40s) Loss: 0.1464(0.1809) Grad: 256048.671875 LR: 0.00006086 \n",
            "[0][679/784] Elapsed: 3m 42s (remain 0m 34s) Loss: 0.0527(0.1786) Grad: 73044.34375 LR: 0.00005995 \n",
            "[0][699/784] Elapsed: 3m 49s (remain 0m 27s) Loss: 0.1644(0.1770) Grad: 84406.671875 LR: 0.00005905 \n",
            "[0][719/784] Elapsed: 3m 56s (remain 0m 20s) Loss: 0.1156(0.1749) Grad: 125529.6015625 LR: 0.00005817 \n",
            "[0][739/784] Elapsed: 4m 2s (remain 0m 14s) Loss: 0.1883(0.1738) Grad: 138993.953125 LR: 0.00005730 \n",
            "[0][759/784] Elapsed: 4m 9s (remain 0m 7s) Loss: 0.0601(0.1720) Grad: 70242.9453125 LR: 0.00005645 \n",
            "[0][779/784] Elapsed: 4m 15s (remain 0m 1s) Loss: 0.1136(0.1709) Grad: 101388.4296875 LR: 0.00005560 \n",
            "[1][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0985(0.1233) \n",
            "[1][39/194] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0662(0.1231) \n",
            "[1][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0773(0.1214) \n",
            "[1][79/194] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0548(0.1231) \n",
            "[1][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.0956(0.1241) \n",
            "[1][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.2094(0.1231) \n",
            "[1][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1058(0.1207) \n",
            "[1][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0792(0.1210) \n",
            "[1][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0942(0.1223) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 1 - avg_train_loss = 0.1705 avg_val_loss: 0.1217 time: 298.4078s\n",
            "Epoch: 1 - Score: 0.4946, Scores: [0.512257296535541, 0.46932700299306623, 0.47766331451387317, 0.47242218452273343, 0.4888122638558455, 0.5472840672497724]\n",
            "Epoch 1 - Save Best Score: 0.4946 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0741(0.1217) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/784] Elapsed: 0m 6s (remain 4m 16s) Loss: 0.0817(0.1303) Grad: 170529.453125 LR: 0.00005461 \n",
            "[1][39/784] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.2020(0.1146) Grad: 271820.03125 LR: 0.00005379 \n",
            "[1][59/784] Elapsed: 0m 19s (remain 3m 59s) Loss: 0.0852(0.1101) Grad: 174002.9375 LR: 0.00005299 \n",
            "[1][79/784] Elapsed: 0m 26s (remain 3m 52s) Loss: 0.1564(0.1093) Grad: 273114.5 LR: 0.00005220 \n",
            "[1][99/784] Elapsed: 0m 33s (remain 3m 45s) Loss: 0.1063(0.1090) Grad: 181513.0625 LR: 0.00005142 \n",
            "[1][119/784] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.0784(0.1081) Grad: 223358.328125 LR: 0.00005065 \n",
            "[1][139/784] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.0907(0.1081) Grad: 194976.453125 LR: 0.00004989 \n",
            "[1][159/784] Elapsed: 0m 52s (remain 3m 25s) Loss: 0.0884(0.1085) Grad: 249102.328125 LR: 0.00004915 \n",
            "[1][179/784] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.1112(0.1060) Grad: 195232.890625 LR: 0.00004841 \n",
            "[1][199/784] Elapsed: 1m 5s (remain 3m 12s) Loss: 0.0555(0.1063) Grad: 143412.671875 LR: 0.00004769 \n",
            "[1][219/784] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.1275(0.1056) Grad: 412679.0625 LR: 0.00004698 \n",
            "[1][239/784] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.0919(0.1060) Grad: 149550.984375 LR: 0.00004628 \n",
            "[1][259/784] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0920(0.1054) Grad: 227649.84375 LR: 0.00004558 \n",
            "[1][279/784] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.0651(0.1054) Grad: 178458.8125 LR: 0.00004490 \n",
            "[1][299/784] Elapsed: 1m 38s (remain 2m 39s) Loss: 0.0832(0.1045) Grad: 181331.640625 LR: 0.00004423 \n",
            "[1][319/784] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0900(0.1035) Grad: 161766.09375 LR: 0.00004357 \n",
            "[1][339/784] Elapsed: 1m 51s (remain 2m 26s) Loss: 0.0983(0.1024) Grad: 208979.1875 LR: 0.00004292 \n",
            "[1][359/784] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.1511(0.1023) Grad: 199768.921875 LR: 0.00004228 \n",
            "[1][379/784] Elapsed: 2m 5s (remain 2m 12s) Loss: 0.1843(0.1035) Grad: 503465.9375 LR: 0.00004165 \n",
            "[1][399/784] Elapsed: 2m 11s (remain 2m 6s) Loss: 0.1318(0.1035) Grad: 90190.90625 LR: 0.00004103 \n",
            "[1][419/784] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.1410(0.1036) Grad: 75269.65625 LR: 0.00004041 \n",
            "[1][439/784] Elapsed: 2m 24s (remain 1m 53s) Loss: 0.1939(0.1040) Grad: 123013.2265625 LR: 0.00003981 \n",
            "[1][459/784] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.0660(0.1038) Grad: 114799.1328125 LR: 0.00003922 \n",
            "[1][479/784] Elapsed: 2m 37s (remain 1m 39s) Loss: 0.0837(0.1037) Grad: 100459.3828125 LR: 0.00003863 \n",
            "[1][499/784] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.0732(0.1038) Grad: 74584.0234375 LR: 0.00003805 \n",
            "[1][519/784] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.0649(0.1037) Grad: 121917.171875 LR: 0.00003748 \n",
            "[1][539/784] Elapsed: 2m 57s (remain 1m 20s) Loss: 0.0636(0.1032) Grad: 82735.28125 LR: 0.00003692 \n",
            "[1][559/784] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0465(0.1030) Grad: 49527.015625 LR: 0.00003637 \n",
            "[1][579/784] Elapsed: 3m 10s (remain 1m 7s) Loss: 0.0951(0.1031) Grad: 54349.51171875 LR: 0.00003583 \n",
            "[1][599/784] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.1480(0.1028) Grad: 121542.1015625 LR: 0.00003529 \n",
            "[1][619/784] Elapsed: 3m 23s (remain 0m 53s) Loss: 0.1504(0.1027) Grad: 155587.421875 LR: 0.00003477 \n",
            "[1][639/784] Elapsed: 3m 30s (remain 0m 47s) Loss: 0.0738(0.1026) Grad: 114202.7578125 LR: 0.00003425 \n",
            "[1][659/784] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.1008(0.1027) Grad: 155273.625 LR: 0.00003374 \n",
            "[1][679/784] Elapsed: 3m 43s (remain 0m 34s) Loss: 0.1986(0.1026) Grad: 272624.375 LR: 0.00003323 \n",
            "[1][699/784] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.0775(0.1029) Grad: 69344.1875 LR: 0.00003274 \n",
            "[1][719/784] Elapsed: 3m 56s (remain 0m 21s) Loss: 0.1054(0.1029) Grad: 114093.28125 LR: 0.00003225 \n",
            "[1][739/784] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.1232(0.1027) Grad: 105502.953125 LR: 0.00003177 \n",
            "[1][759/784] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0730(0.1027) Grad: 134721.8125 LR: 0.00003129 \n",
            "[1][779/784] Elapsed: 4m 16s (remain 0m 1s) Loss: 0.1354(0.1027) Grad: 175523.921875 LR: 0.00003082 \n",
            "[2][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0937(0.1048) \n",
            "[2][39/194] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.1197(0.1060) \n",
            "[2][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0749(0.1053) \n",
            "[2][79/194] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0620(0.1063) \n",
            "[2][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1552(0.1086) \n",
            "[2][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1143(0.1070) \n",
            "[2][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0920(0.1054) \n",
            "[2][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.1047(0.1062) \n",
            "[2][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0762(0.1076) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 2 - avg_train_loss = 0.1027 avg_val_loss: 0.1081 time: 299.1191s\n",
            "Epoch: 2 - Score: 0.4657, Scores: [0.4956899493437762, 0.448770801376623, 0.41927404626869436, 0.4629767982400117, 0.4842734326208937, 0.4832995788499807]\n",
            "Epoch 2 - Save Best Score: 0.4657 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0919(0.1081) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/784] Elapsed: 0m 6s (remain 4m 16s) Loss: 0.1359(0.0930) Grad: 150641.84375 LR: 0.00003027 \n",
            "[2][39/784] Elapsed: 0m 13s (remain 4m 7s) Loss: 0.0676(0.0945) Grad: 174614.46875 LR: 0.00002982 \n",
            "[2][59/784] Elapsed: 0m 19s (remain 4m 0s) Loss: 0.0877(0.0940) Grad: 226332.6875 LR: 0.00002937 \n",
            "[2][79/784] Elapsed: 0m 26s (remain 3m 53s) Loss: 0.0962(0.0974) Grad: 180798.25 LR: 0.00002894 \n",
            "[2][99/784] Elapsed: 0m 33s (remain 3m 46s) Loss: 0.2150(0.0997) Grad: 501478.84375 LR: 0.00002850 \n",
            "[2][119/784] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.1920(0.1001) Grad: 250127.828125 LR: 0.00002808 \n",
            "[2][139/784] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.0442(0.1000) Grad: 220569.421875 LR: 0.00002766 \n",
            "[2][159/784] Elapsed: 0m 52s (remain 3m 26s) Loss: 0.0673(0.0987) Grad: 185182.75 LR: 0.00002725 \n",
            "[2][179/784] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.0531(0.0992) Grad: 171411.421875 LR: 0.00002684 \n",
            "[2][199/784] Elapsed: 1m 5s (remain 3m 12s) Loss: 0.0704(0.0985) Grad: 129025.3359375 LR: 0.00002644 \n",
            "[2][219/784] Elapsed: 1m 12s (remain 3m 6s) Loss: 0.0560(0.0980) Grad: 208099.203125 LR: 0.00002604 \n",
            "[2][239/784] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.0984(0.0975) Grad: 220448.34375 LR: 0.00002565 \n",
            "[2][259/784] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0623(0.0978) Grad: 198839.328125 LR: 0.00002527 \n",
            "[2][279/784] Elapsed: 1m 32s (remain 2m 46s) Loss: 0.1123(0.0973) Grad: 157091.359375 LR: 0.00002489 \n",
            "[2][299/784] Elapsed: 1m 38s (remain 2m 39s) Loss: 0.0591(0.0970) Grad: 117859.0390625 LR: 0.00002452 \n",
            "[2][319/784] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0671(0.0970) Grad: 222405.609375 LR: 0.00002415 \n",
            "[2][339/784] Elapsed: 1m 52s (remain 2m 26s) Loss: 0.1020(0.0967) Grad: 226943.625 LR: 0.00002379 \n",
            "[2][359/784] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.1173(0.0964) Grad: 251846.828125 LR: 0.00002344 \n",
            "[2][379/784] Elapsed: 2m 5s (remain 2m 13s) Loss: 0.0694(0.0967) Grad: 183696.09375 LR: 0.00002309 \n",
            "[2][399/784] Elapsed: 2m 11s (remain 2m 6s) Loss: 0.1185(0.0966) Grad: 260047.640625 LR: 0.00002274 \n",
            "[2][419/784] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.1420(0.0967) Grad: 234239.859375 LR: 0.00002240 \n",
            "[2][439/784] Elapsed: 2m 24s (remain 1m 53s) Loss: 0.0746(0.0972) Grad: 58887.04296875 LR: 0.00002207 \n",
            "[2][459/784] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1009(0.0975) Grad: 78582.09375 LR: 0.00002174 \n",
            "[2][479/784] Elapsed: 2m 38s (remain 1m 40s) Loss: 0.1255(0.0979) Grad: 112269.078125 LR: 0.00002141 \n",
            "[2][499/784] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.1058(0.0982) Grad: 90955.21875 LR: 0.00002109 \n",
            "[2][519/784] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.1041(0.0980) Grad: 126994.2890625 LR: 0.00002078 \n",
            "[2][539/784] Elapsed: 2m 57s (remain 1m 20s) Loss: 0.0647(0.0980) Grad: 108505.1796875 LR: 0.00002047 \n",
            "[2][559/784] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.1005(0.0979) Grad: 180011.234375 LR: 0.00002016 \n",
            "[2][579/784] Elapsed: 3m 11s (remain 1m 7s) Loss: 0.1439(0.0981) Grad: 149850.453125 LR: 0.00001986 \n",
            "[2][599/784] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.0981(0.0977) Grad: 113022.8828125 LR: 0.00001957 \n",
            "[2][619/784] Elapsed: 3m 24s (remain 0m 54s) Loss: 0.0808(0.0975) Grad: 178877.0625 LR: 0.00001927 \n",
            "[2][639/784] Elapsed: 3m 30s (remain 0m 47s) Loss: 0.1315(0.0972) Grad: 163207.015625 LR: 0.00001899 \n",
            "[2][659/784] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.0887(0.0978) Grad: 85849.71875 LR: 0.00001870 \n",
            "[2][679/784] Elapsed: 3m 43s (remain 0m 34s) Loss: 0.0821(0.0979) Grad: 117201.4921875 LR: 0.00001842 \n",
            "[2][699/784] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.0999(0.0977) Grad: 142419.03125 LR: 0.00001815 \n",
            "[2][719/784] Elapsed: 3m 57s (remain 0m 21s) Loss: 0.0548(0.0974) Grad: 83200.296875 LR: 0.00001788 \n",
            "[2][739/784] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.0746(0.0974) Grad: 85745.609375 LR: 0.00001761 \n",
            "[2][759/784] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.1027(0.0973) Grad: 154588.390625 LR: 0.00001735 \n",
            "[2][779/784] Elapsed: 4m 16s (remain 0m 1s) Loss: 0.1016(0.0971) Grad: 66818.2890625 LR: 0.00001709 \n",
            "[3][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0884(0.1066) \n",
            "[3][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.0952(0.1050) \n",
            "[3][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0700(0.1029) \n",
            "[3][79/194] Elapsed: 0m 16s (remain 0m 24s) Loss: 0.0595(0.1047) \n",
            "[3][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1267(0.1065) \n",
            "[3][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1245(0.1051) \n",
            "[3][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1001(0.1035) \n",
            "[3][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0984(0.1042) \n",
            "[3][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0746(0.1056) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 3 - avg_train_loss = 0.0972 avg_val_loss: 0.1059 time: 299.2936s\n",
            "Epoch: 3 - Score: 0.4608, Scores: [0.4859723639517774, 0.4464138210746739, 0.4177311566650099, 0.4650511648552315, 0.486525219443456, 0.46285465918229857]\n",
            "Epoch 3 - Save Best Score: 0.4608 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0778(0.1059) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/784] Elapsed: 0m 6s (remain 4m 17s) Loss: 0.0751(0.0933) Grad: 176688.625 LR: 0.00001678 \n",
            "[3][39/784] Elapsed: 0m 13s (remain 4m 8s) Loss: 0.0624(0.0909) Grad: 189520.34375 LR: 0.00001653 \n",
            "[3][59/784] Elapsed: 0m 19s (remain 4m 0s) Loss: 0.1054(0.0911) Grad: 310128.71875 LR: 0.00001628 \n",
            "[3][79/784] Elapsed: 0m 26s (remain 3m 53s) Loss: 0.1177(0.0926) Grad: 199889.15625 LR: 0.00001604 \n",
            "[3][99/784] Elapsed: 0m 33s (remain 3m 46s) Loss: 0.2259(0.0944) Grad: 321449.5625 LR: 0.00001580 \n",
            "[3][119/784] Elapsed: 0m 39s (remain 3m 39s) Loss: 0.1211(0.0933) Grad: 210641.453125 LR: 0.00001557 \n",
            "[3][139/784] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.0345(0.0926) Grad: 150149.53125 LR: 0.00001533 \n",
            "[3][159/784] Elapsed: 0m 52s (remain 3m 26s) Loss: 0.0744(0.0934) Grad: 160239.21875 LR: 0.00001510 \n",
            "[3][179/784] Elapsed: 0m 59s (remain 3m 19s) Loss: 0.1161(0.0943) Grad: 149232.59375 LR: 0.00001488 \n",
            "[3][199/784] Elapsed: 1m 5s (remain 3m 12s) Loss: 0.1455(0.0948) Grad: 204573.4375 LR: 0.00001466 \n",
            "[3][219/784] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.1978(0.0947) Grad: 440017.9375 LR: 0.00001444 \n",
            "[3][239/784] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.1009(0.0943) Grad: 297813.6875 LR: 0.00001422 \n",
            "[3][259/784] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0710(0.0935) Grad: 181833.265625 LR: 0.00001401 \n",
            "[3][279/784] Elapsed: 1m 32s (remain 2m 46s) Loss: 0.0674(0.0932) Grad: 213307.484375 LR: 0.00001380 \n",
            "[3][299/784] Elapsed: 1m 38s (remain 2m 39s) Loss: 0.0813(0.0926) Grad: 305633.21875 LR: 0.00001359 \n",
            "[3][319/784] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0826(0.0925) Grad: 126442.8046875 LR: 0.00001339 \n",
            "[3][339/784] Elapsed: 1m 52s (remain 2m 26s) Loss: 0.0595(0.0918) Grad: 200796.53125 LR: 0.00001319 \n",
            "[3][359/784] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.0805(0.0932) Grad: 169703.640625 LR: 0.00001299 \n",
            "[3][379/784] Elapsed: 2m 5s (remain 2m 13s) Loss: 0.0558(0.0937) Grad: 209062.015625 LR: 0.00001280 \n",
            "[3][399/784] Elapsed: 2m 11s (remain 2m 6s) Loss: 0.1274(0.0938) Grad: 279346.125 LR: 0.00001261 \n",
            "[3][419/784] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0574(0.0942) Grad: 221027.140625 LR: 0.00001242 \n",
            "[3][439/784] Elapsed: 2m 24s (remain 1m 53s) Loss: 0.0669(0.0946) Grad: 271310.84375 LR: 0.00001223 \n",
            "[3][459/784] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1343(0.0947) Grad: 476161.6875 LR: 0.00001205 \n",
            "[3][479/784] Elapsed: 2m 38s (remain 1m 40s) Loss: 0.1232(0.0948) Grad: 364263.0 LR: 0.00001187 \n",
            "[3][499/784] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.0684(0.0946) Grad: 240897.515625 LR: 0.00001169 \n",
            "[3][519/784] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.1399(0.0943) Grad: 415655.625 LR: 0.00001152 \n",
            "[3][539/784] Elapsed: 2m 57s (remain 1m 20s) Loss: 0.0633(0.0940) Grad: 149101.671875 LR: 0.00001135 \n",
            "[3][559/784] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0813(0.0942) Grad: 216221.65625 LR: 0.00001118 \n",
            "[3][579/784] Elapsed: 3m 10s (remain 1m 7s) Loss: 0.0660(0.0937) Grad: 131409.5625 LR: 0.00001101 \n",
            "[3][599/784] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.0572(0.0937) Grad: 134360.6875 LR: 0.00001085 \n",
            "[3][619/784] Elapsed: 3m 24s (remain 0m 53s) Loss: 0.0714(0.0935) Grad: 149307.90625 LR: 0.00001068 \n",
            "[3][639/784] Elapsed: 3m 30s (remain 0m 47s) Loss: 0.0622(0.0931) Grad: 178312.15625 LR: 0.00001052 \n",
            "[3][659/784] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.0721(0.0931) Grad: 182385.203125 LR: 0.00001037 \n",
            "[3][679/784] Elapsed: 3m 43s (remain 0m 34s) Loss: 0.0979(0.0933) Grad: 154671.6875 LR: 0.00001021 \n",
            "[3][699/784] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.0686(0.0935) Grad: 190196.625 LR: 0.00001006 \n",
            "[3][719/784] Elapsed: 3m 57s (remain 0m 21s) Loss: 0.1521(0.0932) Grad: 303962.3125 LR: 0.00000991 \n",
            "[3][739/784] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.0839(0.0932) Grad: 337066.125 LR: 0.00000976 \n",
            "[3][759/784] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.1399(0.0934) Grad: 441530.0 LR: 0.00000962 \n",
            "[3][779/784] Elapsed: 4m 16s (remain 0m 1s) Loss: 0.1310(0.0935) Grad: 201565.28125 LR: 0.00000947 \n",
            "[4][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0890(0.1112) \n",
            "[4][39/194] Elapsed: 0m 8s (remain 0m 33s) Loss: 0.0748(0.1101) \n",
            "[4][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0702(0.1073) \n",
            "[4][79/194] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0529(0.1094) \n",
            "[4][99/194] Elapsed: 0m 21s (remain 0m 20s) Loss: 0.1022(0.1108) \n",
            "[4][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1475(0.1095) \n",
            "[4][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.0962(0.1074) \n",
            "[4][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0827(0.1076) \n",
            "[4][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0843(0.1089) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 4 - avg_train_loss = 0.0936 avg_val_loss: 0.1089 time: 299.3237s\n",
            "Epoch: 4 - Score: 0.4673, Scores: [0.49827034911734924, 0.4621370416893894, 0.4208501849435237, 0.46857882700439024, 0.4850973720746817, 0.469016626991304]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0712(0.1089) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:257: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/784] Elapsed: 0m 6s (remain 4m 14s) Loss: 0.0925(0.0964) Grad: 200985.03125 LR: 0.00000930 \n",
            "[4][39/784] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.0773(0.0922) Grad: 82997.375 LR: 0.00000916 \n",
            "[4][59/784] Elapsed: 0m 19s (remain 3m 58s) Loss: 0.0655(0.0924) Grad: 116106.9921875 LR: 0.00000903 \n",
            "[4][79/784] Elapsed: 0m 26s (remain 3m 51s) Loss: 0.0605(0.0913) Grad: 61096.40234375 LR: 0.00000889 \n",
            "[4][99/784] Elapsed: 0m 32s (remain 3m 45s) Loss: 0.0519(0.0920) Grad: 85458.9140625 LR: 0.00000876 \n",
            "[4][119/784] Elapsed: 0m 39s (remain 3m 38s) Loss: 0.0801(0.0924) Grad: 189732.9375 LR: 0.00000863 \n",
            "[4][139/784] Elapsed: 0m 46s (remain 3m 32s) Loss: 0.1062(0.0908) Grad: 148996.5625 LR: 0.00000850 \n",
            "[4][159/784] Elapsed: 0m 52s (remain 3m 25s) Loss: 0.0471(0.0893) Grad: 80307.5546875 LR: 0.00000837 \n",
            "[4][179/784] Elapsed: 0m 59s (remain 3m 18s) Loss: 0.0547(0.0886) Grad: 72383.2578125 LR: 0.00000825 \n",
            "[4][199/784] Elapsed: 1m 5s (remain 3m 12s) Loss: 0.0653(0.0882) Grad: 87686.078125 LR: 0.00000812 \n",
            "[4][219/784] Elapsed: 1m 12s (remain 3m 5s) Loss: 0.1295(0.0886) Grad: 154816.59375 LR: 0.00000800 \n",
            "[4][239/784] Elapsed: 1m 19s (remain 2m 59s) Loss: 0.0821(0.0885) Grad: 83061.6796875 LR: 0.00000788 \n",
            "[4][259/784] Elapsed: 1m 25s (remain 2m 52s) Loss: 0.0756(0.0880) Grad: 104313.25 LR: 0.00000777 \n",
            "[4][279/784] Elapsed: 1m 32s (remain 2m 45s) Loss: 0.1136(0.0885) Grad: 83920.015625 LR: 0.00000765 \n",
            "[4][299/784] Elapsed: 1m 38s (remain 2m 39s) Loss: 0.0903(0.0885) Grad: 118247.7109375 LR: 0.00000754 \n",
            "[4][319/784] Elapsed: 1m 45s (remain 2m 32s) Loss: 0.0519(0.0886) Grad: 134364.21875 LR: 0.00000742 \n",
            "[4][339/784] Elapsed: 1m 51s (remain 2m 26s) Loss: 0.1049(0.0886) Grad: 91902.7578125 LR: 0.00000731 \n",
            "[4][359/784] Elapsed: 1m 58s (remain 2m 19s) Loss: 0.1629(0.0891) Grad: 138407.25 LR: 0.00000720 \n",
            "[4][379/784] Elapsed: 2m 5s (remain 2m 12s) Loss: 0.0800(0.0893) Grad: 166701.234375 LR: 0.00000710 \n",
            "[4][399/784] Elapsed: 2m 11s (remain 2m 6s) Loss: 0.0380(0.0888) Grad: 62526.43359375 LR: 0.00000699 \n",
            "[4][419/784] Elapsed: 2m 18s (remain 1m 59s) Loss: 0.0968(0.0893) Grad: 80955.171875 LR: 0.00000688 \n",
            "[4][439/784] Elapsed: 2m 24s (remain 1m 53s) Loss: 0.0877(0.0899) Grad: 142044.09375 LR: 0.00000678 \n",
            "[4][459/784] Elapsed: 2m 31s (remain 1m 46s) Loss: 0.1095(0.0905) Grad: 160253.40625 LR: 0.00000668 \n",
            "[4][479/784] Elapsed: 2m 37s (remain 1m 40s) Loss: 0.0665(0.0906) Grad: 92804.9296875 LR: 0.00000658 \n",
            "[4][499/784] Elapsed: 2m 44s (remain 1m 33s) Loss: 0.1050(0.0906) Grad: 102425.4140625 LR: 0.00000648 \n",
            "[4][519/784] Elapsed: 2m 51s (remain 1m 26s) Loss: 0.1176(0.0909) Grad: 126473.0390625 LR: 0.00000639 \n",
            "[4][539/784] Elapsed: 2m 57s (remain 1m 20s) Loss: 0.0883(0.0907) Grad: 63778.64453125 LR: 0.00000629 \n",
            "[4][559/784] Elapsed: 3m 4s (remain 1m 13s) Loss: 0.0979(0.0914) Grad: 170838.921875 LR: 0.00000620 \n",
            "[4][579/784] Elapsed: 3m 10s (remain 1m 7s) Loss: 0.0844(0.0917) Grad: 102606.6640625 LR: 0.00000610 \n",
            "[4][599/784] Elapsed: 3m 17s (remain 1m 0s) Loss: 0.1082(0.0920) Grad: 111208.9296875 LR: 0.00000601 \n",
            "[4][619/784] Elapsed: 3m 24s (remain 0m 53s) Loss: 0.1258(0.0922) Grad: 197815.640625 LR: 0.00000592 \n",
            "[4][639/784] Elapsed: 3m 30s (remain 0m 47s) Loss: 0.0525(0.0923) Grad: 72694.3359375 LR: 0.00000583 \n",
            "[4][659/784] Elapsed: 3m 37s (remain 0m 40s) Loss: 0.0969(0.0921) Grad: 120743.9453125 LR: 0.00000575 \n",
            "[4][679/784] Elapsed: 3m 43s (remain 0m 34s) Loss: 0.2044(0.0923) Grad: 110404.3515625 LR: 0.00000566 \n",
            "[4][699/784] Elapsed: 3m 50s (remain 0m 27s) Loss: 0.1128(0.0923) Grad: 165340.09375 LR: 0.00000558 \n",
            "[4][719/784] Elapsed: 3m 56s (remain 0m 21s) Loss: 0.1103(0.0923) Grad: 141189.21875 LR: 0.00000549 \n",
            "[4][739/784] Elapsed: 4m 3s (remain 0m 14s) Loss: 0.1581(0.0926) Grad: 206160.046875 LR: 0.00000541 \n",
            "[4][759/784] Elapsed: 4m 10s (remain 0m 7s) Loss: 0.0900(0.0928) Grad: 78002.5546875 LR: 0.00000533 \n",
            "[4][779/784] Elapsed: 4m 16s (remain 0m 1s) Loss: 0.0967(0.0932) Grad: 99246.984375 LR: 0.00000525 \n",
            "[5][19/194] Elapsed: 0m 4s (remain 0m 37s) Loss: 0.0870(0.1036) \n",
            "[5][39/194] Elapsed: 0m 8s (remain 0m 32s) Loss: 0.1055(0.1042) \n",
            "[5][59/194] Elapsed: 0m 12s (remain 0m 28s) Loss: 0.0698(0.1023) \n",
            "[5][79/194] Elapsed: 0m 17s (remain 0m 24s) Loss: 0.0687(0.1039) \n",
            "[5][99/194] Elapsed: 0m 21s (remain 0m 19s) Loss: 0.1471(0.1057) \n",
            "[5][119/194] Elapsed: 0m 25s (remain 0m 15s) Loss: 0.1029(0.1041) \n",
            "[5][139/194] Elapsed: 0m 29s (remain 0m 11s) Loss: 0.1026(0.1028) \n",
            "[5][159/194] Elapsed: 0m 33s (remain 0m 7s) Loss: 0.0964(0.1032) \n",
            "[5][179/194] Elapsed: 0m 38s (remain 0m 2s) Loss: 0.0715(0.1044) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch: 5 - avg_train_loss = 0.0933 avg_val_loss: 0.1049 time: 299.1705s\n",
            "Epoch: 5 - Score: 0.4587, Scores: [0.4854830223515189, 0.4438361967028614, 0.41789888625461663, 0.46396889856107, 0.48186613262388917, 0.45892999541854645]\n",
            "Epoch 5 - Save Best Score: 0.4587 Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5][194/194] Elapsed: 0m 41s (remain -1m 59s) Loss: 0.0867(0.1049) \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3678: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  self[col] = igetitem(value, i)\n",
            "========== fold: 4 result ==========\n",
            "Score: 0.4587  Scores: [0.4854830223515189, 0.4438361967028614, 0.41789888625461663, 0.46396889856107, 0.48186613262388917, 0.45892999541854645]\n",
            "========== CV ==========\n",
            "Score: 0.4550  Scores: [0.4843101272329265, 0.4469884731288726, 0.41886618468616554, 0.4594054979227242, 0.4713665400029151, 0.4492003125655974]\n"
          ]
        }
      ],
      "source": [
        "def get_result(oof_df, fold, best_train_loss, best_val_loss):\n",
        "    labels = oof_df[CFG.target_cols].values\n",
        "    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n",
        "    score, scores = get_score(labels, preds)\n",
        "    LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n",
        "\n",
        "    return None\n",
        "\n",
        "oof_df = pd.DataFrame()\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "for fold in range(CFG.n_folds):\n",
        "    best_train_loss, best_val_loss, _oof_df, best_score = train_loop(df_train, fold, CFG)\n",
        "    oof_df = pd.concat([oof_df, _oof_df])\n",
        "    train_loss_list.append(best_train_loss)\n",
        "    val_loss_list.append(best_val_loss)\n",
        "\n",
        "    LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
        "\n",
        "    get_result(_oof_df, fold, best_train_loss, best_val_loss)\n",
        "\n",
        "oof_df = oof_df.reset_index(drop = True)\n",
        "LOGGER.info(f\"========== CV ==========\")\n",
        "get_result(oof_df, \"OOF\", np.mean(train_loss_list), np.mean(val_loss_list))\n",
        "oof_df.to_csv(CFG.OUTPUT_DIR + 'oof_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "d0lWOzgsVHnP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}