{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd drive/MyDrive/kaggle/notebook"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM0CH5Tr_QKM",
        "outputId": "f070a60b-4743-4ef0-d036-fcd0076e281a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[Errno 2] No such file or directory: 'drive/MyDrive/kaggle/notebook'\n",
            "/content/drive/MyDrive/kaggle/notebook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers==4.20.1\n",
        "!pip install tokenizers==0.12.1\n",
        "!pip install sentencepiece==0.1.97"
      ],
      "metadata": {
        "id": "VvALF3IfS1Ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gc\n",
        "import time\n",
        "import math\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "import transformers\n",
        "import tokenizers\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "\n",
        "os.system('pip install iterative-stratification==0.1.7')\n",
        "os.environ['TOKENIZERS_PARALLELISM']='true'\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-12-02T13:17:27.374897Z",
          "iopub.execute_input": "2022-12-02T13:17:27.375492Z"
        },
        "trusted": true,
        "id": "WhmJrRr1_MvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CFG:\n",
        "    model_name  = \"microsoft/deberta-v3-large\"\n",
        "    \n",
        "    base        = \"../input/feedback-prize-english-language-learning/\"\n",
        "    train       = base + \"train.csv\"\n",
        "    test        = base + \"test.csv\"\n",
        "    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n",
        "    num_targets = 6\n",
        "    \n",
        "    # Backbone\n",
        "    max_length  = 512\n",
        "    hidden_dims = 1024\n",
        "    output_hidden_states = True\n",
        "    pool = 'attention'\n",
        "    \n",
        "    # loss\n",
        "    loss = 'L1smooth'\n",
        "    apex = True\n",
        "    max_norm = 300\n",
        "    \n",
        "    # optimizer\n",
        "    default_lr = 2e-5\n",
        "    head_lr    = 3e-4\n",
        "    adam_eps   = 1e-5\n",
        "    \n",
        "    # scheduler\n",
        "    scheduler = 'linear'\n",
        "    num_warmup_steps = 0.333333\n",
        "    \n",
        "    # CV\n",
        "    n_folds = 5\n",
        "    seed   = 42\n",
        "    epochs = 5\n",
        "    \n",
        "    # Loader\n",
        "    batch_size  = 2\n",
        "    num_workers = 2\n",
        "    print_freq = 20\n",
        "\n",
        "    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')"
      ],
      "metadata": {
        "trusted": true,
        "id": "uXo99OVW_Mvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(CFG.model_name.split(\"/\")) == 2:\n",
        "   CFG.identifier = f\"{CFG.str_now}-{CFG.model_name.split('/')[1]}\"\n",
        "else:\n",
        "   CFG.identifier = f\"{CFG.str_now}-{CFG.model_name}\"\n",
        "\n",
        "CFG.OUTPUT_DIR = f'./{CFG.identifier}/'\n",
        "os.makedirs(CFG.OUTPUT_DIR, exist_ok = True)"
      ],
      "metadata": {
        "id": "82eXqEicr0r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(CFG.train)"
      ],
      "metadata": {
        "trusted": true,
        "id": "EBJTWKqf_Mvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold"
      ],
      "metadata": {
        "trusted": true,
        "id": "2bNvgiep_Mve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fold = MultilabelStratifiedKFold(n_splits = CFG.n_folds, shuffle = True, random_state = CFG.seed)\n",
        "df   = df_train.copy()\n",
        "y    = pd.get_dummies(data = df[CFG.target_cols], columns = CFG.target_cols)\n",
        "\n",
        "for idx, (train_idx, valid_idx) in enumerate(fold.split(df_train, y)):\n",
        "    df_train.loc[valid_idx, 'fold'] = idx"
      ],
      "metadata": {
        "trusted": true,
        "id": "fEyYoSId_Mvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name)\n",
        "tokenizer.add_tokens([\"\\n\"], special_tokens=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "tMKFr56B_Mvi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50500c14-a1bd-4ee1-fb29-da60a56f09dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "             \n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "        \n",
        "    def update(self, val, n = 1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return f'{int(m)}m {int(s)}s'\n",
        "        \n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    \n",
        "    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})'\n",
        "        \n",
        "def prepare_inputs(texts):\n",
        "    inputs = []\n",
        "    for text in texts:\n",
        "        tokens = tokenizer.encode_plus(text,\n",
        "                                       add_special_tokens = True,\n",
        "                                       max_length = CFG.max_length,\n",
        "                                       pad_to_max_length = True,\n",
        "                                       truncation = True,\n",
        "                                       return_attention_mask = True\n",
        "                                       )\n",
        "        \n",
        "        for key, vals in tokens.items():\n",
        "            tokens[key] = torch.tensor(vals, dtype = torch.long)\n",
        "        inputs.append(tokens)\n",
        "    \n",
        "    return inputs\n",
        "\n",
        "class MeanPooling(nn.Module):\n",
        "      def __init__(self):\n",
        "          super(MeanPooling, self).__init__()\n",
        "\n",
        "      def forward(self, inputs, attention_masks):\n",
        "          #print(\"inputs.shape: \", inputs.shape, \" masks.shape: \", attention_masks.shape)\n",
        "          attention_masks = attention_masks.unsqueeze(-1).float()\n",
        "          inputs   = torch.sum(inputs*attention_masks, dim = 1)\n",
        "          num_masks = torch.sum(attention_masks == 1.0, dim = 1)\n",
        "          torch.clamp(num_masks, min = 1e-9)\n",
        "          inputs   = (inputs / num_masks)\n",
        "\n",
        "          return inputs\n",
        "\n",
        "class AttentionPooling(nn.Module):\n",
        "      def __init__(self, hidden_size):\n",
        "          super(AttentionPooling, self).__init__()\n",
        "          self.attention = nn.Sequential(nn.Linear(hidden_size, hidden_size),\n",
        "                                         nn.GELU(),\n",
        "                                         nn.Linear(hidden_size, 1)\n",
        "                                         )\n",
        "\n",
        "      def forward(self, hidden_state, attention_mask):\n",
        "          weights = self.attention(hidden_state)\n",
        "          weights[attention_mask == 0] = float(\"-inf\")\n",
        "          weights = torch.softmax(weights, dim = 1)\n",
        "          context = torch.sum(hidden_state * weights, dim = 1)\n",
        "\n",
        "          return context\n",
        "\n",
        "def MCRMSE(labels, preds):\n",
        "    scores = []\n",
        "    num_targets = labels.shape[1]\n",
        "    \n",
        "    for i in range(num_targets):\n",
        "        pred  = preds[:,i]\n",
        "        label = labels[:,i]\n",
        "        score = mean_squared_error(label, pred, squared = False)\n",
        "        scores.append(score)\n",
        "    \n",
        "    mcrmse = np.mean(scores)\n",
        "    \n",
        "    return mcrmse, scores \n",
        "\n",
        "def get_score(labels, preds):\n",
        "    return MCRMSE(labels, preds)\n",
        "        "
      ],
      "metadata": {
        "trusted": true,
        "id": "vP0hQ3g3_Mvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FB3Dataset(Dataset):\n",
        "    def __init__(self, df_train):\n",
        "        self.inputs = prepare_inputs(df_train['full_text'])\n",
        "        self.labels = torch.tensor(df_train[CFG.target_cols].values, dtype = torch.float)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.inputs)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.inputs[idx], self.labels[idx]"
      ],
      "metadata": {
        "trusted": true,
        "id": "1bMw-Mip_Mvl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FB3Model(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.model      = AutoModel.from_pretrained(cfg.model_name, \n",
        "                                                    output_hidden_states=cfg.output_hidden_states)\n",
        "        self.cfg        = cfg\n",
        "\n",
        "        if self.cfg.pool == 'mean':\n",
        "          self.pool       = MeanPooling()\n",
        "          self.classifier = nn.Linear(cfg.hidden_dims, cfg.num_targets)\n",
        "        elif self.cfg.pool == 'attention':\n",
        "          hidden_size = cfg.hidden_dims * 4\n",
        "          self.lstm = nn.LSTM(input_size = hidden_size, \n",
        "                              hidden_size = hidden_size // 2,\n",
        "                              batch_first = True,\n",
        "                              bidirectional = True)\n",
        "          self.pool = AttentionPooling(hidden_size)\n",
        "          #self.pool       = MeanPooling()\n",
        "        \n",
        "          self.classifier = nn.Linear(hidden_size, cfg.num_targets)\n",
        "    \n",
        "    def feature(self, inputs):\n",
        "        outputs = self.model(**inputs)\n",
        "\n",
        "        if self.cfg.pool == 'mean':\n",
        "           feature = self.pool(outputs.last_hidden_state, inputs['attention_mask'])\n",
        "        elif self.cfg.pool == 'attention':\n",
        "           hidden_states = outputs.hidden_states\n",
        "           #print(\"hidden_states.shape: \", len(hidden_states))\n",
        "           stacked_outputs = torch.cat([hidden_states[-i-1] for i in range(4)], dim = -1)\n",
        "           #print(\"stacked_outputs.shape: \", stacked_outputs.shape)\n",
        "           feature, _ = self.lstm(stacked_outputs)\n",
        "           #print(\"feature.shape: \", feature.shape)\n",
        "           feature = self.pool(feature, inputs['attention_mask'])\n",
        "           #print(\"last.shape: \", feature.shape)\n",
        "        \n",
        "        return feature\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "\n",
        "        feature = self.feature(inputs)\n",
        "        outputs = self.classifier(feature)\n",
        "        \n",
        "        return outputs\n",
        "    "
      ],
      "metadata": {
        "trusted": true,
        "id": "lvyMXacu_Mvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, criterion, optimizer, scheduler, train_loader, epoch, cfg):\n",
        "    losses = AverageMeter()\n",
        "    total_steps = len(train_loader)\n",
        "    start = end = time.time()\n",
        "    model.train()\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=cfg.apex)\n",
        "    \n",
        "    for step, (inputs, labels) in enumerate(train_loader):\n",
        "        #print(\"inputs.shape: \", inputs['input_ids'].shape)\n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        with torch.cuda.amp.autocast(enabled = cfg.apex):\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "        losses.update(loss.item(), labels.shape[0])\n",
        "        \n",
        "        scaler.scale(loss).backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), cfg.max_norm)\n",
        "          \n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        optimizer.zero_grad()\n",
        "        scheduler.step()\n",
        "        \n",
        "        end = time.time()\n",
        "        \n",
        "        if (step + 1) % cfg.print_freq == 0:\n",
        "            print(\"[{0}][{1}/{2}] \"\n",
        "                  \"Elapsed: {remain:s} \"\n",
        "                  \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
        "                  \"Grad: {grad_norm} \"\n",
        "                  \"LR: {lr:.8f} \"\n",
        "                  .format(epoch, step, total_steps, remain = timeSince(start, float(step+1)/total_steps),\n",
        "                          loss = losses,\n",
        "                          grad_norm = grad_norm,\n",
        "                          lr = scheduler.get_lr()[0]\n",
        "                          )\n",
        "                 )\n",
        "    \n",
        "    return losses.avg\n",
        "\n",
        "def valid_fn(model, criterion, valid_loader, epoch, cfg):\n",
        "    losses = AverageMeter()\n",
        "    predictions = []\n",
        "    total_steps = len(valid_loader) - 1\n",
        "    model.eval()\n",
        "    start = end = time.time()\n",
        "    \n",
        "    for step, (inputs, labels) in enumerate(valid_loader):\n",
        "        \n",
        "        for k, v in inputs.items():\n",
        "            inputs[k] = v.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            preds = model(inputs)\n",
        "        \n",
        "        loss = criterion(preds, labels)\n",
        "        losses.update(loss.item(), labels.shape[0])\n",
        "        \n",
        "        predictions.append(preds.detach().cpu().numpy())\n",
        "        \n",
        "        if (step + 1) % cfg.print_freq == 0 or step == total_steps:\n",
        "            print(\"[{0}][{1}/{2}] \"\n",
        "                  \"Elapsed: {remain:s} \"\n",
        "                  \"Loss: {loss.val:.4f}({loss.avg:.4f}) \"\n",
        "                  .format(epoch, step, total_steps, remain = timeSince(start, float(step+1)/total_steps),\n",
        "                          loss = losses,\n",
        "                          )\n",
        "                 )\n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis = 0)\n",
        "    \n",
        "    return losses.avg, predictions"
      ],
      "metadata": {
        "trusted": true,
        "id": "qElgZ6d-_Mvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def train_loop(folds, fold, cfg):\n",
        "    train_folds  = folds[folds['fold'] != fold]\n",
        "    valid_folds  = folds[folds['fold'] == fold]\n",
        "    valid_labels = valid_folds[cfg.target_cols].values\n",
        "    \n",
        "    train_dataset = FB3Dataset(train_folds)\n",
        "    valid_dataset = FB3Dataset(valid_folds)\n",
        "    \n",
        "    train_loader  = DataLoader(train_dataset, \n",
        "                               batch_size  = cfg.batch_size,\n",
        "                               shuffle = True,\n",
        "                               pin_memory = True,\n",
        "                               num_workers = cfg.num_workers)\n",
        "    \n",
        "    valid_loader  = DataLoader(valid_dataset, \n",
        "                               batch_size = cfg.batch_size,\n",
        "                               shuffle = False,\n",
        "                               pin_memory = True,\n",
        "                               num_workers = cfg.num_workers)\n",
        "    \n",
        "    def get_optimizer(cfg, model):\n",
        "        \n",
        "        all_parameters = list(model.named_parameters())\n",
        "        used_name_parameters = set()\n",
        "        params = []\n",
        "        named_params = []\n",
        "        no_wd = ['word_embeddings', 'bias', 'LayerNorm.weight']\n",
        "\n",
        "        head   = [(n, p) for n, p in model.named_parameters() if not 'model' in n]\n",
        "        for n, _ in head: used_name_parameters.add(n)\n",
        "  \n",
        "        params.append({\"params\": [p for n, p in head if not any(no in n for no in no_wd)], \"weight_decay\": 0.01, \"lr\": cfg.head_lr})\n",
        "        named_params.append({\"params\": [n for n, p in head if not any(no in n for no in no_wd)], \"weight_decay\": 0.01, \"lr\": cfg.head_lr})\n",
        "        params.append({\"params\": [p for n, p in head if any(no in n for no in no_wd)], \"weight_decay\": 0.0, \"lr\": cfg.head_lr})\n",
        "        named_params.append({\"params\": [n for n, p in head if any(no in n for no in no_wd)], \"weight_decay\": 0.0, \"lr\": cfg.head_lr})\n",
        "\n",
        "        backbone = [(n, p) for n, p in model.named_parameters() if 'model' in n]\n",
        "        \n",
        "        groups = [\n",
        "            [ [\".embeddings.\"],                           1e-6],\n",
        "            [ [\"encoder.LayerNorm\", \"rel_embeddings\"],    1e-6],\n",
        "            [ [\".\" + str(i) + \".\" for i in range(0,6)],   1e-8],\n",
        "            [ [\".\" + str(i) + \".\" for i in range(6,12)],  1e-7],\n",
        "            [ [\".\" + str(i) + \".\" for i in range(12,23)], 1e-6],\n",
        "            [ [\".23.\"], cfg.head_lr]\n",
        "        ]\n",
        "\n",
        "        for group in groups:\n",
        "            names, lr = group[0], group[1]\n",
        "            print(\"names: \", names)\n",
        "            parameters = [(n, p) for n, p in backbone if any(name in n for name in names)]\n",
        "            for n, _ in parameters: used_name_parameters.add(n)\n",
        "\n",
        "            params.append({\"params\": [p for n, p in parameters if not any(no in n for no in no_wd)], 'weight_decay': 0.01, \"lr\": lr})\n",
        "            named_params.append({\"params\": [n for n, p in parameters if not any(no in n for no in no_wd)], 'weight_decay': 0.01, \"lr\": lr})\n",
        "            params.append({\"params\": [p for n, p in parameters if any(no in n for no in no_wd)], 'weight_decay': 0.0, \"lr\": lr})\n",
        "            named_params.append({\"params\": [n for n, p in parameters if any(no in n for no in no_wd)], 'weight_decay': 0.0, \"lr\": lr})\n",
        "        \n",
        "\n",
        "        param_dict = defaultdict()\n",
        "        for param in params:\n",
        "            for _param in param['params']:\n",
        "                key = id(_param)\n",
        "                param_dict[key] = param_dict.get(key, 0) + 1\n",
        "        \n",
        "        for key, val in param_dict.items():\n",
        "           if val > 1: print(\"key: \", key, \" val: \", val)\n",
        "        optimizer = torch.optim.AdamW(params, eps = cfg.adam_eps)\n",
        "        \n",
        "        state_dict_keys = {n:p for n, p in all_parameters}.keys()\n",
        "        assert(len(state_dict_keys - used_name_parameters) == 0), \\\n",
        "               f\"Missing parameters: {str(state_dict_keys-used_name_parameters)}\"\n",
        "\n",
        "        return optimizer\n",
        "    \n",
        "    def get_scheduler(cfg, optimizer, num_train_steps):\n",
        "        warmup_start = int(cfg.epochs * num_train_steps)\n",
        "        print(\"warmup start: \", warmup_start)\n",
        "        scheduler = transformers.get_scheduler(cfg.scheduler, optimizer, warmup_start, num_train_steps)\n",
        "        \n",
        "        return scheduler\n",
        "    \n",
        "    model = FB3Model(cfg).to(device)\n",
        "    criterion = nn.SmoothL1Loss(reduction = 'mean')\n",
        "    optimizer = get_optimizer(cfg, model)\n",
        "    scheduler = get_scheduler(cfg, optimizer, len(train_loader))\n",
        "\n",
        "    best_score = np.inf\n",
        "    best_val_loss = np.inf\n",
        "    best_train_loss = np.inf\n",
        "\n",
        "    for epoch in range(cfg.epochs):\n",
        "        \n",
        "        start = time.time()\n",
        "        \n",
        "        avg_loss = train_fn(model, criterion, optimizer, scheduler, train_loader, epoch, cfg)\n",
        "        \n",
        "        avg_val_loss, preds = valid_fn(model, criterion, valid_loader, epoch, cfg)\n",
        "        score, scores = get_score(preds, valid_labels)\n",
        "        \n",
        "        elapsed = time.time() - start\n",
        "        \n",
        "        print(f\"Epoch: {epoch+1} - avg_train_loss = {avg_loss:.4f} avg_val_loss: {avg_val_loss:.4f} time: {elapsed:.4f}s\")\n",
        "        print(f\"Epoch: {epoch+1} - Score: {score:.4f}, Scores: {scores}\")\n",
        "\n",
        "        if best_score > score:\n",
        "           best_score = score\n",
        "           best_train_loss = avg_loss\n",
        "           bset_val_loss = avg_val_loss\n",
        "           \n",
        "           torch.save({'model': model.state_dict(),\n",
        "                       'predictions': preds},\n",
        "                      CFG.OUTPUT_DIR + f\"{CFG.model_name.replace('/', '-')}_fold{fold}_epoch{epoch+1}.pth\")\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    return best_train_loss, best_val_loss, best_score"
      ],
      "metadata": {
        "trusted": true,
        "id": "WtvABLSm_Mvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for fold in range(CFG.n_folds):\n",
        "    best_train_loss, best_val_loss, best_score = train_loop(df_train, fold, CFG)"
      ],
      "metadata": {
        "trusted": true,
        "id": "9jx_34Ji_Mvr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea2b54bf-97c9-4ff0-ca84-12aa585030bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names:  ['.embeddings.']\n",
            "names:  ['encoder.LayerNorm', 'rel_embeddings']\n",
            "names:  ['.0.', '.1.', '.2.', '.3.', '.4.', '.5.']\n",
            "names:  ['.6.', '.7.', '.8.', '.9.', '.10.', '.11.']\n",
            "names:  ['.12.', '.13.', '.14.', '.15.', '.16.', '.17.', '.18.', '.19.', '.20.', '.21.', '.22.']\n",
            "names:  ['.23.']\n",
            "warmup start:  7825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/1565] Elapsed: 0m 20s (remain 26m 59s) Loss: 2.6929(2.4295) Grad: 419478.875 LR: 0.00000077 \n",
            "[0][39/1565] Elapsed: 0m 41s (remain 26m 23s) Loss: 2.3067(2.4609) Grad: 417741.875 LR: 0.00000153 \n",
            "[0][59/1565] Elapsed: 1m 1s (remain 25m 53s) Loss: 2.1510(2.4950) Grad: 419321.65625 LR: 0.00000230 \n",
            "[0][79/1565] Elapsed: 1m 22s (remain 25m 28s) Loss: 2.6312(2.4824) Grad: 415949.15625 LR: 0.00000307 \n",
            "[0][99/1565] Elapsed: 1m 42s (remain 25m 6s) Loss: 2.5555(2.4301) Grad: 401013.4375 LR: 0.00000383 \n",
            "[0][119/1565] Elapsed: 2m 3s (remain 24m 45s) Loss: 2.0924(2.4091) Grad: 415186.6875 LR: 0.00000460 \n",
            "[0][139/1565] Elapsed: 2m 23s (remain 24m 23s) Loss: 2.8557(2.3796) Grad: 419205.03125 LR: 0.00000537 \n",
            "[0][159/1565] Elapsed: 2m 44s (remain 24m 2s) Loss: 1.7566(2.3601) Grad: 414584.375 LR: 0.00000613 \n",
            "[0][179/1565] Elapsed: 3m 4s (remain 23m 41s) Loss: 2.2624(2.3205) Grad: 435310.8125 LR: 0.00000690 \n",
            "[0][199/1565] Elapsed: 3m 25s (remain 23m 20s) Loss: 1.8345(2.2596) Grad: 424898.3125 LR: 0.00000767 \n",
            "[0][219/1565] Elapsed: 3m 45s (remain 22m 59s) Loss: 1.7567(2.2219) Grad: 441989.40625 LR: 0.00000843 \n",
            "[0][239/1565] Elapsed: 4m 6s (remain 22m 38s) Loss: 1.2220(2.1836) Grad: 463454.34375 LR: 0.00000920 \n",
            "[0][259/1565] Elapsed: 4m 26s (remain 22m 17s) Loss: 1.1034(2.1164) Grad: 404546.90625 LR: 0.00000997 \n",
            "[0][279/1565] Elapsed: 4m 46s (remain 21m 56s) Loss: 1.6003(2.0663) Grad: 482276.65625 LR: 0.00001073 \n",
            "[0][299/1565] Elapsed: 5m 7s (remain 21m 35s) Loss: 1.0291(2.0087) Grad: 495167.375 LR: 0.00001150 \n",
            "[0][319/1565] Elapsed: 5m 27s (remain 21m 15s) Loss: 0.9641(1.9464) Grad: 482110.96875 LR: 0.00001227 \n",
            "[0][339/1565] Elapsed: 5m 48s (remain 20m 54s) Loss: 0.6703(1.8819) Grad: 502283.40625 LR: 0.00001304 \n",
            "[0][359/1565] Elapsed: 6m 8s (remain 20m 34s) Loss: 1.1940(1.8094) Grad: 550893.0625 LR: 0.00001380 \n",
            "[0][379/1565] Elapsed: 6m 29s (remain 20m 13s) Loss: 0.6618(1.7368) Grad: 480293.0625 LR: 0.00001457 \n",
            "[0][399/1565] Elapsed: 6m 49s (remain 19m 53s) Loss: 0.1130(1.6652) Grad: 178563.984375 LR: 0.00001534 \n",
            "[0][419/1565] Elapsed: 7m 10s (remain 19m 32s) Loss: 0.1003(1.5978) Grad: 178198.359375 LR: 0.00001610 \n",
            "[0][439/1565] Elapsed: 7m 30s (remain 19m 11s) Loss: 0.2003(1.5364) Grad: 230836.5625 LR: 0.00001687 \n",
            "[0][459/1565] Elapsed: 7m 51s (remain 18m 51s) Loss: 0.1906(1.4787) Grad: 243406.9375 LR: 0.00001764 \n",
            "[0][479/1565] Elapsed: 8m 11s (remain 18m 30s) Loss: 0.3232(1.4258) Grad: 314380.21875 LR: 0.00001840 \n",
            "[0][499/1565] Elapsed: 8m 31s (remain 18m 10s) Loss: 0.5697(1.3755) Grad: 544610.375 LR: 0.00001917 \n",
            "[0][519/1565] Elapsed: 8m 52s (remain 17m 49s) Loss: 0.0866(1.3281) Grad: 136612.828125 LR: 0.00001994 \n",
            "[0][539/1565] Elapsed: 9m 12s (remain 17m 29s) Loss: 0.1290(1.2849) Grad: 263500.25 LR: 0.00002070 \n",
            "[0][559/1565] Elapsed: 9m 33s (remain 17m 8s) Loss: 0.0748(1.2485) Grad: 207850.78125 LR: 0.00002147 \n",
            "[0][579/1565] Elapsed: 9m 54s (remain 16m 48s) Loss: 0.4704(1.2126) Grad: 506030.96875 LR: 0.00002224 \n",
            "[0][599/1565] Elapsed: 10m 14s (remain 16m 28s) Loss: 0.3522(1.1782) Grad: 396863.375 LR: 0.00002300 \n",
            "[0][619/1565] Elapsed: 10m 34s (remain 16m 7s) Loss: 0.3213(1.1460) Grad: 150988.03125 LR: 0.00002377 \n",
            "[0][639/1565] Elapsed: 10m 55s (remain 15m 47s) Loss: 0.0940(1.1163) Grad: 191625.21875 LR: 0.00002454 \n",
            "[0][659/1565] Elapsed: 11m 15s (remain 15m 26s) Loss: 0.1281(1.0884) Grad: 217271.203125 LR: 0.00002530 \n",
            "[0][679/1565] Elapsed: 11m 36s (remain 15m 6s) Loss: 0.2123(1.0623) Grad: 185427.875 LR: 0.00002607 \n",
            "[0][699/1565] Elapsed: 11m 56s (remain 14m 45s) Loss: 0.0931(1.0367) Grad: 192131.78125 LR: 0.00002684 \n",
            "[0][719/1565] Elapsed: 12m 17s (remain 14m 25s) Loss: 0.4900(1.0127) Grad: 247343.234375 LR: 0.00002760 \n",
            "[0][739/1565] Elapsed: 12m 37s (remain 14m 4s) Loss: 0.0995(0.9909) Grad: 136942.96875 LR: 0.00002837 \n",
            "[0][759/1565] Elapsed: 12m 58s (remain 13m 44s) Loss: 0.0692(0.9687) Grad: 98605.078125 LR: 0.00002914 \n",
            "[0][779/1565] Elapsed: 13m 18s (remain 13m 23s) Loss: 0.0688(0.9469) Grad: 140334.5 LR: 0.00002990 \n",
            "[0][799/1565] Elapsed: 13m 39s (remain 13m 3s) Loss: 0.0680(0.9276) Grad: 146293.328125 LR: 0.00003067 \n",
            "[0][819/1565] Elapsed: 13m 59s (remain 12m 42s) Loss: 0.1394(0.9092) Grad: 146999.921875 LR: 0.00003144 \n",
            "[0][839/1565] Elapsed: 14m 19s (remain 12m 22s) Loss: 0.1556(0.8915) Grad: 209468.359375 LR: 0.00003220 \n",
            "[0][859/1565] Elapsed: 14m 40s (remain 12m 1s) Loss: 0.1319(0.8745) Grad: 212939.703125 LR: 0.00003297 \n",
            "[0][879/1565] Elapsed: 15m 0s (remain 11m 41s) Loss: 0.0960(0.8579) Grad: 201021.265625 LR: 0.00003374 \n",
            "[0][899/1565] Elapsed: 15m 21s (remain 11m 20s) Loss: 0.1167(0.8424) Grad: 217309.65625 LR: 0.00003450 \n",
            "[0][919/1565] Elapsed: 15m 41s (remain 11m 0s) Loss: 0.3180(0.8283) Grad: 444507.96875 LR: 0.00003527 \n",
            "[0][939/1565] Elapsed: 16m 2s (remain 10m 39s) Loss: 0.1808(0.8143) Grad: 372160.4375 LR: 0.00003604 \n",
            "[0][959/1565] Elapsed: 16m 22s (remain 10m 19s) Loss: 0.1844(0.8014) Grad: 398756.84375 LR: 0.00003681 \n",
            "[0][979/1565] Elapsed: 16m 43s (remain 9m 58s) Loss: 0.0646(0.7890) Grad: 89313.984375 LR: 0.00003757 \n",
            "[0][999/1565] Elapsed: 17m 3s (remain 9m 38s) Loss: 0.1525(0.7764) Grad: 274413.78125 LR: 0.00003834 \n",
            "[0][1019/1565] Elapsed: 17m 24s (remain 9m 18s) Loss: 0.0652(0.7639) Grad: 193552.875 LR: 0.00003911 \n",
            "[0][1039/1565] Elapsed: 17m 44s (remain 8m 57s) Loss: 0.0893(0.7511) Grad: 100934.8359375 LR: 0.00003987 \n",
            "[0][1059/1565] Elapsed: 18m 5s (remain 8m 37s) Loss: 0.0989(0.7403) Grad: 150662.359375 LR: 0.00004064 \n",
            "[0][1079/1565] Elapsed: 18m 25s (remain 8m 16s) Loss: 0.0763(0.7295) Grad: 220126.78125 LR: 0.00004141 \n",
            "[0][1099/1565] Elapsed: 18m 46s (remain 7m 56s) Loss: 0.0389(0.7186) Grad: 130280.8359375 LR: 0.00004217 \n",
            "[0][1119/1565] Elapsed: 19m 6s (remain 7m 35s) Loss: 0.0686(0.7081) Grad: 158583.390625 LR: 0.00004294 \n",
            "[0][1139/1565] Elapsed: 19m 27s (remain 7m 15s) Loss: 0.1198(0.6983) Grad: 394648.8125 LR: 0.00004371 \n",
            "[0][1159/1565] Elapsed: 19m 47s (remain 6m 54s) Loss: 0.0676(0.6887) Grad: 128279.296875 LR: 0.00004447 \n",
            "[0][1179/1565] Elapsed: 20m 8s (remain 6m 34s) Loss: 0.0469(0.6788) Grad: 159293.8125 LR: 0.00004524 \n",
            "[0][1199/1565] Elapsed: 20m 28s (remain 6m 13s) Loss: 0.1410(0.6697) Grad: 189119.3125 LR: 0.00004601 \n",
            "[0][1219/1565] Elapsed: 20m 48s (remain 5m 53s) Loss: 0.0676(0.6607) Grad: 227349.59375 LR: 0.00004677 \n",
            "[0][1239/1565] Elapsed: 21m 9s (remain 5m 32s) Loss: 0.0601(0.6518) Grad: 192070.453125 LR: 0.00004754 \n",
            "[0][1259/1565] Elapsed: 21m 29s (remain 5m 12s) Loss: 0.1479(0.6439) Grad: 197838.078125 LR: 0.00004831 \n",
            "[0][1279/1565] Elapsed: 21m 50s (remain 4m 51s) Loss: 0.2959(0.6362) Grad: 373205.28125 LR: 0.00004907 \n",
            "[0][1299/1565] Elapsed: 22m 10s (remain 4m 31s) Loss: 0.2541(0.6286) Grad: 479668.4375 LR: 0.00004984 \n",
            "[0][1319/1565] Elapsed: 22m 31s (remain 4m 10s) Loss: 0.1070(0.6210) Grad: 234243.484375 LR: 0.00005061 \n",
            "[0][1339/1565] Elapsed: 22m 51s (remain 3m 50s) Loss: 0.0668(0.6132) Grad: 120820.171875 LR: 0.00005137 \n",
            "[0][1359/1565] Elapsed: 23m 12s (remain 3m 29s) Loss: 0.0984(0.6065) Grad: 183540.65625 LR: 0.00005214 \n",
            "[0][1379/1565] Elapsed: 23m 32s (remain 3m 9s) Loss: 0.1062(0.5995) Grad: 348097.28125 LR: 0.00005291 \n",
            "[0][1399/1565] Elapsed: 23m 53s (remain 2m 48s) Loss: 0.1356(0.5925) Grad: 224426.921875 LR: 0.00005367 \n",
            "[0][1419/1565] Elapsed: 24m 13s (remain 2m 28s) Loss: 0.0696(0.5859) Grad: 115845.3828125 LR: 0.00005444 \n",
            "[0][1439/1565] Elapsed: 24m 34s (remain 2m 7s) Loss: 0.0370(0.5795) Grad: 109808.1640625 LR: 0.00005521 \n",
            "[0][1459/1565] Elapsed: 24m 54s (remain 1m 47s) Loss: 0.0728(0.5731) Grad: 133812.984375 LR: 0.00005597 \n",
            "[0][1479/1565] Elapsed: 25m 15s (remain 1m 27s) Loss: 0.1046(0.5671) Grad: 244031.4375 LR: 0.00005674 \n",
            "[0][1499/1565] Elapsed: 25m 35s (remain 1m 6s) Loss: 0.0925(0.5611) Grad: 137787.75 LR: 0.00005751 \n",
            "[0][1519/1565] Elapsed: 25m 56s (remain 0m 46s) Loss: 0.2658(0.5554) Grad: 309623.6875 LR: 0.00005827 \n",
            "[0][1539/1565] Elapsed: 26m 16s (remain 0m 25s) Loss: 0.0881(0.5496) Grad: 282388.78125 LR: 0.00005904 \n",
            "[0][1559/1565] Elapsed: 26m 37s (remain 0m 5s) Loss: 0.1122(0.5441) Grad: 224886.140625 LR: 0.00005981 \n",
            "[0][19/390] Elapsed: 0m 13s (remain 4m 2s) Loss: 0.1674(0.1144) \n",
            "[0][39/390] Elapsed: 0m 25s (remain 3m 47s) Loss: 0.0986(0.1079) \n",
            "[0][59/390] Elapsed: 0m 38s (remain 3m 34s) Loss: 0.2234(0.1175) \n",
            "[0][79/390] Elapsed: 0m 51s (remain 3m 20s) Loss: 0.1923(0.1151) \n",
            "[0][99/390] Elapsed: 1m 4s (remain 3m 7s) Loss: 0.1019(0.1192) \n",
            "[0][119/390] Elapsed: 1m 17s (remain 2m 54s) Loss: 0.1798(0.1217) \n",
            "[0][139/390] Elapsed: 1m 30s (remain 2m 41s) Loss: 0.0454(0.1200) \n",
            "[0][159/390] Elapsed: 1m 43s (remain 2m 28s) Loss: 0.1092(0.1178) \n",
            "[0][179/390] Elapsed: 1m 56s (remain 2m 15s) Loss: 0.0412(0.1180) \n",
            "[0][199/390] Elapsed: 2m 9s (remain 2m 2s) Loss: 0.1017(0.1168) \n",
            "[0][219/390] Elapsed: 2m 22s (remain 1m 49s) Loss: 0.1257(0.1177) \n",
            "[0][239/390] Elapsed: 2m 35s (remain 1m 36s) Loss: 0.0986(0.1182) \n",
            "[0][259/390] Elapsed: 2m 47s (remain 1m 23s) Loss: 0.1083(0.1193) \n",
            "[0][279/390] Elapsed: 3m 0s (remain 1m 11s) Loss: 0.0720(0.1205) \n",
            "[0][299/390] Elapsed: 3m 13s (remain 0m 58s) Loss: 0.0470(0.1217) \n",
            "[0][319/390] Elapsed: 3m 26s (remain 0m 45s) Loss: 0.0821(0.1207) \n",
            "[0][339/390] Elapsed: 3m 39s (remain 0m 32s) Loss: 0.1755(0.1217) \n",
            "[0][359/390] Elapsed: 3m 52s (remain 0m 19s) Loss: 0.1048(0.1235) \n",
            "[0][379/390] Elapsed: 4m 5s (remain 0m 6s) Loss: 0.2104(0.1231) \n",
            "[0][390/390] Elapsed: 4m 12s (remain -1m 59s) Loss: 0.0531(0.1228) \n",
            "Epoch: 1 - avg_train_loss = 0.5428 avg_val_loss: 0.1228 time: 1854.5773s\n",
            "Epoch: 1 - Score: 0.4966, Scores: [0.5511371401313914, 0.502525435370026, 0.4490807745144381, 0.47573754125468865, 0.4822711492930764, 0.5190390351198458]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/1565] Elapsed: 0m 20s (remain 26m 39s) Loss: 0.0684(0.1264) Grad: 164123.140625 LR: 0.00006077 \n",
            "[1][39/1565] Elapsed: 0m 41s (remain 26m 14s) Loss: 0.2191(0.1222) Grad: 239148.359375 LR: 0.00006153 \n",
            "[1][59/1565] Elapsed: 1m 1s (remain 25m 50s) Loss: 0.0749(0.1193) Grad: 147209.296875 LR: 0.00006230 \n",
            "[1][79/1565] Elapsed: 1m 22s (remain 25m 26s) Loss: 0.2401(0.1169) Grad: 169666.5625 LR: 0.00006307 \n",
            "[1][99/1565] Elapsed: 1m 42s (remain 25m 4s) Loss: 0.1901(0.1178) Grad: 354363.4375 LR: 0.00006383 \n",
            "[1][119/1565] Elapsed: 2m 3s (remain 24m 43s) Loss: 0.1935(0.1147) Grad: 418134.875 LR: 0.00006460 \n",
            "[1][139/1565] Elapsed: 2m 23s (remain 24m 22s) Loss: 0.1543(0.1148) Grad: 150467.625 LR: 0.00006537 \n",
            "[1][159/1565] Elapsed: 2m 44s (remain 24m 1s) Loss: 0.0521(0.1148) Grad: 124627.578125 LR: 0.00006613 \n",
            "[1][179/1565] Elapsed: 3m 4s (remain 23m 40s) Loss: 0.0815(0.1141) Grad: 204087.84375 LR: 0.00006690 \n",
            "[1][199/1565] Elapsed: 3m 25s (remain 23m 19s) Loss: 0.0338(0.1121) Grad: 219677.78125 LR: 0.00006767 \n",
            "[1][219/1565] Elapsed: 3m 45s (remain 22m 58s) Loss: 0.1741(0.1127) Grad: 495261.71875 LR: 0.00006843 \n",
            "[1][239/1565] Elapsed: 4m 5s (remain 22m 37s) Loss: 0.1297(0.1139) Grad: 219349.859375 LR: 0.00006920 \n",
            "[1][259/1565] Elapsed: 4m 26s (remain 22m 17s) Loss: 0.0819(0.1137) Grad: 146010.828125 LR: 0.00006997 \n",
            "[1][279/1565] Elapsed: 4m 46s (remain 21m 56s) Loss: 0.0968(0.1129) Grad: 271753.75 LR: 0.00007073 \n",
            "[1][299/1565] Elapsed: 5m 7s (remain 21m 35s) Loss: 0.0866(0.1129) Grad: 182170.640625 LR: 0.00007150 \n",
            "[1][319/1565] Elapsed: 5m 27s (remain 21m 15s) Loss: 0.0657(0.1129) Grad: 223471.296875 LR: 0.00007227 \n",
            "[1][339/1565] Elapsed: 5m 48s (remain 20m 54s) Loss: 0.0586(0.1130) Grad: 115971.6796875 LR: 0.00007304 \n",
            "[1][359/1565] Elapsed: 6m 8s (remain 20m 33s) Loss: 0.0581(0.1133) Grad: 178120.21875 LR: 0.00007380 \n",
            "[1][379/1565] Elapsed: 6m 29s (remain 20m 13s) Loss: 0.0683(0.1141) Grad: 171817.578125 LR: 0.00007457 \n",
            "[1][399/1565] Elapsed: 6m 49s (remain 19m 52s) Loss: 0.0641(0.1153) Grad: 305826.625 LR: 0.00007534 \n",
            "[1][419/1565] Elapsed: 7m 10s (remain 19m 32s) Loss: 0.0587(0.1152) Grad: 174429.1875 LR: 0.00007610 \n",
            "[1][439/1565] Elapsed: 7m 30s (remain 19m 12s) Loss: 0.1187(0.1152) Grad: 314366.0625 LR: 0.00007687 \n",
            "[1][459/1565] Elapsed: 7m 51s (remain 18m 51s) Loss: 0.1414(0.1147) Grad: 249194.421875 LR: 0.00007764 \n",
            "[1][479/1565] Elapsed: 8m 11s (remain 18m 31s) Loss: 0.0529(0.1154) Grad: 86106.3359375 LR: 0.00007840 \n",
            "[1][499/1565] Elapsed: 8m 31s (remain 18m 10s) Loss: 0.1059(0.1150) Grad: 281766.4375 LR: 0.00007917 \n",
            "[1][519/1565] Elapsed: 8m 52s (remain 17m 50s) Loss: 0.0973(0.1150) Grad: 150585.34375 LR: 0.00007994 \n",
            "[1][539/1565] Elapsed: 9m 12s (remain 17m 29s) Loss: 0.0683(0.1144) Grad: 202036.203125 LR: 0.00008070 \n",
            "[1][559/1565] Elapsed: 9m 33s (remain 17m 9s) Loss: 0.0775(0.1152) Grad: 139374.390625 LR: 0.00008147 \n",
            "[1][579/1565] Elapsed: 9m 53s (remain 16m 48s) Loss: 0.0447(0.1153) Grad: 226811.046875 LR: 0.00008224 \n",
            "[1][599/1565] Elapsed: 10m 14s (remain 16m 28s) Loss: 0.1894(0.1154) Grad: 366299.875 LR: 0.00008300 \n",
            "[1][619/1565] Elapsed: 10m 34s (remain 16m 7s) Loss: 0.2134(0.1157) Grad: 702411.5625 LR: 0.00008377 \n",
            "[1][639/1565] Elapsed: 10m 55s (remain 15m 47s) Loss: 0.0818(0.1154) Grad: 270146.375 LR: 0.00008454 \n",
            "[1][659/1565] Elapsed: 11m 15s (remain 15m 26s) Loss: 0.1505(0.1159) Grad: 518593.71875 LR: 0.00008530 \n",
            "[1][679/1565] Elapsed: 11m 36s (remain 15m 6s) Loss: 0.1047(0.1154) Grad: 190695.8125 LR: 0.00008607 \n",
            "[1][699/1565] Elapsed: 11m 56s (remain 14m 45s) Loss: 0.0809(0.1152) Grad: 207671.46875 LR: 0.00008684 \n",
            "[1][719/1565] Elapsed: 12m 17s (remain 14m 25s) Loss: 0.0620(0.1150) Grad: 193683.0 LR: 0.00008760 \n",
            "[1][739/1565] Elapsed: 12m 37s (remain 14m 4s) Loss: 0.1002(0.1147) Grad: 285934.96875 LR: 0.00008837 \n",
            "[1][759/1565] Elapsed: 12m 58s (remain 13m 44s) Loss: 0.1246(0.1148) Grad: 178733.4375 LR: 0.00008914 \n",
            "[1][779/1565] Elapsed: 13m 18s (remain 13m 23s) Loss: 0.1567(0.1149) Grad: 361121.71875 LR: 0.00008990 \n",
            "[1][799/1565] Elapsed: 13m 38s (remain 13m 3s) Loss: 0.1438(0.1152) Grad: 307616.09375 LR: 0.00009067 \n",
            "[1][819/1565] Elapsed: 13m 59s (remain 12m 42s) Loss: 0.1982(0.1156) Grad: 504362.78125 LR: 0.00009144 \n",
            "[1][839/1565] Elapsed: 14m 19s (remain 12m 22s) Loss: 0.0414(0.1156) Grad: 159196.53125 LR: 0.00009220 \n",
            "[1][859/1565] Elapsed: 14m 40s (remain 12m 1s) Loss: 0.1383(0.1155) Grad: 307838.53125 LR: 0.00009297 \n",
            "[1][879/1565] Elapsed: 15m 0s (remain 11m 41s) Loss: 0.0658(0.1153) Grad: 231719.09375 LR: 0.00009374 \n",
            "[1][899/1565] Elapsed: 15m 21s (remain 11m 20s) Loss: 0.2860(0.1155) Grad: 645863.1875 LR: 0.00009450 \n",
            "[1][919/1565] Elapsed: 15m 41s (remain 11m 0s) Loss: 0.0656(0.1156) Grad: 179054.9375 LR: 0.00009527 \n",
            "[1][939/1565] Elapsed: 16m 2s (remain 10m 39s) Loss: 0.0488(0.1154) Grad: 109442.703125 LR: 0.00009604 \n",
            "[1][959/1565] Elapsed: 16m 22s (remain 10m 19s) Loss: 0.0456(0.1148) Grad: 141148.03125 LR: 0.00009681 \n",
            "[1][979/1565] Elapsed: 16m 43s (remain 9m 58s) Loss: 0.1091(0.1144) Grad: 259863.25 LR: 0.00009757 \n",
            "[1][999/1565] Elapsed: 17m 3s (remain 9m 38s) Loss: 0.1220(0.1145) Grad: 343675.875 LR: 0.00009834 \n",
            "[1][1019/1565] Elapsed: 17m 23s (remain 9m 17s) Loss: 0.1440(0.1142) Grad: 290464.40625 LR: 0.00009911 \n",
            "[1][1039/1565] Elapsed: 17m 44s (remain 8m 57s) Loss: 0.1069(0.1141) Grad: 371667.3125 LR: 0.00009987 \n",
            "[1][1059/1565] Elapsed: 18m 4s (remain 8m 36s) Loss: 0.0662(0.1144) Grad: 137823.65625 LR: 0.00010064 \n",
            "[1][1079/1565] Elapsed: 18m 25s (remain 8m 16s) Loss: 0.0190(0.1146) Grad: 109448.5859375 LR: 0.00010141 \n",
            "[1][1099/1565] Elapsed: 18m 45s (remain 7m 55s) Loss: 0.0766(0.1143) Grad: 106428.546875 LR: 0.00010217 \n",
            "[1][1119/1565] Elapsed: 19m 6s (remain 7m 35s) Loss: 0.1158(0.1142) Grad: 326390.46875 LR: 0.00010294 \n",
            "[1][1139/1565] Elapsed: 19m 26s (remain 7m 14s) Loss: 0.1388(0.1143) Grad: 428182.5 LR: 0.00010371 \n",
            "[1][1159/1565] Elapsed: 19m 47s (remain 6m 54s) Loss: 0.0797(0.1150) Grad: 142841.125 LR: 0.00010447 \n",
            "[1][1179/1565] Elapsed: 20m 7s (remain 6m 33s) Loss: 0.0645(0.1155) Grad: 268042.09375 LR: 0.00010524 \n",
            "[1][1199/1565] Elapsed: 20m 27s (remain 6m 13s) Loss: 0.1813(0.1160) Grad: 213066.40625 LR: 0.00010601 \n",
            "[1][1219/1565] Elapsed: 20m 48s (remain 5m 53s) Loss: 0.0733(0.1160) Grad: 126467.40625 LR: 0.00010677 \n",
            "[1][1239/1565] Elapsed: 21m 8s (remain 5m 32s) Loss: 0.0602(0.1155) Grad: 346723.875 LR: 0.00010754 \n",
            "[1][1259/1565] Elapsed: 21m 29s (remain 5m 12s) Loss: 0.0677(0.1153) Grad: 131310.765625 LR: 0.00010831 \n",
            "[1][1279/1565] Elapsed: 21m 49s (remain 4m 51s) Loss: 0.0390(0.1158) Grad: 161685.171875 LR: 0.00010907 \n",
            "[1][1299/1565] Elapsed: 22m 10s (remain 4m 31s) Loss: 0.2791(0.1160) Grad: 667984.8125 LR: 0.00010984 \n",
            "[1][1319/1565] Elapsed: 22m 30s (remain 4m 10s) Loss: 0.1157(0.1161) Grad: 422437.8125 LR: 0.00011061 \n",
            "[1][1339/1565] Elapsed: 22m 51s (remain 3m 50s) Loss: 0.0725(0.1159) Grad: 156422.1875 LR: 0.00011137 \n",
            "[1][1359/1565] Elapsed: 23m 11s (remain 3m 29s) Loss: 0.1067(0.1157) Grad: 386636.28125 LR: 0.00011214 \n",
            "[1][1379/1565] Elapsed: 23m 31s (remain 3m 9s) Loss: 0.1165(0.1157) Grad: 406209.28125 LR: 0.00011291 \n",
            "[1][1399/1565] Elapsed: 23m 52s (remain 2m 48s) Loss: 0.1813(0.1157) Grad: 331076.65625 LR: 0.00011367 \n",
            "[1][1419/1565] Elapsed: 24m 12s (remain 2m 28s) Loss: 0.1131(0.1157) Grad: 221309.03125 LR: 0.00011444 \n",
            "[1][1439/1565] Elapsed: 24m 33s (remain 2m 7s) Loss: 0.0504(0.1157) Grad: 190174.171875 LR: 0.00011521 \n",
            "[1][1459/1565] Elapsed: 24m 53s (remain 1m 47s) Loss: 0.1286(0.1158) Grad: 453677.0625 LR: 0.00011597 \n",
            "[1][1479/1565] Elapsed: 25m 14s (remain 1m 26s) Loss: 0.1242(0.1157) Grad: 329854.90625 LR: 0.00011674 \n",
            "[1][1499/1565] Elapsed: 25m 34s (remain 1m 6s) Loss: 0.0794(0.1159) Grad: 172269.65625 LR: 0.00011751 \n",
            "[1][1519/1565] Elapsed: 25m 55s (remain 0m 46s) Loss: 0.0997(0.1158) Grad: 255472.96875 LR: 0.00011827 \n",
            "[1][1539/1565] Elapsed: 26m 15s (remain 0m 25s) Loss: 0.0704(0.1157) Grad: 189333.53125 LR: 0.00011904 \n",
            "[1][1559/1565] Elapsed: 26m 35s (remain 0m 5s) Loss: 0.0796(0.1158) Grad: 137957.625 LR: 0.00011981 \n",
            "[1][19/390] Elapsed: 0m 13s (remain 4m 1s) Loss: 0.1685(0.1059) \n",
            "[1][39/390] Elapsed: 0m 25s (remain 3m 47s) Loss: 0.0656(0.0946) \n",
            "[1][59/390] Elapsed: 0m 38s (remain 3m 33s) Loss: 0.1529(0.1033) \n",
            "[1][79/390] Elapsed: 0m 51s (remain 3m 20s) Loss: 0.1371(0.1010) \n",
            "[1][99/390] Elapsed: 1m 4s (remain 3m 7s) Loss: 0.1969(0.1059) \n",
            "[1][119/390] Elapsed: 1m 17s (remain 2m 54s) Loss: 0.1583(0.1073) \n",
            "[1][139/390] Elapsed: 1m 30s (remain 2m 41s) Loss: 0.0435(0.1051) \n",
            "[1][159/390] Elapsed: 1m 43s (remain 2m 28s) Loss: 0.0932(0.1047) \n",
            "[1][179/390] Elapsed: 1m 56s (remain 2m 15s) Loss: 0.0223(0.1036) \n",
            "[1][199/390] Elapsed: 2m 8s (remain 2m 2s) Loss: 0.1120(0.1023) \n",
            "[1][219/390] Elapsed: 2m 21s (remain 1m 49s) Loss: 0.1009(0.1035) \n",
            "[1][239/390] Elapsed: 2m 34s (remain 1m 36s) Loss: 0.1053(0.1034) \n",
            "[1][259/390] Elapsed: 2m 47s (remain 1m 23s) Loss: 0.0774(0.1040) \n",
            "[1][279/390] Elapsed: 3m 0s (remain 1m 10s) Loss: 0.0812(0.1047) \n",
            "[1][299/390] Elapsed: 3m 13s (remain 0m 58s) Loss: 0.0418(0.1054) \n",
            "[1][319/390] Elapsed: 3m 26s (remain 0m 45s) Loss: 0.0756(0.1048) \n",
            "[1][339/390] Elapsed: 3m 39s (remain 0m 32s) Loss: 0.1315(0.1057) \n",
            "[1][359/390] Elapsed: 3m 52s (remain 0m 19s) Loss: 0.1167(0.1073) \n",
            "[1][379/390] Elapsed: 4m 4s (remain 0m 6s) Loss: 0.2449(0.1074) \n",
            "[1][390/390] Elapsed: 4m 11s (remain -1m 59s) Loss: 0.0454(0.1071) \n",
            "Epoch: 2 - avg_train_loss = 0.1158 avg_val_loss: 0.1071 time: 1853.1260s\n",
            "Epoch: 2 - Score: 0.4637, Scores: [0.4813375754678385, 0.4630641692222467, 0.4249817198944702, 0.45903981325681736, 0.5000930567309317, 0.4537513461508106]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/1565] Elapsed: 0m 20s (remain 26m 41s) Loss: 0.0647(0.0769) Grad: 132411.796875 LR: 0.00012077 \n",
            "[2][39/1565] Elapsed: 0m 41s (remain 26m 16s) Loss: 0.1076(0.0963) Grad: 185489.09375 LR: 0.00012153 \n",
            "[2][59/1565] Elapsed: 1m 1s (remain 25m 50s) Loss: 0.0946(0.1057) Grad: 287660.25 LR: 0.00012230 \n",
            "[2][79/1565] Elapsed: 1m 22s (remain 25m 26s) Loss: 0.2240(0.1096) Grad: 670586.5 LR: 0.00012307 \n",
            "[2][99/1565] Elapsed: 1m 42s (remain 25m 3s) Loss: 0.0957(0.1117) Grad: 182390.015625 LR: 0.00012383 \n",
            "[2][119/1565] Elapsed: 2m 3s (remain 24m 42s) Loss: 0.0594(0.1094) Grad: 169186.71875 LR: 0.00012460 \n",
            "[2][139/1565] Elapsed: 2m 23s (remain 24m 21s) Loss: 0.0797(0.1097) Grad: 180845.984375 LR: 0.00012537 \n",
            "[2][159/1565] Elapsed: 2m 44s (remain 24m 0s) Loss: 0.0723(0.1094) Grad: 249915.421875 LR: 0.00012613 \n",
            "[2][179/1565] Elapsed: 3m 4s (remain 23m 39s) Loss: 0.0386(0.1086) Grad: 123311.9921875 LR: 0.00012690 \n",
            "[2][199/1565] Elapsed: 3m 24s (remain 23m 18s) Loss: 0.0393(0.1088) Grad: 189097.203125 LR: 0.00012767 \n",
            "[2][219/1565] Elapsed: 3m 45s (remain 22m 57s) Loss: 0.1183(0.1084) Grad: 238252.3125 LR: 0.00012843 \n",
            "[2][239/1565] Elapsed: 4m 5s (remain 22m 36s) Loss: 0.1686(0.1100) Grad: 310262.1875 LR: 0.00012920 \n",
            "[2][259/1565] Elapsed: 4m 26s (remain 22m 17s) Loss: 0.2363(0.1103) Grad: 504272.34375 LR: 0.00012997 \n",
            "[2][279/1565] Elapsed: 4m 46s (remain 21m 56s) Loss: 0.3483(0.1106) Grad: 321847.9375 LR: 0.00013073 \n",
            "[2][299/1565] Elapsed: 5m 7s (remain 21m 35s) Loss: 0.1074(0.1103) Grad: 216155.40625 LR: 0.00013150 \n",
            "[2][319/1565] Elapsed: 5m 27s (remain 21m 15s) Loss: 0.0929(0.1111) Grad: 166326.921875 LR: 0.00013227 \n",
            "[2][339/1565] Elapsed: 5m 48s (remain 20m 54s) Loss: 0.1372(0.1110) Grad: 296489.53125 LR: 0.00013304 \n",
            "[2][359/1565] Elapsed: 6m 8s (remain 20m 34s) Loss: 0.1008(0.1126) Grad: 362599.625 LR: 0.00013380 \n",
            "[2][379/1565] Elapsed: 6m 29s (remain 20m 13s) Loss: 0.0799(0.1118) Grad: 171740.75 LR: 0.00013457 \n",
            "[2][399/1565] Elapsed: 6m 49s (remain 19m 52s) Loss: 0.0365(0.1111) Grad: 107336.328125 LR: 0.00013534 \n",
            "[2][419/1565] Elapsed: 7m 10s (remain 19m 32s) Loss: 0.0931(0.1127) Grad: 168484.25 LR: 0.00013610 \n",
            "[2][439/1565] Elapsed: 7m 30s (remain 19m 11s) Loss: 0.1291(0.1134) Grad: 416296.53125 LR: 0.00013687 \n",
            "[2][459/1565] Elapsed: 7m 50s (remain 18m 51s) Loss: 0.0736(0.1128) Grad: 193818.234375 LR: 0.00013764 \n",
            "[2][479/1565] Elapsed: 8m 11s (remain 18m 30s) Loss: 0.1633(0.1125) Grad: 173985.1875 LR: 0.00013840 \n",
            "[2][499/1565] Elapsed: 8m 31s (remain 18m 10s) Loss: 0.0884(0.1121) Grad: 188133.984375 LR: 0.00013917 \n",
            "[2][519/1565] Elapsed: 8m 52s (remain 17m 49s) Loss: 0.1130(0.1122) Grad: 164852.09375 LR: 0.00013994 \n",
            "[2][539/1565] Elapsed: 9m 12s (remain 17m 29s) Loss: 0.1584(0.1119) Grad: 316790.3125 LR: 0.00014070 \n",
            "[2][559/1565] Elapsed: 9m 33s (remain 17m 8s) Loss: 0.1757(0.1121) Grad: 594589.5 LR: 0.00014147 \n",
            "[2][579/1565] Elapsed: 9m 53s (remain 16m 48s) Loss: 0.0993(0.1117) Grad: 177537.609375 LR: 0.00014224 \n",
            "[2][599/1565] Elapsed: 10m 14s (remain 16m 27s) Loss: 0.0718(0.1116) Grad: 198606.59375 LR: 0.00014300 \n",
            "[2][619/1565] Elapsed: 10m 34s (remain 16m 7s) Loss: 0.0932(0.1125) Grad: 197305.859375 LR: 0.00014377 \n",
            "[2][639/1565] Elapsed: 10m 54s (remain 15m 46s) Loss: 0.0644(0.1128) Grad: 185087.109375 LR: 0.00014454 \n",
            "[2][659/1565] Elapsed: 11m 15s (remain 15m 26s) Loss: 0.0464(0.1129) Grad: 160357.765625 LR: 0.00014530 \n",
            "[2][679/1565] Elapsed: 11m 35s (remain 15m 5s) Loss: 0.0599(0.1124) Grad: 119021.6015625 LR: 0.00014607 \n",
            "[2][699/1565] Elapsed: 11m 56s (remain 14m 45s) Loss: 0.0904(0.1126) Grad: 316050.4375 LR: 0.00014684 \n",
            "[2][719/1565] Elapsed: 12m 16s (remain 14m 24s) Loss: 0.0971(0.1131) Grad: 355737.59375 LR: 0.00014760 \n",
            "[2][739/1565] Elapsed: 12m 37s (remain 14m 4s) Loss: 0.0759(0.1131) Grad: 169916.515625 LR: 0.00014837 \n",
            "[2][759/1565] Elapsed: 12m 57s (remain 13m 43s) Loss: 0.1112(0.1129) Grad: 151356.8125 LR: 0.00014914 \n",
            "[2][779/1565] Elapsed: 13m 18s (remain 13m 23s) Loss: 0.0480(0.1132) Grad: 167788.453125 LR: 0.00014990 \n",
            "[2][799/1565] Elapsed: 13m 38s (remain 13m 2s) Loss: 0.0949(0.1133) Grad: 239830.125 LR: 0.00015067 \n",
            "[2][819/1565] Elapsed: 13m 59s (remain 12m 42s) Loss: 0.1431(0.1132) Grad: 330982.625 LR: 0.00015144 \n",
            "[2][839/1565] Elapsed: 14m 19s (remain 12m 21s) Loss: 0.1512(0.1130) Grad: 169376.546875 LR: 0.00015220 \n",
            "[2][859/1565] Elapsed: 14m 39s (remain 12m 1s) Loss: 0.1646(0.1127) Grad: 577428.3125 LR: 0.00015297 \n",
            "[2][879/1565] Elapsed: 15m 0s (remain 11m 40s) Loss: 0.0349(0.1123) Grad: 171205.203125 LR: 0.00015374 \n",
            "[2][899/1565] Elapsed: 15m 20s (remain 11m 20s) Loss: 0.1112(0.1120) Grad: 348903.34375 LR: 0.00015450 \n",
            "[2][919/1565] Elapsed: 15m 41s (remain 10m 59s) Loss: 0.0451(0.1119) Grad: 103852.2421875 LR: 0.00015527 \n",
            "[2][939/1565] Elapsed: 16m 1s (remain 10m 39s) Loss: 0.1523(0.1112) Grad: 417348.28125 LR: 0.00015604 \n",
            "[2][959/1565] Elapsed: 16m 22s (remain 10m 19s) Loss: 0.0968(0.1117) Grad: 258177.09375 LR: 0.00015681 \n",
            "[2][979/1565] Elapsed: 16m 42s (remain 9m 58s) Loss: 0.0498(0.1121) Grad: 214270.84375 LR: 0.00015757 \n",
            "[2][999/1565] Elapsed: 17m 3s (remain 9m 38s) Loss: 0.0771(0.1120) Grad: 132122.65625 LR: 0.00015834 \n",
            "[2][1019/1565] Elapsed: 17m 23s (remain 9m 17s) Loss: 0.0644(0.1118) Grad: 221685.453125 LR: 0.00015911 \n",
            "[2][1039/1565] Elapsed: 17m 44s (remain 8m 57s) Loss: 0.0358(0.1116) Grad: 138856.21875 LR: 0.00015987 \n",
            "[2][1059/1565] Elapsed: 18m 4s (remain 8m 36s) Loss: 0.0580(0.1118) Grad: 151543.3125 LR: 0.00016064 \n",
            "[2][1079/1565] Elapsed: 18m 24s (remain 8m 16s) Loss: 0.1261(0.1118) Grad: 199771.140625 LR: 0.00016141 \n",
            "[2][1099/1565] Elapsed: 18m 45s (remain 7m 55s) Loss: 0.0882(0.1123) Grad: 147179.453125 LR: 0.00016217 \n",
            "[2][1119/1565] Elapsed: 19m 5s (remain 7m 35s) Loss: 0.1031(0.1124) Grad: 185509.78125 LR: 0.00016294 \n",
            "[2][1139/1565] Elapsed: 19m 26s (remain 7m 14s) Loss: 0.0552(0.1121) Grad: 124573.421875 LR: 0.00016371 \n",
            "[2][1159/1565] Elapsed: 19m 46s (remain 6m 54s) Loss: 0.0948(0.1120) Grad: 342176.09375 LR: 0.00016447 \n",
            "[2][1179/1565] Elapsed: 20m 7s (remain 6m 33s) Loss: 0.1000(0.1121) Grad: 341622.5 LR: 0.00016524 \n",
            "[2][1199/1565] Elapsed: 20m 27s (remain 6m 13s) Loss: 0.0971(0.1119) Grad: 163948.84375 LR: 0.00016601 \n",
            "[2][1219/1565] Elapsed: 20m 48s (remain 5m 52s) Loss: 0.1325(0.1121) Grad: 384427.875 LR: 0.00016677 \n",
            "[2][1239/1565] Elapsed: 21m 8s (remain 5m 32s) Loss: 0.0625(0.1121) Grad: 106408.65625 LR: 0.00016754 \n",
            "[2][1259/1565] Elapsed: 21m 28s (remain 5m 12s) Loss: 0.2666(0.1123) Grad: 350817.84375 LR: 0.00016831 \n",
            "[2][1279/1565] Elapsed: 21m 49s (remain 4m 51s) Loss: 0.0490(0.1128) Grad: 158995.609375 LR: 0.00016907 \n",
            "[2][1299/1565] Elapsed: 22m 9s (remain 4m 31s) Loss: 0.1064(0.1129) Grad: 171152.734375 LR: 0.00016984 \n",
            "[2][1319/1565] Elapsed: 22m 30s (remain 4m 10s) Loss: 0.2689(0.1128) Grad: 527283.625 LR: 0.00017061 \n",
            "[2][1339/1565] Elapsed: 22m 50s (remain 3m 50s) Loss: 0.0739(0.1123) Grad: 279419.65625 LR: 0.00017137 \n",
            "[2][1359/1565] Elapsed: 23m 11s (remain 3m 29s) Loss: 0.1849(0.1125) Grad: 311828.4375 LR: 0.00017214 \n",
            "[2][1379/1565] Elapsed: 23m 31s (remain 3m 9s) Loss: 0.2172(0.1123) Grad: 318979.1875 LR: 0.00017291 \n",
            "[2][1399/1565] Elapsed: 23m 52s (remain 2m 48s) Loss: 0.1437(0.1125) Grad: 362775.21875 LR: 0.00017367 \n",
            "[2][1419/1565] Elapsed: 24m 12s (remain 2m 28s) Loss: 0.1030(0.1128) Grad: 166820.921875 LR: 0.00017444 \n",
            "[2][1439/1565] Elapsed: 24m 33s (remain 2m 7s) Loss: 0.1559(0.1130) Grad: 370914.9375 LR: 0.00017521 \n",
            "[2][1459/1565] Elapsed: 24m 53s (remain 1m 47s) Loss: 0.1917(0.1130) Grad: 576838.5 LR: 0.00017597 \n",
            "[2][1479/1565] Elapsed: 25m 14s (remain 1m 26s) Loss: 0.0761(0.1128) Grad: 247030.03125 LR: 0.00017674 \n",
            "[2][1499/1565] Elapsed: 25m 34s (remain 1m 6s) Loss: 0.1191(0.1127) Grad: 241421.71875 LR: 0.00017751 \n",
            "[2][1519/1565] Elapsed: 25m 54s (remain 0m 46s) Loss: 0.0422(0.1127) Grad: 138552.28125 LR: 0.00017827 \n",
            "[2][1539/1565] Elapsed: 26m 15s (remain 0m 25s) Loss: 0.0658(0.1127) Grad: 135170.328125 LR: 0.00017904 \n",
            "[2][1559/1565] Elapsed: 26m 35s (remain 0m 5s) Loss: 0.1511(0.1127) Grad: 377913.59375 LR: 0.00017981 \n",
            "[2][19/390] Elapsed: 0m 13s (remain 4m 3s) Loss: 0.1565(0.1201) \n",
            "[2][39/390] Elapsed: 0m 26s (remain 3m 48s) Loss: 0.0697(0.1048) \n",
            "[2][59/390] Elapsed: 0m 39s (remain 3m 34s) Loss: 0.1225(0.1130) \n",
            "[2][79/390] Elapsed: 0m 51s (remain 3m 21s) Loss: 0.1255(0.1099) \n",
            "[2][99/390] Elapsed: 1m 4s (remain 3m 8s) Loss: 0.1467(0.1138) \n",
            "[2][119/390] Elapsed: 1m 17s (remain 2m 55s) Loss: 0.1900(0.1149) \n",
            "[2][139/390] Elapsed: 1m 30s (remain 2m 41s) Loss: 0.0393(0.1135) \n",
            "[2][159/390] Elapsed: 1m 43s (remain 2m 28s) Loss: 0.0907(0.1126) \n",
            "[2][179/390] Elapsed: 1m 56s (remain 2m 16s) Loss: 0.0357(0.1115) \n",
            "[2][199/390] Elapsed: 2m 9s (remain 2m 3s) Loss: 0.0804(0.1096) \n",
            "[2][219/390] Elapsed: 2m 22s (remain 1m 50s) Loss: 0.1289(0.1111) \n",
            "[2][239/390] Elapsed: 2m 35s (remain 1m 37s) Loss: 0.1113(0.1109) \n",
            "[2][259/390] Elapsed: 2m 48s (remain 1m 24s) Loss: 0.1020(0.1117) \n",
            "[2][279/390] Elapsed: 3m 1s (remain 1m 11s) Loss: 0.0750(0.1123) \n",
            "[2][299/390] Elapsed: 3m 14s (remain 0m 58s) Loss: 0.0536(0.1132) \n",
            "[2][319/390] Elapsed: 3m 26s (remain 0m 45s) Loss: 0.0551(0.1121) \n",
            "[2][339/390] Elapsed: 3m 39s (remain 0m 32s) Loss: 0.1864(0.1135) \n",
            "[2][359/390] Elapsed: 3m 52s (remain 0m 19s) Loss: 0.1001(0.1152) \n",
            "[2][379/390] Elapsed: 4m 5s (remain 0m 6s) Loss: 0.2610(0.1152) \n",
            "[2][390/390] Elapsed: 4m 12s (remain -1m 59s) Loss: 0.0502(0.1149) \n",
            "Epoch: 3 - avg_train_loss = 0.1127 avg_val_loss: 0.1149 time: 1853.7356s\n",
            "Epoch: 3 - Score: 0.4808, Scores: [0.47791317289394597, 0.4870436960931031, 0.4580148849508136, 0.4599041303477294, 0.48661507729509157, 0.5150113809102793]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/1565] Elapsed: 0m 20s (remain 26m 30s) Loss: 0.0971(0.1152) Grad: 253376.15625 LR: 0.00018077 \n",
            "[3][39/1565] Elapsed: 0m 41s (remain 26m 5s) Loss: 0.2359(0.1188) Grad: 587455.6875 LR: 0.00018153 \n",
            "[3][59/1565] Elapsed: 1m 1s (remain 25m 42s) Loss: 0.0727(0.1060) Grad: 159275.59375 LR: 0.00018230 \n",
            "[3][79/1565] Elapsed: 1m 22s (remain 25m 22s) Loss: 0.0776(0.1073) Grad: 225189.984375 LR: 0.00018307 \n",
            "[3][99/1565] Elapsed: 1m 42s (remain 25m 1s) Loss: 0.1114(0.1057) Grad: 380754.3125 LR: 0.00018383 \n",
            "[3][119/1565] Elapsed: 2m 3s (remain 24m 42s) Loss: 0.1409(0.1054) Grad: 292008.34375 LR: 0.00018460 \n",
            "[3][139/1565] Elapsed: 2m 23s (remain 24m 21s) Loss: 0.0873(0.1057) Grad: 187182.359375 LR: 0.00018537 \n",
            "[3][159/1565] Elapsed: 2m 44s (remain 24m 0s) Loss: 0.1578(0.1061) Grad: 246976.03125 LR: 0.00018613 \n",
            "[3][179/1565] Elapsed: 3m 4s (remain 23m 40s) Loss: 0.0098(0.1067) Grad: 59560.61328125 LR: 0.00018690 \n",
            "[3][199/1565] Elapsed: 3m 25s (remain 23m 19s) Loss: 0.1819(0.1085) Grad: 319569.78125 LR: 0.00018767 \n",
            "[3][219/1565] Elapsed: 3m 45s (remain 22m 58s) Loss: 0.0948(0.1119) Grad: 206098.609375 LR: 0.00018843 \n",
            "[3][239/1565] Elapsed: 4m 6s (remain 22m 38s) Loss: 0.0777(0.1138) Grad: 223554.703125 LR: 0.00018920 \n",
            "[3][259/1565] Elapsed: 4m 26s (remain 22m 17s) Loss: 0.1252(0.1136) Grad: 154970.234375 LR: 0.00018997 \n",
            "[3][279/1565] Elapsed: 4m 46s (remain 21m 57s) Loss: 0.0852(0.1122) Grad: 167234.0625 LR: 0.00019073 \n",
            "[3][299/1565] Elapsed: 5m 7s (remain 21m 36s) Loss: 0.1389(0.1128) Grad: 327974.9375 LR: 0.00019150 \n",
            "[3][319/1565] Elapsed: 5m 27s (remain 21m 16s) Loss: 0.1146(0.1120) Grad: 166583.171875 LR: 0.00019227 \n",
            "[3][339/1565] Elapsed: 5m 48s (remain 20m 55s) Loss: 0.1891(0.1128) Grad: 202053.34375 LR: 0.00019304 \n",
            "[3][359/1565] Elapsed: 6m 8s (remain 20m 34s) Loss: 0.1070(0.1140) Grad: 176392.84375 LR: 0.00019380 \n",
            "[3][379/1565] Elapsed: 6m 29s (remain 20m 14s) Loss: 0.1182(0.1141) Grad: 244188.359375 LR: 0.00019457 \n",
            "[3][399/1565] Elapsed: 6m 49s (remain 19m 53s) Loss: 0.0394(0.1139) Grad: 150614.796875 LR: 0.00019534 \n",
            "[3][419/1565] Elapsed: 7m 10s (remain 19m 33s) Loss: 0.1524(0.1139) Grad: 364850.8125 LR: 0.00019610 \n",
            "[3][439/1565] Elapsed: 7m 30s (remain 19m 12s) Loss: 0.0418(0.1137) Grad: 101925.0859375 LR: 0.00019687 \n",
            "[3][459/1565] Elapsed: 7m 51s (remain 18m 51s) Loss: 0.1629(0.1138) Grad: 454258.84375 LR: 0.00019764 \n",
            "[3][479/1565] Elapsed: 8m 11s (remain 18m 31s) Loss: 0.0419(0.1144) Grad: 114633.3515625 LR: 0.00019840 \n",
            "[3][499/1565] Elapsed: 8m 32s (remain 18m 10s) Loss: 0.3118(0.1149) Grad: 326383.59375 LR: 0.00019917 \n",
            "[3][519/1565] Elapsed: 8m 52s (remain 17m 50s) Loss: 0.0601(0.1144) Grad: 109160.9921875 LR: 0.00019994 \n",
            "[3][539/1565] Elapsed: 9m 13s (remain 17m 29s) Loss: 0.0907(0.1141) Grad: 197641.65625 LR: 0.00020070 \n",
            "[3][559/1565] Elapsed: 9m 33s (remain 17m 9s) Loss: 0.1293(0.1137) Grad: 337086.0625 LR: 0.00020147 \n",
            "[3][579/1565] Elapsed: 9m 53s (remain 16m 48s) Loss: 0.1445(0.1136) Grad: 324075.6875 LR: 0.00020224 \n",
            "[3][599/1565] Elapsed: 10m 14s (remain 16m 28s) Loss: 0.1000(0.1138) Grad: 286627.15625 LR: 0.00020300 \n",
            "[3][619/1565] Elapsed: 10m 34s (remain 16m 7s) Loss: 0.1077(0.1135) Grad: 264577.875 LR: 0.00020377 \n",
            "[3][639/1565] Elapsed: 10m 55s (remain 15m 47s) Loss: 0.1864(0.1140) Grad: 413838.40625 LR: 0.00020454 \n",
            "[3][659/1565] Elapsed: 11m 15s (remain 15m 26s) Loss: 0.0313(0.1133) Grad: 69702.078125 LR: 0.00020530 \n",
            "[3][679/1565] Elapsed: 11m 36s (remain 15m 6s) Loss: 0.0439(0.1140) Grad: 198587.71875 LR: 0.00020607 \n",
            "[3][699/1565] Elapsed: 11m 56s (remain 14m 45s) Loss: 0.0557(0.1139) Grad: 114153.6640625 LR: 0.00020684 \n",
            "[3][719/1565] Elapsed: 12m 17s (remain 14m 25s) Loss: 0.1000(0.1135) Grad: 148829.6875 LR: 0.00020760 \n",
            "[3][739/1565] Elapsed: 12m 37s (remain 14m 4s) Loss: 0.1199(0.1140) Grad: 327734.5625 LR: 0.00020837 \n",
            "[3][759/1565] Elapsed: 12m 57s (remain 13m 44s) Loss: 0.0989(0.1144) Grad: 211217.359375 LR: 0.00020914 \n",
            "[3][779/1565] Elapsed: 13m 18s (remain 13m 23s) Loss: 0.0991(0.1137) Grad: 313898.03125 LR: 0.00020990 \n",
            "[3][799/1565] Elapsed: 13m 38s (remain 13m 3s) Loss: 0.2185(0.1137) Grad: 247938.375 LR: 0.00021067 \n",
            "[3][819/1565] Elapsed: 13m 59s (remain 12m 42s) Loss: 0.1476(0.1139) Grad: 595209.8125 LR: 0.00021144 \n",
            "[3][839/1565] Elapsed: 14m 19s (remain 12m 22s) Loss: 0.0949(0.1135) Grad: 273963.6875 LR: 0.00021220 \n",
            "[3][859/1565] Elapsed: 14m 40s (remain 12m 1s) Loss: 0.0989(0.1132) Grad: 231132.0625 LR: 0.00021297 \n",
            "[3][879/1565] Elapsed: 15m 0s (remain 11m 41s) Loss: 0.0614(0.1133) Grad: 166447.953125 LR: 0.00021374 \n",
            "[3][899/1565] Elapsed: 15m 21s (remain 11m 20s) Loss: 0.1056(0.1132) Grad: 220920.71875 LR: 0.00021450 \n",
            "[3][919/1565] Elapsed: 15m 41s (remain 11m 0s) Loss: 0.0490(0.1130) Grad: 127633.4921875 LR: 0.00021527 \n",
            "[3][939/1565] Elapsed: 16m 2s (remain 10m 39s) Loss: 0.2153(0.1129) Grad: 529360.5 LR: 0.00021604 \n",
            "[3][959/1565] Elapsed: 16m 22s (remain 10m 19s) Loss: 0.0998(0.1130) Grad: 157907.703125 LR: 0.00021681 \n",
            "[3][979/1565] Elapsed: 16m 42s (remain 9m 58s) Loss: 0.1486(0.1130) Grad: 254211.6875 LR: 0.00021757 \n",
            "[3][999/1565] Elapsed: 17m 3s (remain 9m 38s) Loss: 0.0847(0.1133) Grad: 178875.859375 LR: 0.00021834 \n",
            "[3][1019/1565] Elapsed: 17m 23s (remain 9m 17s) Loss: 0.0533(0.1135) Grad: 140115.421875 LR: 0.00021911 \n",
            "[3][1039/1565] Elapsed: 17m 44s (remain 8m 57s) Loss: 0.2971(0.1133) Grad: 379846.90625 LR: 0.00021987 \n",
            "[3][1059/1565] Elapsed: 18m 4s (remain 8m 36s) Loss: 0.1263(0.1135) Grad: 416787.09375 LR: 0.00022064 \n",
            "[3][1079/1565] Elapsed: 18m 25s (remain 8m 16s) Loss: 0.0704(0.1136) Grad: 210181.703125 LR: 0.00022141 \n",
            "[3][1099/1565] Elapsed: 18m 45s (remain 7m 55s) Loss: 0.0592(0.1134) Grad: 181581.234375 LR: 0.00022217 \n",
            "[3][1119/1565] Elapsed: 19m 6s (remain 7m 35s) Loss: 0.1603(0.1134) Grad: 469121.65625 LR: 0.00022294 \n",
            "[3][1139/1565] Elapsed: 19m 26s (remain 7m 14s) Loss: 0.0933(0.1137) Grad: 265610.65625 LR: 0.00022371 \n",
            "[3][1159/1565] Elapsed: 19m 47s (remain 6m 54s) Loss: 0.0282(0.1136) Grad: 66224.6796875 LR: 0.00022447 \n",
            "[3][1179/1565] Elapsed: 20m 7s (remain 6m 33s) Loss: 0.0945(0.1136) Grad: 424730.53125 LR: 0.00022524 \n",
            "[3][1199/1565] Elapsed: 20m 27s (remain 6m 13s) Loss: 0.0818(0.1139) Grad: 126543.9765625 LR: 0.00022601 \n",
            "[3][1219/1565] Elapsed: 20m 48s (remain 5m 53s) Loss: 0.1465(0.1139) Grad: 239771.265625 LR: 0.00022677 \n",
            "[3][1239/1565] Elapsed: 21m 8s (remain 5m 32s) Loss: 0.0774(0.1138) Grad: 131968.5 LR: 0.00022754 \n",
            "[3][1259/1565] Elapsed: 21m 29s (remain 5m 12s) Loss: 0.1882(0.1136) Grad: 551046.1875 LR: 0.00022831 \n",
            "[3][1279/1565] Elapsed: 21m 49s (remain 4m 51s) Loss: 0.2051(0.1138) Grad: 513849.375 LR: 0.00022907 \n",
            "[3][1299/1565] Elapsed: 22m 10s (remain 4m 31s) Loss: 0.0946(0.1141) Grad: 132841.765625 LR: 0.00022984 \n",
            "[3][1319/1565] Elapsed: 22m 30s (remain 4m 10s) Loss: 0.1068(0.1142) Grad: 156023.28125 LR: 0.00023061 \n",
            "[3][1339/1565] Elapsed: 22m 51s (remain 3m 50s) Loss: 0.1118(0.1141) Grad: 232050.21875 LR: 0.00023137 \n",
            "[3][1359/1565] Elapsed: 23m 11s (remain 3m 29s) Loss: 0.1176(0.1141) Grad: 257300.296875 LR: 0.00023214 \n",
            "[3][1379/1565] Elapsed: 23m 32s (remain 3m 9s) Loss: 0.1346(0.1145) Grad: 347312.59375 LR: 0.00023291 \n",
            "[3][1399/1565] Elapsed: 23m 52s (remain 2m 48s) Loss: 0.0506(0.1144) Grad: 224488.015625 LR: 0.00023367 \n",
            "[3][1419/1565] Elapsed: 24m 13s (remain 2m 28s) Loss: 0.1155(0.1145) Grad: 150470.90625 LR: 0.00023444 \n",
            "[3][1439/1565] Elapsed: 24m 33s (remain 2m 7s) Loss: 0.0329(0.1144) Grad: 157654.03125 LR: 0.00023521 \n",
            "[3][1459/1565] Elapsed: 24m 53s (remain 1m 47s) Loss: 0.0468(0.1145) Grad: 249386.671875 LR: 0.00023597 \n",
            "[3][1479/1565] Elapsed: 25m 14s (remain 1m 26s) Loss: 0.2042(0.1144) Grad: 304759.28125 LR: 0.00023674 \n",
            "[3][1499/1565] Elapsed: 25m 34s (remain 1m 6s) Loss: 0.1030(0.1143) Grad: 280387.5625 LR: 0.00023751 \n",
            "[3][1519/1565] Elapsed: 25m 55s (remain 0m 46s) Loss: 0.0912(0.1141) Grad: 319376.3125 LR: 0.00023827 \n",
            "[3][1539/1565] Elapsed: 26m 15s (remain 0m 25s) Loss: 0.0483(0.1143) Grad: 139251.484375 LR: 0.00023904 \n",
            "[3][1559/1565] Elapsed: 26m 36s (remain 0m 5s) Loss: 0.0634(0.1143) Grad: 123277.234375 LR: 0.00023981 \n",
            "[3][19/390] Elapsed: 0m 13s (remain 4m 2s) Loss: 0.1198(0.0947) \n",
            "[3][39/390] Elapsed: 0m 26s (remain 3m 47s) Loss: 0.0804(0.0981) \n",
            "[3][59/390] Elapsed: 0m 38s (remain 3m 34s) Loss: 0.2244(0.1060) \n",
            "[3][79/390] Elapsed: 0m 51s (remain 3m 20s) Loss: 0.2176(0.1057) \n",
            "[3][99/390] Elapsed: 1m 4s (remain 3m 7s) Loss: 0.2570(0.1104) \n",
            "[3][119/390] Elapsed: 1m 17s (remain 2m 54s) Loss: 0.1150(0.1126) \n",
            "[3][139/390] Elapsed: 1m 30s (remain 2m 41s) Loss: 0.0865(0.1090) \n",
            "[3][159/390] Elapsed: 1m 43s (remain 2m 28s) Loss: 0.1041(0.1084) \n",
            "[3][179/390] Elapsed: 1m 56s (remain 2m 15s) Loss: 0.0476(0.1064) \n",
            "[3][199/390] Elapsed: 2m 9s (remain 2m 2s) Loss: 0.1414(0.1060) \n",
            "[3][219/390] Elapsed: 2m 22s (remain 1m 49s) Loss: 0.0486(0.1063) \n",
            "[3][239/390] Elapsed: 2m 35s (remain 1m 36s) Loss: 0.0747(0.1059) \n",
            "[3][259/390] Elapsed: 2m 47s (remain 1m 23s) Loss: 0.0900(0.1062) \n",
            "[3][279/390] Elapsed: 3m 0s (remain 1m 11s) Loss: 0.0644(0.1061) \n",
            "[3][299/390] Elapsed: 3m 13s (remain 0m 58s) Loss: 0.0600(0.1055) \n",
            "[3][319/390] Elapsed: 3m 26s (remain 0m 45s) Loss: 0.1014(0.1058) \n",
            "[3][339/390] Elapsed: 3m 39s (remain 0m 32s) Loss: 0.0875(0.1063) \n",
            "[3][359/390] Elapsed: 3m 52s (remain 0m 19s) Loss: 0.1462(0.1072) \n",
            "[3][379/390] Elapsed: 4m 5s (remain 0m 6s) Loss: 0.2202(0.1070) \n",
            "[3][390/390] Elapsed: 4m 12s (remain -1m 59s) Loss: 0.0395(0.1064) \n",
            "Epoch: 4 - avg_train_loss = 0.1142 avg_val_loss: 0.1064 time: 1853.9828s\n",
            "Epoch: 4 - Score: 0.4621, Scores: [0.49392432737964026, 0.46997680288942983, 0.4139375903620926, 0.4635038972300725, 0.49026636386908345, 0.4407722557647633]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/1565] Elapsed: 0m 20s (remain 26m 35s) Loss: 0.1068(0.1393) Grad: 428486.5 LR: 0.00024077 \n",
            "[4][39/1565] Elapsed: 0m 41s (remain 26m 13s) Loss: 0.1615(0.1349) Grad: 379528.6875 LR: 0.00024153 \n",
            "[4][59/1565] Elapsed: 1m 1s (remain 25m 48s) Loss: 0.2405(0.1392) Grad: 549583.0625 LR: 0.00024230 \n",
            "[4][79/1565] Elapsed: 1m 22s (remain 25m 25s) Loss: 0.0942(0.1326) Grad: 265684.5625 LR: 0.00024307 \n",
            "[4][99/1565] Elapsed: 1m 42s (remain 25m 3s) Loss: 0.1987(0.1321) Grad: 197711.421875 LR: 0.00024383 \n",
            "[4][119/1565] Elapsed: 2m 3s (remain 24m 42s) Loss: 0.0650(0.1327) Grad: 110972.03125 LR: 0.00024460 \n",
            "[4][139/1565] Elapsed: 2m 23s (remain 24m 21s) Loss: 0.0368(0.1262) Grad: 72889.9140625 LR: 0.00024537 \n",
            "[4][159/1565] Elapsed: 2m 44s (remain 24m 0s) Loss: 0.0497(0.1255) Grad: 97169.609375 LR: 0.00024613 \n",
            "[4][179/1565] Elapsed: 3m 4s (remain 23m 39s) Loss: 0.1176(0.1246) Grad: 291356.65625 LR: 0.00024690 \n",
            "[4][199/1565] Elapsed: 3m 25s (remain 23m 19s) Loss: 0.0813(0.1235) Grad: 230435.796875 LR: 0.00024767 \n",
            "[4][219/1565] Elapsed: 3m 45s (remain 22m 58s) Loss: 0.0859(0.1227) Grad: 277822.40625 LR: 0.00024843 \n",
            "[4][239/1565] Elapsed: 4m 5s (remain 22m 37s) Loss: 0.0593(0.1217) Grad: 257139.21875 LR: 0.00024920 \n",
            "[4][259/1565] Elapsed: 4m 26s (remain 22m 16s) Loss: 0.1086(0.1198) Grad: 195832.234375 LR: 0.00024997 \n",
            "[4][279/1565] Elapsed: 4m 46s (remain 21m 55s) Loss: 0.0729(0.1197) Grad: 148937.046875 LR: 0.00025073 \n",
            "[4][299/1565] Elapsed: 5m 7s (remain 21m 35s) Loss: 0.1136(0.1185) Grad: 176406.59375 LR: 0.00025150 \n",
            "[4][319/1565] Elapsed: 5m 27s (remain 21m 14s) Loss: 0.0422(0.1184) Grad: 127633.765625 LR: 0.00025227 \n",
            "[4][339/1565] Elapsed: 5m 48s (remain 20m 54s) Loss: 0.0383(0.1189) Grad: 136678.59375 LR: 0.00025304 \n",
            "[4][359/1565] Elapsed: 6m 8s (remain 20m 33s) Loss: 0.1851(0.1185) Grad: 382593.78125 LR: 0.00025380 \n",
            "[4][379/1565] Elapsed: 6m 28s (remain 20m 12s) Loss: 0.0804(0.1192) Grad: 188154.328125 LR: 0.00025457 \n",
            "[4][399/1565] Elapsed: 6m 49s (remain 19m 52s) Loss: 0.1102(0.1187) Grad: 264954.84375 LR: 0.00025534 \n",
            "[4][419/1565] Elapsed: 7m 9s (remain 19m 31s) Loss: 0.2823(0.1184) Grad: 507781.59375 LR: 0.00025610 \n",
            "[4][439/1565] Elapsed: 7m 30s (remain 19m 11s) Loss: 0.0885(0.1177) Grad: 279933.71875 LR: 0.00025687 \n",
            "[4][459/1565] Elapsed: 7m 50s (remain 18m 50s) Loss: 0.0725(0.1167) Grad: 225651.671875 LR: 0.00025764 \n",
            "[4][479/1565] Elapsed: 8m 11s (remain 18m 30s) Loss: 0.0832(0.1169) Grad: 170981.6875 LR: 0.00025840 \n",
            "[4][499/1565] Elapsed: 8m 31s (remain 18m 9s) Loss: 0.0914(0.1171) Grad: 223137.0 LR: 0.00025917 \n",
            "[4][519/1565] Elapsed: 8m 52s (remain 17m 49s) Loss: 0.1477(0.1170) Grad: 319128.46875 LR: 0.00025994 \n",
            "[4][539/1565] Elapsed: 9m 12s (remain 17m 28s) Loss: 0.0877(0.1160) Grad: 259691.375 LR: 0.00026070 \n",
            "[4][559/1565] Elapsed: 9m 33s (remain 17m 8s) Loss: 0.0222(0.1158) Grad: 53756.3671875 LR: 0.00026147 \n",
            "[4][579/1565] Elapsed: 9m 53s (remain 16m 47s) Loss: 0.1809(0.1154) Grad: 193161.15625 LR: 0.00026224 \n",
            "[4][599/1565] Elapsed: 10m 13s (remain 16m 27s) Loss: 0.0369(0.1150) Grad: 167291.671875 LR: 0.00026300 \n",
            "[4][619/1565] Elapsed: 10m 34s (remain 16m 7s) Loss: 0.0962(0.1155) Grad: 154473.84375 LR: 0.00026377 \n",
            "[4][639/1565] Elapsed: 10m 54s (remain 15m 46s) Loss: 0.1914(0.1145) Grad: 306840.65625 LR: 0.00026454 \n",
            "[4][659/1565] Elapsed: 11m 15s (remain 15m 26s) Loss: 0.1554(0.1146) Grad: 509471.71875 LR: 0.00026530 \n",
            "[4][679/1565] Elapsed: 11m 35s (remain 15m 5s) Loss: 0.2204(0.1143) Grad: 583984.875 LR: 0.00026607 \n",
            "[4][699/1565] Elapsed: 11m 56s (remain 14m 45s) Loss: 0.0381(0.1144) Grad: 109736.2578125 LR: 0.00026684 \n",
            "[4][719/1565] Elapsed: 12m 16s (remain 14m 24s) Loss: 0.0732(0.1137) Grad: 121519.046875 LR: 0.00026760 \n",
            "[4][739/1565] Elapsed: 12m 37s (remain 14m 4s) Loss: 0.3395(0.1144) Grad: 354247.03125 LR: 0.00026837 \n",
            "[4][759/1565] Elapsed: 12m 57s (remain 13m 43s) Loss: 0.0581(0.1142) Grad: 282735.71875 LR: 0.00026914 \n",
            "[4][779/1565] Elapsed: 13m 18s (remain 13m 23s) Loss: 0.0724(0.1136) Grad: 98039.8984375 LR: 0.00026990 \n",
            "[4][799/1565] Elapsed: 13m 38s (remain 13m 2s) Loss: 0.0617(0.1130) Grad: 245066.25 LR: 0.00027067 \n",
            "[4][819/1565] Elapsed: 13m 58s (remain 12m 42s) Loss: 0.1482(0.1127) Grad: 412632.1875 LR: 0.00027144 \n",
            "[4][839/1565] Elapsed: 14m 19s (remain 12m 21s) Loss: 0.2386(0.1134) Grad: 424621.53125 LR: 0.00027220 \n",
            "[4][859/1565] Elapsed: 14m 39s (remain 12m 1s) Loss: 0.2270(0.1141) Grad: 379516.34375 LR: 0.00027297 \n",
            "[4][879/1565] Elapsed: 15m 0s (remain 11m 40s) Loss: 0.0567(0.1141) Grad: 94155.921875 LR: 0.00027374 \n",
            "[4][899/1565] Elapsed: 15m 20s (remain 11m 20s) Loss: 0.0655(0.1141) Grad: 176116.265625 LR: 0.00027450 \n",
            "[4][919/1565] Elapsed: 15m 41s (remain 10m 59s) Loss: 0.1841(0.1141) Grad: 381879.65625 LR: 0.00027527 \n",
            "[4][939/1565] Elapsed: 16m 1s (remain 10m 39s) Loss: 0.1443(0.1141) Grad: 316433.5625 LR: 0.00027604 \n",
            "[4][959/1565] Elapsed: 16m 22s (remain 10m 18s) Loss: 0.0654(0.1139) Grad: 165395.25 LR: 0.00027681 \n",
            "[4][979/1565] Elapsed: 16m 42s (remain 9m 58s) Loss: 0.1216(0.1136) Grad: 246453.40625 LR: 0.00027757 \n",
            "[4][999/1565] Elapsed: 17m 2s (remain 9m 37s) Loss: 0.0680(0.1135) Grad: 219833.5 LR: 0.00027834 \n",
            "[4][1019/1565] Elapsed: 17m 23s (remain 9m 17s) Loss: 0.2463(0.1139) Grad: 240706.96875 LR: 0.00027911 \n",
            "[4][1039/1565] Elapsed: 17m 43s (remain 8m 57s) Loss: 0.1488(0.1138) Grad: 483167.90625 LR: 0.00027987 \n",
            "[4][1059/1565] Elapsed: 18m 4s (remain 8m 36s) Loss: 0.0305(0.1132) Grad: 87317.0390625 LR: 0.00028064 \n",
            "[4][1079/1565] Elapsed: 18m 24s (remain 8m 16s) Loss: 0.0724(0.1130) Grad: 154639.6875 LR: 0.00028141 \n",
            "[4][1099/1565] Elapsed: 18m 45s (remain 7m 55s) Loss: 0.0916(0.1133) Grad: 150319.5625 LR: 0.00028217 \n",
            "[4][1119/1565] Elapsed: 19m 5s (remain 7m 35s) Loss: 0.1512(0.1132) Grad: 365313.59375 LR: 0.00028294 \n",
            "[4][1139/1565] Elapsed: 19m 26s (remain 7m 14s) Loss: 0.1731(0.1134) Grad: 386992.65625 LR: 0.00028371 \n",
            "[4][1159/1565] Elapsed: 19m 46s (remain 6m 54s) Loss: 0.0638(0.1135) Grad: 225077.984375 LR: 0.00028447 \n",
            "[4][1179/1565] Elapsed: 20m 7s (remain 6m 33s) Loss: 0.0490(0.1137) Grad: 123229.4921875 LR: 0.00028524 \n",
            "[4][1199/1565] Elapsed: 20m 27s (remain 6m 13s) Loss: 0.1895(0.1140) Grad: 390152.46875 LR: 0.00028601 \n",
            "[4][1219/1565] Elapsed: 20m 48s (remain 5m 52s) Loss: 0.0551(0.1143) Grad: 84019.6875 LR: 0.00028677 \n",
            "[4][1239/1565] Elapsed: 21m 8s (remain 5m 32s) Loss: 0.2555(0.1146) Grad: 323872.84375 LR: 0.00028754 \n",
            "[4][1259/1565] Elapsed: 21m 29s (remain 5m 12s) Loss: 0.1352(0.1146) Grad: 447562.0 LR: 0.00028831 \n",
            "[4][1279/1565] Elapsed: 21m 49s (remain 4m 51s) Loss: 0.3400(0.1146) Grad: 727011.125 LR: 0.00028907 \n",
            "[4][1299/1565] Elapsed: 22m 9s (remain 4m 31s) Loss: 0.0573(0.1149) Grad: 95149.0625 LR: 0.00028984 \n",
            "[4][1319/1565] Elapsed: 22m 30s (remain 4m 10s) Loss: 0.0352(0.1148) Grad: 132357.84375 LR: 0.00029061 \n",
            "[4][1339/1565] Elapsed: 22m 50s (remain 3m 50s) Loss: 0.1088(0.1147) Grad: 209495.21875 LR: 0.00029137 \n",
            "[4][1359/1565] Elapsed: 23m 11s (remain 3m 29s) Loss: 0.0864(0.1149) Grad: 321393.25 LR: 0.00029214 \n",
            "[4][1379/1565] Elapsed: 23m 31s (remain 3m 9s) Loss: 0.1786(0.1148) Grad: 505427.125 LR: 0.00029291 \n",
            "[4][1399/1565] Elapsed: 23m 52s (remain 2m 48s) Loss: 0.1457(0.1149) Grad: 181180.796875 LR: 0.00029367 \n",
            "[4][1419/1565] Elapsed: 24m 13s (remain 2m 28s) Loss: 0.0827(0.1149) Grad: 101062.265625 LR: 0.00029444 \n",
            "[4][1439/1565] Elapsed: 24m 33s (remain 2m 7s) Loss: 0.1331(0.1150) Grad: 160154.546875 LR: 0.00029521 \n",
            "[4][1459/1565] Elapsed: 24m 54s (remain 1m 47s) Loss: 0.1032(0.1149) Grad: 131618.9375 LR: 0.00029597 \n",
            "[4][1479/1565] Elapsed: 25m 14s (remain 1m 26s) Loss: 0.0858(0.1144) Grad: 354092.6875 LR: 0.00029674 \n",
            "[4][1499/1565] Elapsed: 25m 34s (remain 1m 6s) Loss: 0.3456(0.1146) Grad: 527872.5625 LR: 0.00029751 \n",
            "[4][1519/1565] Elapsed: 25m 55s (remain 0m 46s) Loss: 0.0753(0.1144) Grad: 227431.40625 LR: 0.00029827 \n",
            "[4][1539/1565] Elapsed: 26m 15s (remain 0m 25s) Loss: 0.0339(0.1141) Grad: 110030.15625 LR: 0.00029904 \n",
            "[4][1559/1565] Elapsed: 26m 36s (remain 0m 5s) Loss: 0.0789(0.1140) Grad: 279518.3125 LR: 0.00029981 \n",
            "[4][19/390] Elapsed: 0m 13s (remain 4m 1s) Loss: 0.1334(0.1120) \n",
            "[4][39/390] Elapsed: 0m 25s (remain 3m 47s) Loss: 0.0706(0.1020) \n",
            "[4][59/390] Elapsed: 0m 38s (remain 3m 33s) Loss: 0.1398(0.1096) \n",
            "[4][79/390] Elapsed: 0m 51s (remain 3m 20s) Loss: 0.1259(0.1074) \n",
            "[4][99/390] Elapsed: 1m 4s (remain 3m 7s) Loss: 0.1489(0.1112) \n",
            "[4][119/390] Elapsed: 1m 17s (remain 2m 54s) Loss: 0.1947(0.1127) \n",
            "[4][139/390] Elapsed: 1m 30s (remain 2m 41s) Loss: 0.0489(0.1111) \n",
            "[4][159/390] Elapsed: 1m 43s (remain 2m 28s) Loss: 0.1010(0.1100) \n",
            "[4][179/390] Elapsed: 1m 56s (remain 2m 15s) Loss: 0.0380(0.1088) \n",
            "[4][199/390] Elapsed: 2m 9s (remain 2m 2s) Loss: 0.0815(0.1070) \n",
            "[4][219/390] Elapsed: 2m 22s (remain 1m 49s) Loss: 0.1033(0.1080) \n",
            "[4][239/390] Elapsed: 2m 35s (remain 1m 36s) Loss: 0.1231(0.1073) \n",
            "[4][259/390] Elapsed: 2m 47s (remain 1m 23s) Loss: 0.1041(0.1081) \n",
            "[4][279/390] Elapsed: 3m 0s (remain 1m 11s) Loss: 0.0809(0.1084) \n",
            "[4][299/390] Elapsed: 3m 13s (remain 0m 58s) Loss: 0.0490(0.1090) \n",
            "[4][319/390] Elapsed: 3m 26s (remain 0m 45s) Loss: 0.0374(0.1078) \n",
            "[4][339/390] Elapsed: 3m 39s (remain 0m 32s) Loss: 0.1757(0.1093) \n",
            "[4][359/390] Elapsed: 3m 52s (remain 0m 19s) Loss: 0.0914(0.1107) \n",
            "[4][379/390] Elapsed: 4m 5s (remain 0m 6s) Loss: 0.2839(0.1111) \n",
            "[4][390/390] Elapsed: 4m 12s (remain -1m 59s) Loss: 0.0556(0.1107) \n",
            "Epoch: 5 - avg_train_loss = 0.1141 avg_val_loss: 0.1107 time: 1853.7699s\n",
            "Epoch: 5 - Score: 0.4716, Scores: [0.5002603234653321, 0.46803860324324137, 0.42162930021801276, 0.4695954326552189, 0.4914218969365889, 0.4784660139041404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names:  ['.embeddings.']\n",
            "names:  ['encoder.LayerNorm', 'rel_embeddings']\n",
            "names:  ['.0.', '.1.', '.2.', '.3.', '.4.', '.5.']\n",
            "names:  ['.6.', '.7.', '.8.', '.9.', '.10.', '.11.']\n",
            "names:  ['.12.', '.13.', '.14.', '.15.', '.16.', '.17.', '.18.', '.19.', '.20.', '.21.', '.22.']\n",
            "names:  ['.23.']\n",
            "warmup start:  7810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/1562] Elapsed: 0m 20s (remain 26m 31s) Loss: 2.5822(2.5861) Grad: 421879.21875 LR: 0.00000077 \n",
            "[0][39/1562] Elapsed: 0m 41s (remain 26m 11s) Loss: 2.2719(2.5632) Grad: 412118.90625 LR: 0.00000154 \n",
            "[0][59/1562] Elapsed: 1m 1s (remain 25m 45s) Loss: 2.1032(2.5009) Grad: 405502.125 LR: 0.00000230 \n",
            "[0][79/1562] Elapsed: 1m 22s (remain 25m 21s) Loss: 2.8923(2.5036) Grad: 424057.34375 LR: 0.00000307 \n",
            "[0][99/1562] Elapsed: 1m 42s (remain 24m 59s) Loss: 3.0062(2.4763) Grad: 408744.0 LR: 0.00000384 \n",
            "[0][119/1562] Elapsed: 2m 3s (remain 24m 38s) Loss: 2.8028(2.4590) Grad: 422890.65625 LR: 0.00000461 \n",
            "[0][139/1562] Elapsed: 2m 23s (remain 24m 17s) Loss: 2.1468(2.4378) Grad: 429266.90625 LR: 0.00000538 \n",
            "[0][159/1562] Elapsed: 2m 43s (remain 23m 56s) Loss: 2.0887(2.3841) Grad: 422323.53125 LR: 0.00000615 \n",
            "[0][179/1562] Elapsed: 3m 4s (remain 23m 35s) Loss: 2.4856(2.3521) Grad: 425937.46875 LR: 0.00000691 \n",
            "[0][199/1562] Elapsed: 3m 24s (remain 23m 14s) Loss: 1.8583(2.3019) Grad: 430980.5 LR: 0.00000768 \n",
            "[0][219/1562] Elapsed: 3m 45s (remain 22m 53s) Loss: 1.3027(2.2578) Grad: 428642.125 LR: 0.00000845 \n",
            "[0][239/1562] Elapsed: 4m 5s (remain 22m 33s) Loss: 1.6586(2.2149) Grad: 452632.84375 LR: 0.00000922 \n",
            "[0][259/1562] Elapsed: 4m 26s (remain 22m 12s) Loss: 0.6923(2.1598) Grad: 355007.875 LR: 0.00000999 \n",
            "[0][279/1562] Elapsed: 4m 46s (remain 21m 52s) Loss: 1.0942(2.1146) Grad: 432683.96875 LR: 0.00001076 \n",
            "[0][299/1562] Elapsed: 5m 7s (remain 21m 31s) Loss: 1.4138(2.0546) Grad: 473659.75 LR: 0.00001152 \n",
            "[0][319/1562] Elapsed: 5m 27s (remain 21m 11s) Loss: 1.4154(1.9930) Grad: 497504.5 LR: 0.00001229 \n",
            "[0][339/1562] Elapsed: 5m 47s (remain 20m 50s) Loss: 0.8583(1.9234) Grad: 507930.59375 LR: 0.00001306 \n",
            "[0][359/1562] Elapsed: 6m 8s (remain 20m 30s) Loss: 0.6803(1.8513) Grad: 475167.90625 LR: 0.00001383 \n",
            "[0][379/1562] Elapsed: 6m 28s (remain 20m 9s) Loss: 0.3326(1.7830) Grad: 293589.03125 LR: 0.00001460 \n",
            "[0][399/1562] Elapsed: 6m 49s (remain 19m 49s) Loss: 0.0864(1.7083) Grad: 191610.375 LR: 0.00001536 \n",
            "[0][419/1562] Elapsed: 7m 9s (remain 19m 28s) Loss: 0.0986(1.6375) Grad: 197553.8125 LR: 0.00001613 \n",
            "[0][439/1562] Elapsed: 7m 30s (remain 19m 8s) Loss: 0.1357(1.5747) Grad: 265900.9375 LR: 0.00001690 \n",
            "[0][459/1562] Elapsed: 7m 50s (remain 18m 47s) Loss: 0.1137(1.5132) Grad: 148899.9375 LR: 0.00001767 \n",
            "[0][479/1562] Elapsed: 8m 11s (remain 18m 27s) Loss: 0.1086(1.4611) Grad: 241129.71875 LR: 0.00001844 \n",
            "[0][499/1562] Elapsed: 8m 31s (remain 18m 6s) Loss: 0.3311(1.4102) Grad: 363495.40625 LR: 0.00001921 \n",
            "[0][519/1562] Elapsed: 8m 52s (remain 17m 46s) Loss: 0.1681(1.3623) Grad: 301203.1875 LR: 0.00001997 \n",
            "[0][539/1562] Elapsed: 9m 12s (remain 17m 25s) Loss: 0.1004(1.3184) Grad: 136958.359375 LR: 0.00002074 \n",
            "[0][559/1562] Elapsed: 9m 33s (remain 17m 5s) Loss: 0.1557(1.2796) Grad: 313263.71875 LR: 0.00002151 \n",
            "[0][579/1562] Elapsed: 9m 53s (remain 16m 44s) Loss: 0.1346(1.2419) Grad: 309353.0625 LR: 0.00002228 \n",
            "[0][599/1562] Elapsed: 10m 13s (remain 16m 24s) Loss: 0.1051(1.2072) Grad: 263275.0625 LR: 0.00002305 \n",
            "[0][619/1562] Elapsed: 10m 34s (remain 16m 3s) Loss: 0.2059(1.1738) Grad: 390349.0625 LR: 0.00002382 \n",
            "[0][639/1562] Elapsed: 10m 54s (remain 15m 43s) Loss: 0.0750(1.1418) Grad: 105303.7734375 LR: 0.00002458 \n",
            "[0][659/1562] Elapsed: 11m 15s (remain 15m 22s) Loss: 0.1664(1.1122) Grad: 137829.78125 LR: 0.00002535 \n",
            "[0][679/1562] Elapsed: 11m 35s (remain 15m 2s) Loss: 0.1275(1.0854) Grad: 136019.578125 LR: 0.00002612 \n",
            "[0][699/1562] Elapsed: 11m 56s (remain 14m 41s) Loss: 0.3538(1.0602) Grad: 214461.109375 LR: 0.00002689 \n",
            "[0][719/1562] Elapsed: 12m 16s (remain 14m 21s) Loss: 0.1519(1.0351) Grad: 188409.828125 LR: 0.00002766 \n",
            "[0][739/1562] Elapsed: 12m 37s (remain 14m 1s) Loss: 0.1314(1.0122) Grad: 118360.875 LR: 0.00002843 \n",
            "[0][759/1562] Elapsed: 12m 57s (remain 13m 40s) Loss: 0.1091(0.9894) Grad: 204423.40625 LR: 0.00002919 \n",
            "[0][779/1562] Elapsed: 13m 18s (remain 13m 20s) Loss: 0.1026(0.9681) Grad: 174383.859375 LR: 0.00002996 \n",
            "[0][799/1562] Elapsed: 13m 38s (remain 12m 59s) Loss: 0.3698(0.9481) Grad: 398543.09375 LR: 0.00003073 \n",
            "[0][819/1562] Elapsed: 13m 58s (remain 12m 39s) Loss: 0.2770(0.9283) Grad: 195456.5625 LR: 0.00003150 \n",
            "[0][839/1562] Elapsed: 14m 19s (remain 12m 18s) Loss: 0.2012(0.9101) Grad: 352393.71875 LR: 0.00003227 \n",
            "[0][859/1562] Elapsed: 14m 39s (remain 11m 58s) Loss: 0.0447(0.8918) Grad: 143056.390625 LR: 0.00003303 \n",
            "[0][879/1562] Elapsed: 15m 0s (remain 11m 37s) Loss: 0.0975(0.8747) Grad: 159032.859375 LR: 0.00003380 \n",
            "[0][899/1562] Elapsed: 15m 20s (remain 11m 17s) Loss: 0.0953(0.8588) Grad: 155716.890625 LR: 0.00003457 \n",
            "[0][919/1562] Elapsed: 15m 41s (remain 10m 56s) Loss: 0.2990(0.8432) Grad: 419448.59375 LR: 0.00003534 \n",
            "[0][939/1562] Elapsed: 16m 1s (remain 10m 36s) Loss: 0.0761(0.8285) Grad: 138566.703125 LR: 0.00003611 \n",
            "[0][959/1562] Elapsed: 16m 22s (remain 10m 15s) Loss: 0.1899(0.8147) Grad: 356821.625 LR: 0.00003688 \n",
            "[0][979/1562] Elapsed: 16m 42s (remain 9m 55s) Loss: 0.0678(0.8011) Grad: 141170.9375 LR: 0.00003764 \n",
            "[0][999/1562] Elapsed: 17m 3s (remain 9m 35s) Loss: 0.1159(0.7879) Grad: 306700.03125 LR: 0.00003841 \n",
            "[0][1019/1562] Elapsed: 17m 23s (remain 9m 14s) Loss: 0.2937(0.7755) Grad: 347912.59375 LR: 0.00003918 \n",
            "[0][1039/1562] Elapsed: 17m 44s (remain 8m 54s) Loss: 0.0814(0.7631) Grad: 150487.53125 LR: 0.00003995 \n",
            "[0][1059/1562] Elapsed: 18m 4s (remain 8m 33s) Loss: 0.2113(0.7514) Grad: 522245.28125 LR: 0.00004072 \n",
            "[0][1079/1562] Elapsed: 18m 25s (remain 8m 13s) Loss: 0.1862(0.7408) Grad: 304380.46875 LR: 0.00004149 \n",
            "[0][1099/1562] Elapsed: 18m 45s (remain 7m 52s) Loss: 0.1184(0.7303) Grad: 218930.40625 LR: 0.00004225 \n",
            "[0][1119/1562] Elapsed: 19m 5s (remain 7m 32s) Loss: 0.0923(0.7201) Grad: 258414.296875 LR: 0.00004302 \n",
            "[0][1139/1562] Elapsed: 19m 26s (remain 7m 11s) Loss: 0.0915(0.7095) Grad: 131004.03125 LR: 0.00004379 \n",
            "[0][1159/1562] Elapsed: 19m 46s (remain 6m 51s) Loss: 0.0424(0.6995) Grad: 94713.8828125 LR: 0.00004456 \n",
            "[0][1179/1562] Elapsed: 20m 7s (remain 6m 30s) Loss: 0.4476(0.6903) Grad: 490645.1875 LR: 0.00004533 \n",
            "[0][1199/1562] Elapsed: 20m 27s (remain 6m 10s) Loss: 0.1183(0.6807) Grad: 177064.5625 LR: 0.00004609 \n",
            "[0][1219/1562] Elapsed: 20m 48s (remain 5m 49s) Loss: 0.1665(0.6718) Grad: 304512.5 LR: 0.00004686 \n",
            "[0][1239/1562] Elapsed: 21m 8s (remain 5m 29s) Loss: 0.0903(0.6632) Grad: 126453.3671875 LR: 0.00004763 \n",
            "[0][1259/1562] Elapsed: 21m 29s (remain 5m 8s) Loss: 0.0574(0.6550) Grad: 119538.3359375 LR: 0.00004840 \n",
            "[0][1279/1562] Elapsed: 21m 49s (remain 4m 48s) Loss: 0.4924(0.6480) Grad: 353619.5 LR: 0.00004917 \n",
            "[0][1299/1562] Elapsed: 22m 9s (remain 4m 28s) Loss: 0.0777(0.6400) Grad: 255294.453125 LR: 0.00004994 \n",
            "[0][1319/1562] Elapsed: 22m 30s (remain 4m 7s) Loss: 0.0953(0.6320) Grad: 234655.03125 LR: 0.00005070 \n",
            "[0][1339/1562] Elapsed: 22m 50s (remain 3m 47s) Loss: 0.1212(0.6247) Grad: 184975.75 LR: 0.00005147 \n",
            "[0][1359/1562] Elapsed: 23m 11s (remain 3m 26s) Loss: 0.0257(0.6172) Grad: 92270.0234375 LR: 0.00005224 \n",
            "[0][1379/1562] Elapsed: 23m 31s (remain 3m 6s) Loss: 0.0671(0.6101) Grad: 224146.890625 LR: 0.00005301 \n",
            "[0][1399/1562] Elapsed: 23m 52s (remain 2m 45s) Loss: 0.0858(0.6033) Grad: 131597.4375 LR: 0.00005378 \n",
            "[0][1419/1562] Elapsed: 24m 12s (remain 2m 25s) Loss: 0.4038(0.5968) Grad: 271666.15625 LR: 0.00005455 \n",
            "[0][1439/1562] Elapsed: 24m 33s (remain 2m 4s) Loss: 0.0745(0.5901) Grad: 198969.203125 LR: 0.00005531 \n",
            "[0][1459/1562] Elapsed: 24m 53s (remain 1m 44s) Loss: 0.1196(0.5838) Grad: 328409.46875 LR: 0.00005608 \n",
            "[0][1479/1562] Elapsed: 25m 14s (remain 1m 23s) Loss: 0.0827(0.5777) Grad: 139319.1875 LR: 0.00005685 \n",
            "[0][1499/1562] Elapsed: 25m 34s (remain 1m 3s) Loss: 0.1030(0.5717) Grad: 215036.703125 LR: 0.00005762 \n",
            "[0][1519/1562] Elapsed: 25m 55s (remain 0m 42s) Loss: 0.1823(0.5655) Grad: 210269.0625 LR: 0.00005839 \n",
            "[0][1539/1562] Elapsed: 26m 15s (remain 0m 22s) Loss: 0.1345(0.5595) Grad: 180553.625 LR: 0.00005915 \n",
            "[0][1559/1562] Elapsed: 26m 36s (remain 0m 2s) Loss: 0.1688(0.5538) Grad: 339387.1875 LR: 0.00005992 \n",
            "[0][19/393] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0567(0.1170) \n",
            "[0][39/393] Elapsed: 0m 25s (remain 3m 49s) Loss: 0.1885(0.1253) \n",
            "[0][59/393] Elapsed: 0m 38s (remain 3m 35s) Loss: 0.4818(0.1258) \n",
            "[0][79/393] Elapsed: 0m 51s (remain 3m 22s) Loss: 0.0586(0.1212) \n",
            "[0][99/393] Elapsed: 1m 4s (remain 3m 9s) Loss: 0.0468(0.1170) \n",
            "[0][119/393] Elapsed: 1m 17s (remain 2m 56s) Loss: 0.0791(0.1152) \n",
            "[0][139/393] Elapsed: 1m 30s (remain 2m 43s) Loss: 0.1271(0.1144) \n",
            "[0][159/393] Elapsed: 1m 43s (remain 2m 30s) Loss: 0.1276(0.1157) \n",
            "[0][179/393] Elapsed: 1m 56s (remain 2m 17s) Loss: 0.2342(0.1167) \n",
            "[0][199/393] Elapsed: 2m 8s (remain 2m 4s) Loss: 0.1798(0.1167) \n",
            "[0][219/393] Elapsed: 2m 21s (remain 1m 51s) Loss: 0.1410(0.1165) \n",
            "[0][239/393] Elapsed: 2m 34s (remain 1m 38s) Loss: 0.0540(0.1172) \n",
            "[0][259/393] Elapsed: 2m 47s (remain 1m 25s) Loss: 0.0511(0.1179) \n",
            "[0][279/393] Elapsed: 3m 0s (remain 1m 12s) Loss: 0.1118(0.1161) \n",
            "[0][299/393] Elapsed: 3m 13s (remain 0m 59s) Loss: 0.0869(0.1157) \n",
            "[0][319/393] Elapsed: 3m 26s (remain 0m 47s) Loss: 0.1459(0.1162) \n",
            "[0][339/393] Elapsed: 3m 38s (remain 0m 34s) Loss: 0.2094(0.1171) \n",
            "[0][359/393] Elapsed: 3m 51s (remain 0m 21s) Loss: 0.0807(0.1175) \n",
            "[0][379/393] Elapsed: 4m 4s (remain 0m 8s) Loss: 0.0602(0.1170) \n",
            "[0][393/393] Elapsed: 4m 13s (remain -1m 59s) Loss: 0.1858(0.1164) \n",
            "Epoch: 1 - avg_train_loss = 0.5532 avg_val_loss: 0.1164 time: 1851.8593s\n",
            "Epoch: 1 - Score: 0.4837, Scores: [0.5266572250796118, 0.47004477113387294, 0.4580829003497461, 0.4710708888893129, 0.5029570116819282, 0.4736551959578214]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/1562] Elapsed: 0m 20s (remain 26m 34s) Loss: 0.0825(0.1125) Grad: 135373.21875 LR: 0.00006077 \n",
            "[1][39/1562] Elapsed: 0m 41s (remain 26m 12s) Loss: 0.2375(0.1240) Grad: 586580.125 LR: 0.00006154 \n",
            "[1][59/1562] Elapsed: 1m 1s (remain 25m 47s) Loss: 0.0353(0.1282) Grad: 100453.2421875 LR: 0.00006230 \n",
            "[1][79/1562] Elapsed: 1m 22s (remain 25m 23s) Loss: 0.0683(0.1283) Grad: 100715.7421875 LR: 0.00006307 \n",
            "[1][99/1562] Elapsed: 1m 42s (remain 25m 1s) Loss: 0.0503(0.1236) Grad: 144491.828125 LR: 0.00006384 \n",
            "[1][119/1562] Elapsed: 2m 3s (remain 24m 40s) Loss: 0.2626(0.1250) Grad: 615623.5 LR: 0.00006461 \n",
            "[1][139/1562] Elapsed: 2m 23s (remain 24m 18s) Loss: 0.0913(0.1230) Grad: 137272.453125 LR: 0.00006538 \n",
            "[1][159/1562] Elapsed: 2m 44s (remain 23m 58s) Loss: 0.1474(0.1235) Grad: 452404.875 LR: 0.00006615 \n",
            "[1][179/1562] Elapsed: 3m 4s (remain 23m 37s) Loss: 0.1129(0.1228) Grad: 327447.09375 LR: 0.00006691 \n",
            "[1][199/1562] Elapsed: 3m 25s (remain 23m 16s) Loss: 0.1275(0.1209) Grad: 328846.75 LR: 0.00006768 \n",
            "[1][219/1562] Elapsed: 3m 45s (remain 22m 55s) Loss: 0.2146(0.1217) Grad: 333232.0625 LR: 0.00006845 \n",
            "[1][239/1562] Elapsed: 4m 5s (remain 22m 34s) Loss: 0.0805(0.1204) Grad: 121014.515625 LR: 0.00006922 \n",
            "[1][259/1562] Elapsed: 4m 26s (remain 22m 14s) Loss: 0.0843(0.1211) Grad: 127673.6953125 LR: 0.00006999 \n",
            "[1][279/1562] Elapsed: 4m 46s (remain 21m 53s) Loss: 0.0616(0.1189) Grad: 260806.625 LR: 0.00007076 \n",
            "[1][299/1562] Elapsed: 5m 7s (remain 21m 32s) Loss: 0.2395(0.1196) Grad: 656847.5 LR: 0.00007152 \n",
            "[1][319/1562] Elapsed: 5m 27s (remain 21m 12s) Loss: 0.0908(0.1209) Grad: 176602.984375 LR: 0.00007229 \n",
            "[1][339/1562] Elapsed: 5m 48s (remain 20m 51s) Loss: 0.1503(0.1206) Grad: 231222.984375 LR: 0.00007306 \n",
            "[1][359/1562] Elapsed: 6m 8s (remain 20m 30s) Loss: 0.0731(0.1205) Grad: 107622.1171875 LR: 0.00007383 \n",
            "[1][379/1562] Elapsed: 6m 29s (remain 20m 10s) Loss: 0.0728(0.1207) Grad: 150963.0625 LR: 0.00007460 \n",
            "[1][399/1562] Elapsed: 6m 49s (remain 19m 49s) Loss: 0.1741(0.1209) Grad: 393325.3125 LR: 0.00007536 \n",
            "[1][419/1562] Elapsed: 7m 10s (remain 19m 29s) Loss: 0.0405(0.1204) Grad: 158872.765625 LR: 0.00007613 \n",
            "[1][439/1562] Elapsed: 7m 30s (remain 19m 8s) Loss: 0.0591(0.1202) Grad: 139523.75 LR: 0.00007690 \n",
            "[1][459/1562] Elapsed: 7m 51s (remain 18m 48s) Loss: 0.1836(0.1205) Grad: 311231.5625 LR: 0.00007767 \n",
            "[1][479/1562] Elapsed: 8m 11s (remain 18m 27s) Loss: 0.1093(0.1212) Grad: 139299.453125 LR: 0.00007844 \n",
            "[1][499/1562] Elapsed: 8m 31s (remain 18m 6s) Loss: 0.0883(0.1207) Grad: 209430.75 LR: 0.00007921 \n",
            "[1][519/1562] Elapsed: 8m 52s (remain 17m 46s) Loss: 0.0720(0.1205) Grad: 99999.0390625 LR: 0.00007997 \n",
            "[1][539/1562] Elapsed: 9m 12s (remain 17m 25s) Loss: 0.1698(0.1207) Grad: 104683.7421875 LR: 0.00008074 \n",
            "[1][559/1562] Elapsed: 9m 33s (remain 17m 5s) Loss: 0.1194(0.1197) Grad: 166025.171875 LR: 0.00008151 \n",
            "[1][579/1562] Elapsed: 9m 53s (remain 16m 44s) Loss: 0.2161(0.1198) Grad: 273500.46875 LR: 0.00008228 \n",
            "[1][599/1562] Elapsed: 10m 13s (remain 16m 24s) Loss: 0.1451(0.1199) Grad: 87930.4921875 LR: 0.00008305 \n",
            "[1][619/1562] Elapsed: 10m 34s (remain 16m 3s) Loss: 0.0993(0.1201) Grad: 91428.2421875 LR: 0.00008382 \n",
            "[1][639/1562] Elapsed: 10m 54s (remain 15m 43s) Loss: 0.0425(0.1195) Grad: 38584.171875 LR: 0.00008458 \n",
            "[1][659/1562] Elapsed: 11m 15s (remain 15m 22s) Loss: 0.0260(0.1191) Grad: 43858.65625 LR: 0.00008535 \n",
            "[1][679/1562] Elapsed: 11m 35s (remain 15m 2s) Loss: 0.1567(0.1188) Grad: 229054.90625 LR: 0.00008612 \n",
            "[1][699/1562] Elapsed: 11m 56s (remain 14m 41s) Loss: 0.0803(0.1183) Grad: 99575.9609375 LR: 0.00008689 \n",
            "[1][719/1562] Elapsed: 12m 16s (remain 14m 21s) Loss: 0.1757(0.1185) Grad: 231730.921875 LR: 0.00008766 \n",
            "[1][739/1562] Elapsed: 12m 37s (remain 14m 1s) Loss: 0.0775(0.1181) Grad: 126490.5859375 LR: 0.00008843 \n",
            "[1][759/1562] Elapsed: 12m 57s (remain 13m 40s) Loss: 0.0976(0.1185) Grad: 122248.859375 LR: 0.00008919 \n",
            "[1][779/1562] Elapsed: 13m 18s (remain 13m 20s) Loss: 0.2042(0.1189) Grad: 191827.390625 LR: 0.00008996 \n",
            "[1][799/1562] Elapsed: 13m 38s (remain 12m 59s) Loss: 0.1136(0.1187) Grad: 152019.453125 LR: 0.00009073 \n",
            "[1][819/1562] Elapsed: 13m 58s (remain 12m 39s) Loss: 0.0675(0.1187) Grad: 74280.6015625 LR: 0.00009150 \n",
            "[1][839/1562] Elapsed: 14m 19s (remain 12m 18s) Loss: 0.1019(0.1190) Grad: 151094.578125 LR: 0.00009227 \n",
            "[1][859/1562] Elapsed: 14m 40s (remain 11m 58s) Loss: 0.0787(0.1192) Grad: 83102.671875 LR: 0.00009303 \n",
            "[1][879/1562] Elapsed: 15m 0s (remain 11m 37s) Loss: 0.1361(0.1192) Grad: 168152.375 LR: 0.00009380 \n",
            "[1][899/1562] Elapsed: 15m 20s (remain 11m 17s) Loss: 0.0816(0.1190) Grad: 135383.359375 LR: 0.00009457 \n",
            "[1][919/1562] Elapsed: 15m 41s (remain 10m 56s) Loss: 0.1098(0.1194) Grad: 249138.21875 LR: 0.00009534 \n",
            "[1][939/1562] Elapsed: 16m 1s (remain 10m 36s) Loss: 0.2498(0.1204) Grad: 327552.40625 LR: 0.00009611 \n",
            "[1][959/1562] Elapsed: 16m 22s (remain 10m 15s) Loss: 0.1405(0.1206) Grad: 151962.421875 LR: 0.00009688 \n",
            "[1][979/1562] Elapsed: 16m 42s (remain 9m 55s) Loss: 0.1633(0.1209) Grad: 123865.9140625 LR: 0.00009764 \n",
            "[1][999/1562] Elapsed: 17m 3s (remain 9m 35s) Loss: 0.0971(0.1210) Grad: 117888.453125 LR: 0.00009841 \n",
            "[1][1019/1562] Elapsed: 17m 23s (remain 9m 14s) Loss: 0.1514(0.1207) Grad: 140849.09375 LR: 0.00009918 \n",
            "[1][1039/1562] Elapsed: 17m 44s (remain 8m 54s) Loss: 0.0246(0.1205) Grad: 68897.5390625 LR: 0.00009995 \n",
            "[1][1059/1562] Elapsed: 18m 4s (remain 8m 33s) Loss: 0.0807(0.1199) Grad: 131392.296875 LR: 0.00010072 \n",
            "[1][1079/1562] Elapsed: 18m 25s (remain 8m 13s) Loss: 0.1063(0.1197) Grad: 181199.46875 LR: 0.00010149 \n",
            "[1][1099/1562] Elapsed: 18m 45s (remain 7m 52s) Loss: 0.0878(0.1195) Grad: 80039.90625 LR: 0.00010225 \n",
            "[1][1119/1562] Elapsed: 19m 5s (remain 7m 32s) Loss: 0.0810(0.1201) Grad: 101970.3046875 LR: 0.00010302 \n",
            "[1][1139/1562] Elapsed: 19m 26s (remain 7m 11s) Loss: 0.1110(0.1197) Grad: 91821.90625 LR: 0.00010379 \n",
            "[1][1159/1562] Elapsed: 19m 46s (remain 6m 51s) Loss: 0.0851(0.1200) Grad: 83092.09375 LR: 0.00010456 \n",
            "[1][1179/1562] Elapsed: 20m 7s (remain 6m 30s) Loss: 0.1256(0.1197) Grad: 133007.96875 LR: 0.00010533 \n",
            "[1][1199/1562] Elapsed: 20m 27s (remain 6m 10s) Loss: 0.0380(0.1196) Grad: 93850.3125 LR: 0.00010609 \n",
            "[1][1219/1562] Elapsed: 20m 48s (remain 5m 49s) Loss: 0.0938(0.1201) Grad: 90340.8203125 LR: 0.00010686 \n",
            "[1][1239/1562] Elapsed: 21m 8s (remain 5m 29s) Loss: 0.0640(0.1200) Grad: 114295.109375 LR: 0.00010763 \n",
            "[1][1259/1562] Elapsed: 21m 29s (remain 5m 8s) Loss: 0.0748(0.1199) Grad: 71805.0546875 LR: 0.00010840 \n",
            "[1][1279/1562] Elapsed: 21m 49s (remain 4m 48s) Loss: 0.1312(0.1198) Grad: 144674.5625 LR: 0.00010917 \n",
            "[1][1299/1562] Elapsed: 22m 10s (remain 4m 28s) Loss: 0.2645(0.1205) Grad: 294674.8125 LR: 0.00010994 \n",
            "[1][1319/1562] Elapsed: 22m 30s (remain 4m 7s) Loss: 0.0707(0.1202) Grad: 114292.5625 LR: 0.00011070 \n",
            "[1][1339/1562] Elapsed: 22m 51s (remain 3m 47s) Loss: 0.0950(0.1199) Grad: 92500.53125 LR: 0.00011147 \n",
            "[1][1359/1562] Elapsed: 23m 11s (remain 3m 26s) Loss: 0.0812(0.1200) Grad: 119391.5078125 LR: 0.00011224 \n",
            "[1][1379/1562] Elapsed: 23m 32s (remain 3m 6s) Loss: 0.0348(0.1198) Grad: 51010.703125 LR: 0.00011301 \n",
            "[1][1399/1562] Elapsed: 23m 52s (remain 2m 45s) Loss: 0.1637(0.1197) Grad: 194393.71875 LR: 0.00011378 \n",
            "[1][1419/1562] Elapsed: 24m 12s (remain 2m 25s) Loss: 0.1754(0.1196) Grad: 239641.90625 LR: 0.00011455 \n",
            "[1][1439/1562] Elapsed: 24m 33s (remain 2m 4s) Loss: 0.0295(0.1196) Grad: 27708.57421875 LR: 0.00011531 \n",
            "[1][1459/1562] Elapsed: 24m 53s (remain 1m 44s) Loss: 0.1124(0.1194) Grad: 111142.109375 LR: 0.00011608 \n",
            "[1][1479/1562] Elapsed: 25m 14s (remain 1m 23s) Loss: 0.0559(0.1198) Grad: 78503.8671875 LR: 0.00011685 \n",
            "[1][1499/1562] Elapsed: 25m 34s (remain 1m 3s) Loss: 0.0940(0.1195) Grad: 200736.296875 LR: 0.00011762 \n",
            "[1][1519/1562] Elapsed: 25m 55s (remain 0m 42s) Loss: 0.2541(0.1196) Grad: 304803.15625 LR: 0.00011839 \n",
            "[1][1539/1562] Elapsed: 26m 15s (remain 0m 22s) Loss: 0.1904(0.1197) Grad: 119195.7578125 LR: 0.00011915 \n",
            "[1][1559/1562] Elapsed: 26m 36s (remain 0m 2s) Loss: 0.1086(0.1199) Grad: 177015.0625 LR: 0.00011992 \n",
            "[1][19/393] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0475(0.1082) \n",
            "[1][39/393] Elapsed: 0m 25s (remain 3m 49s) Loss: 0.1395(0.1155) \n",
            "[1][59/393] Elapsed: 0m 38s (remain 3m 35s) Loss: 0.4708(0.1173) \n",
            "[1][79/393] Elapsed: 0m 51s (remain 3m 22s) Loss: 0.0437(0.1134) \n",
            "[1][99/393] Elapsed: 1m 4s (remain 3m 9s) Loss: 0.0599(0.1101) \n",
            "[1][119/393] Elapsed: 1m 17s (remain 2m 56s) Loss: 0.0632(0.1084) \n",
            "[1][139/393] Elapsed: 1m 30s (remain 2m 43s) Loss: 0.1336(0.1075) \n",
            "[1][159/393] Elapsed: 1m 43s (remain 2m 30s) Loss: 0.1057(0.1084) \n",
            "[1][179/393] Elapsed: 1m 56s (remain 2m 17s) Loss: 0.2135(0.1095) \n",
            "[1][199/393] Elapsed: 2m 9s (remain 2m 4s) Loss: 0.1897(0.1098) \n",
            "[1][219/393] Elapsed: 2m 21s (remain 1m 51s) Loss: 0.1225(0.1090) \n",
            "[1][239/393] Elapsed: 2m 34s (remain 1m 38s) Loss: 0.0438(0.1087) \n",
            "[1][259/393] Elapsed: 2m 47s (remain 1m 25s) Loss: 0.1002(0.1090) \n",
            "[1][279/393] Elapsed: 3m 0s (remain 1m 12s) Loss: 0.1066(0.1084) \n",
            "[1][299/393] Elapsed: 3m 13s (remain 0m 59s) Loss: 0.0936(0.1082) \n",
            "[1][319/393] Elapsed: 3m 26s (remain 0m 47s) Loss: 0.1349(0.1091) \n",
            "[1][339/393] Elapsed: 3m 39s (remain 0m 34s) Loss: 0.1730(0.1097) \n",
            "[1][359/393] Elapsed: 3m 52s (remain 0m 21s) Loss: 0.0758(0.1100) \n",
            "[1][379/393] Elapsed: 4m 5s (remain 0m 8s) Loss: 0.0591(0.1101) \n",
            "[1][393/393] Elapsed: 4m 14s (remain -1m 59s) Loss: 0.1071(0.1097) \n",
            "Epoch: 2 - avg_train_loss = 0.1198 avg_val_loss: 0.1097 time: 1852.5687s\n",
            "Epoch: 2 - Score: 0.4693, Scores: [0.48184679244107853, 0.47247460355968807, 0.45823519298474474, 0.4417031197558423, 0.46646783233221667, 0.4951265168205355]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/1562] Elapsed: 0m 20s (remain 26m 38s) Loss: 0.0887(0.1197) Grad: 344986.3125 LR: 0.00012077 \n",
            "[2][39/1562] Elapsed: 0m 41s (remain 26m 15s) Loss: 0.1320(0.1113) Grad: 193154.1875 LR: 0.00012154 \n",
            "[2][59/1562] Elapsed: 1m 1s (remain 25m 50s) Loss: 0.1323(0.1100) Grad: 396117.90625 LR: 0.00012230 \n",
            "[2][79/1562] Elapsed: 1m 22s (remain 25m 25s) Loss: 0.1283(0.1066) Grad: 199982.796875 LR: 0.00012307 \n",
            "[2][99/1562] Elapsed: 1m 42s (remain 25m 3s) Loss: 0.0469(0.1060) Grad: 189191.84375 LR: 0.00012384 \n",
            "[2][119/1562] Elapsed: 2m 3s (remain 24m 42s) Loss: 0.1210(0.1078) Grad: 365705.125 LR: 0.00012461 \n",
            "[2][139/1562] Elapsed: 2m 23s (remain 24m 20s) Loss: 0.1263(0.1116) Grad: 393103.03125 LR: 0.00012538 \n",
            "[2][159/1562] Elapsed: 2m 44s (remain 23m 59s) Loss: 0.0402(0.1116) Grad: 120760.046875 LR: 0.00012615 \n",
            "[2][179/1562] Elapsed: 3m 4s (remain 23m 38s) Loss: 0.0664(0.1105) Grad: 140190.9375 LR: 0.00012691 \n",
            "[2][199/1562] Elapsed: 3m 25s (remain 23m 17s) Loss: 0.0378(0.1108) Grad: 101865.5625 LR: 0.00012768 \n",
            "[2][219/1562] Elapsed: 3m 45s (remain 22m 56s) Loss: 0.0496(0.1108) Grad: 203998.25 LR: 0.00012845 \n",
            "[2][239/1562] Elapsed: 4m 6s (remain 22m 35s) Loss: 0.0857(0.1111) Grad: 317822.90625 LR: 0.00012922 \n",
            "[2][259/1562] Elapsed: 4m 26s (remain 22m 14s) Loss: 0.0689(0.1110) Grad: 169467.671875 LR: 0.00012999 \n",
            "[2][279/1562] Elapsed: 4m 47s (remain 21m 54s) Loss: 0.0377(0.1113) Grad: 202637.03125 LR: 0.00013076 \n",
            "[2][299/1562] Elapsed: 5m 7s (remain 21m 33s) Loss: 0.0638(0.1118) Grad: 213291.65625 LR: 0.00013152 \n",
            "[2][319/1562] Elapsed: 5m 28s (remain 21m 13s) Loss: 0.1299(0.1108) Grad: 294003.125 LR: 0.00013229 \n",
            "[2][339/1562] Elapsed: 5m 48s (remain 20m 52s) Loss: 0.0655(0.1097) Grad: 140788.953125 LR: 0.00013306 \n",
            "[2][359/1562] Elapsed: 6m 8s (remain 20m 31s) Loss: 0.0476(0.1103) Grad: 121031.3359375 LR: 0.00013383 \n",
            "[2][379/1562] Elapsed: 6m 29s (remain 20m 11s) Loss: 0.0507(0.1092) Grad: 163625.0625 LR: 0.00013460 \n",
            "[2][399/1562] Elapsed: 6m 49s (remain 19m 50s) Loss: 0.0467(0.1096) Grad: 114867.9140625 LR: 0.00013536 \n",
            "[2][419/1562] Elapsed: 7m 10s (remain 19m 29s) Loss: 0.1684(0.1100) Grad: 397182.625 LR: 0.00013613 \n",
            "[2][439/1562] Elapsed: 7m 30s (remain 19m 9s) Loss: 0.1291(0.1108) Grad: 449613.6875 LR: 0.00013690 \n",
            "[2][459/1562] Elapsed: 7m 51s (remain 18m 48s) Loss: 0.0220(0.1123) Grad: 73516.15625 LR: 0.00013767 \n",
            "[2][479/1562] Elapsed: 8m 11s (remain 18m 27s) Loss: 0.0858(0.1131) Grad: 61164.3828125 LR: 0.00013844 \n",
            "[2][499/1562] Elapsed: 8m 31s (remain 18m 7s) Loss: 0.1467(0.1132) Grad: 160358.3125 LR: 0.00013921 \n",
            "[2][519/1562] Elapsed: 8m 52s (remain 17m 46s) Loss: 0.1534(0.1130) Grad: 235412.765625 LR: 0.00013997 \n",
            "[2][539/1562] Elapsed: 9m 12s (remain 17m 26s) Loss: 0.0882(0.1132) Grad: 65383.515625 LR: 0.00014074 \n",
            "[2][559/1562] Elapsed: 9m 33s (remain 17m 5s) Loss: 0.0745(0.1126) Grad: 104401.7734375 LR: 0.00014151 \n",
            "[2][579/1562] Elapsed: 9m 53s (remain 16m 45s) Loss: 0.1062(0.1124) Grad: 88388.0546875 LR: 0.00014228 \n",
            "[2][599/1562] Elapsed: 10m 14s (remain 16m 24s) Loss: 0.0655(0.1129) Grad: 85750.5859375 LR: 0.00014305 \n",
            "[2][619/1562] Elapsed: 10m 34s (remain 16m 4s) Loss: 0.0822(0.1133) Grad: 147882.375 LR: 0.00014382 \n",
            "[2][639/1562] Elapsed: 10m 55s (remain 15m 43s) Loss: 0.1129(0.1137) Grad: 173940.578125 LR: 0.00014458 \n",
            "[2][659/1562] Elapsed: 11m 15s (remain 15m 23s) Loss: 0.2247(0.1137) Grad: 137828.140625 LR: 0.00014535 \n",
            "[2][679/1562] Elapsed: 11m 36s (remain 15m 2s) Loss: 0.2321(0.1139) Grad: 229304.84375 LR: 0.00014612 \n",
            "[2][699/1562] Elapsed: 11m 56s (remain 14m 42s) Loss: 0.1091(0.1139) Grad: 171667.640625 LR: 0.00014689 \n",
            "[2][719/1562] Elapsed: 12m 17s (remain 14m 22s) Loss: 0.0627(0.1138) Grad: 126309.390625 LR: 0.00014766 \n",
            "[2][739/1562] Elapsed: 12m 37s (remain 14m 1s) Loss: 0.1815(0.1139) Grad: 229982.625 LR: 0.00014843 \n",
            "[2][759/1562] Elapsed: 12m 58s (remain 13m 41s) Loss: 0.0810(0.1140) Grad: 74987.953125 LR: 0.00014919 \n",
            "[2][779/1562] Elapsed: 13m 18s (remain 13m 20s) Loss: 0.1166(0.1138) Grad: 146101.53125 LR: 0.00014996 \n",
            "[2][799/1562] Elapsed: 13m 38s (remain 13m 0s) Loss: 0.0637(0.1138) Grad: 112527.4453125 LR: 0.00015073 \n",
            "[2][819/1562] Elapsed: 13m 59s (remain 12m 39s) Loss: 0.1780(0.1140) Grad: 208235.078125 LR: 0.00015150 \n",
            "[2][839/1562] Elapsed: 14m 19s (remain 12m 19s) Loss: 0.0829(0.1139) Grad: 121395.8984375 LR: 0.00015227 \n",
            "[2][859/1562] Elapsed: 14m 40s (remain 11m 58s) Loss: 0.0941(0.1140) Grad: 158301.453125 LR: 0.00015303 \n",
            "[2][879/1562] Elapsed: 15m 0s (remain 11m 38s) Loss: 0.0391(0.1141) Grad: 74548.8125 LR: 0.00015380 \n",
            "[2][899/1562] Elapsed: 15m 21s (remain 11m 17s) Loss: 0.0971(0.1142) Grad: 110826.5859375 LR: 0.00015457 \n",
            "[2][919/1562] Elapsed: 15m 41s (remain 10m 57s) Loss: 0.1648(0.1144) Grad: 245500.890625 LR: 0.00015534 \n",
            "[2][939/1562] Elapsed: 16m 2s (remain 10m 36s) Loss: 0.1115(0.1144) Grad: 94610.9609375 LR: 0.00015611 \n",
            "[2][959/1562] Elapsed: 16m 22s (remain 10m 16s) Loss: 0.0944(0.1148) Grad: 115914.75 LR: 0.00015688 \n",
            "[2][979/1562] Elapsed: 16m 43s (remain 9m 55s) Loss: 0.3413(0.1150) Grad: 311608.96875 LR: 0.00015764 \n",
            "[2][999/1562] Elapsed: 17m 3s (remain 9m 35s) Loss: 0.0919(0.1152) Grad: 116982.71875 LR: 0.00015841 \n",
            "[2][1019/1562] Elapsed: 17m 24s (remain 9m 14s) Loss: 0.1178(0.1150) Grad: 124886.09375 LR: 0.00015918 \n",
            "[2][1039/1562] Elapsed: 17m 44s (remain 8m 54s) Loss: 0.0906(0.1146) Grad: 103453.6796875 LR: 0.00015995 \n",
            "[2][1059/1562] Elapsed: 18m 4s (remain 8m 33s) Loss: 0.0925(0.1142) Grad: 71283.3046875 LR: 0.00016072 \n",
            "[2][1079/1562] Elapsed: 18m 25s (remain 8m 13s) Loss: 0.2042(0.1146) Grad: 237794.6875 LR: 0.00016149 \n",
            "[2][1099/1562] Elapsed: 18m 45s (remain 7m 52s) Loss: 0.1051(0.1147) Grad: 85039.046875 LR: 0.00016225 \n",
            "[2][1119/1562] Elapsed: 19m 6s (remain 7m 32s) Loss: 0.1053(0.1146) Grad: 156340.71875 LR: 0.00016302 \n",
            "[2][1139/1562] Elapsed: 19m 26s (remain 7m 11s) Loss: 0.0529(0.1146) Grad: 58501.35546875 LR: 0.00016379 \n",
            "[2][1159/1562] Elapsed: 19m 47s (remain 6m 51s) Loss: 0.0643(0.1145) Grad: 115743.140625 LR: 0.00016456 \n",
            "[2][1179/1562] Elapsed: 20m 7s (remain 6m 30s) Loss: 0.1154(0.1143) Grad: 135985.125 LR: 0.00016533 \n",
            "[2][1199/1562] Elapsed: 20m 28s (remain 6m 10s) Loss: 0.0450(0.1142) Grad: 81110.9296875 LR: 0.00016609 \n",
            "[2][1219/1562] Elapsed: 20m 48s (remain 5m 50s) Loss: 0.0672(0.1144) Grad: 74631.75 LR: 0.00016686 \n",
            "[2][1239/1562] Elapsed: 21m 9s (remain 5m 29s) Loss: 0.1121(0.1147) Grad: 85807.96875 LR: 0.00016763 \n",
            "[2][1259/1562] Elapsed: 21m 29s (remain 5m 9s) Loss: 0.0772(0.1150) Grad: 72709.1171875 LR: 0.00016840 \n",
            "[2][1279/1562] Elapsed: 21m 50s (remain 4m 48s) Loss: 0.1421(0.1146) Grad: 88514.5234375 LR: 0.00016917 \n",
            "[2][1299/1562] Elapsed: 22m 10s (remain 4m 28s) Loss: 0.0817(0.1145) Grad: 119322.8203125 LR: 0.00016994 \n",
            "[2][1319/1562] Elapsed: 22m 30s (remain 4m 7s) Loss: 0.1310(0.1146) Grad: 121477.25 LR: 0.00017070 \n",
            "[2][1339/1562] Elapsed: 22m 51s (remain 3m 47s) Loss: 0.0882(0.1145) Grad: 86324.90625 LR: 0.00017147 \n",
            "[2][1359/1562] Elapsed: 23m 11s (remain 3m 26s) Loss: 0.2980(0.1148) Grad: 244082.5625 LR: 0.00017224 \n",
            "[2][1379/1562] Elapsed: 23m 32s (remain 3m 6s) Loss: 0.1292(0.1148) Grad: 233078.109375 LR: 0.00017301 \n",
            "[2][1399/1562] Elapsed: 23m 52s (remain 2m 45s) Loss: 0.0879(0.1148) Grad: 158080.21875 LR: 0.00017378 \n",
            "[2][1419/1562] Elapsed: 24m 13s (remain 2m 25s) Loss: 0.0775(0.1147) Grad: 92062.28125 LR: 0.00017455 \n",
            "[2][1439/1562] Elapsed: 24m 33s (remain 2m 4s) Loss: 0.0734(0.1145) Grad: 73821.3515625 LR: 0.00017531 \n",
            "[2][1459/1562] Elapsed: 24m 54s (remain 1m 44s) Loss: 0.0972(0.1148) Grad: 76209.4609375 LR: 0.00017608 \n",
            "[2][1479/1562] Elapsed: 25m 14s (remain 1m 23s) Loss: 0.0656(0.1151) Grad: 79490.625 LR: 0.00017685 \n",
            "[2][1499/1562] Elapsed: 25m 34s (remain 1m 3s) Loss: 0.0730(0.1152) Grad: 64668.45703125 LR: 0.00017762 \n",
            "[2][1519/1562] Elapsed: 25m 55s (remain 0m 42s) Loss: 0.2480(0.1153) Grad: 218990.671875 LR: 0.00017839 \n",
            "[2][1539/1562] Elapsed: 26m 15s (remain 0m 22s) Loss: 0.0893(0.1153) Grad: 145035.046875 LR: 0.00017915 \n",
            "[2][1559/1562] Elapsed: 26m 36s (remain 0m 2s) Loss: 0.0877(0.1156) Grad: 63977.80859375 LR: 0.00017992 \n",
            "[2][19/393] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.0366(0.1128) \n",
            "[2][39/393] Elapsed: 0m 26s (remain 3m 49s) Loss: 0.1543(0.1176) \n",
            "[2][59/393] Elapsed: 0m 39s (remain 3m 36s) Loss: 0.4872(0.1166) \n",
            "[2][79/393] Elapsed: 0m 51s (remain 3m 23s) Loss: 0.0408(0.1117) \n",
            "[2][99/393] Elapsed: 1m 4s (remain 3m 10s) Loss: 0.0430(0.1081) \n",
            "[2][119/393] Elapsed: 1m 17s (remain 2m 56s) Loss: 0.0502(0.1067) \n",
            "[2][139/393] Elapsed: 1m 30s (remain 2m 43s) Loss: 0.1067(0.1050) \n",
            "[2][159/393] Elapsed: 1m 43s (remain 2m 30s) Loss: 0.1139(0.1061) \n",
            "[2][179/393] Elapsed: 1m 56s (remain 2m 17s) Loss: 0.1942(0.1075) \n",
            "[2][199/393] Elapsed: 2m 9s (remain 2m 4s) Loss: 0.1525(0.1076) \n",
            "[2][219/393] Elapsed: 2m 22s (remain 1m 51s) Loss: 0.0970(0.1069) \n",
            "[2][239/393] Elapsed: 2m 35s (remain 1m 38s) Loss: 0.0471(0.1068) \n",
            "[2][259/393] Elapsed: 2m 48s (remain 1m 25s) Loss: 0.0728(0.1074) \n",
            "[2][279/393] Elapsed: 3m 0s (remain 1m 13s) Loss: 0.1170(0.1065) \n",
            "[2][299/393] Elapsed: 3m 13s (remain 1m 0s) Loss: 0.0897(0.1063) \n",
            "[2][319/393] Elapsed: 3m 26s (remain 0m 47s) Loss: 0.1580(0.1079) \n",
            "[2][339/393] Elapsed: 3m 39s (remain 0m 34s) Loss: 0.1657(0.1087) \n",
            "[2][359/393] Elapsed: 3m 52s (remain 0m 21s) Loss: 0.0944(0.1091) \n",
            "[2][379/393] Elapsed: 4m 5s (remain 0m 8s) Loss: 0.0702(0.1093) \n",
            "[2][393/393] Elapsed: 4m 14s (remain -1m 59s) Loss: 0.1017(0.1090) \n",
            "Epoch: 3 - avg_train_loss = 0.1157 avg_val_loss: 0.1090 time: 1853.0356s\n",
            "Epoch: 3 - Score: 0.4672, Scores: [0.5130515601882393, 0.4390140685055859, 0.41666049305392383, 0.503736117565563, 0.4983748844324652, 0.4323047394601088]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3][19/1562] Elapsed: 0m 20s (remain 26m 35s) Loss: 0.1054(0.0979) Grad: 231275.40625 LR: 0.00018077 \n",
            "[3][39/1562] Elapsed: 0m 41s (remain 26m 11s) Loss: 0.0851(0.1128) Grad: 207987.71875 LR: 0.00018154 \n",
            "[3][59/1562] Elapsed: 1m 1s (remain 25m 47s) Loss: 0.1515(0.1128) Grad: 305321.25 LR: 0.00018230 \n",
            "[3][79/1562] Elapsed: 1m 22s (remain 25m 22s) Loss: 0.0876(0.1050) Grad: 319365.0625 LR: 0.00018307 \n",
            "[3][99/1562] Elapsed: 1m 42s (remain 25m 1s) Loss: 0.0912(0.1078) Grad: 185644.890625 LR: 0.00018384 \n",
            "[3][119/1562] Elapsed: 2m 3s (remain 24m 40s) Loss: 0.0938(0.1060) Grad: 261269.9375 LR: 0.00018461 \n",
            "[3][139/1562] Elapsed: 2m 23s (remain 24m 19s) Loss: 0.0640(0.1049) Grad: 209501.859375 LR: 0.00018538 \n",
            "[3][159/1562] Elapsed: 2m 44s (remain 23m 57s) Loss: 0.0826(0.1041) Grad: 243573.140625 LR: 0.00018615 \n",
            "[3][179/1562] Elapsed: 3m 4s (remain 23m 36s) Loss: 0.0378(0.1044) Grad: 117632.9921875 LR: 0.00018691 \n",
            "[3][199/1562] Elapsed: 3m 25s (remain 23m 16s) Loss: 0.1053(0.1059) Grad: 210351.796875 LR: 0.00018768 \n",
            "[3][219/1562] Elapsed: 3m 45s (remain 22m 55s) Loss: 0.1530(0.1048) Grad: 492686.46875 LR: 0.00018845 \n",
            "[3][239/1562] Elapsed: 4m 5s (remain 22m 34s) Loss: 0.0678(0.1039) Grad: 170144.328125 LR: 0.00018922 \n",
            "[3][259/1562] Elapsed: 4m 26s (remain 22m 13s) Loss: 0.0340(0.1031) Grad: 89512.7265625 LR: 0.00018999 \n",
            "[3][279/1562] Elapsed: 4m 46s (remain 21m 52s) Loss: 0.0508(0.1041) Grad: 122645.09375 LR: 0.00019076 \n",
            "[3][299/1562] Elapsed: 5m 7s (remain 21m 32s) Loss: 0.1143(0.1036) Grad: 199960.015625 LR: 0.00019152 \n",
            "[3][319/1562] Elapsed: 5m 27s (remain 21m 12s) Loss: 0.1353(0.1040) Grad: 242536.03125 LR: 0.00019229 \n",
            "[3][339/1562] Elapsed: 5m 48s (remain 20m 51s) Loss: 0.0622(0.1046) Grad: 114311.75 LR: 0.00019306 \n",
            "[3][359/1562] Elapsed: 6m 8s (remain 20m 30s) Loss: 0.0691(0.1042) Grad: 104590.875 LR: 0.00019383 \n",
            "[3][379/1562] Elapsed: 6m 29s (remain 20m 10s) Loss: 0.0610(0.1038) Grad: 133349.015625 LR: 0.00019460 \n",
            "[3][399/1562] Elapsed: 6m 49s (remain 19m 49s) Loss: 0.1350(0.1043) Grad: 174505.40625 LR: 0.00019536 \n",
            "[3][419/1562] Elapsed: 7m 10s (remain 19m 29s) Loss: 0.0991(0.1042) Grad: 231098.25 LR: 0.00019613 \n",
            "[3][439/1562] Elapsed: 7m 30s (remain 19m 8s) Loss: 0.1244(0.1062) Grad: 145582.921875 LR: 0.00019690 \n",
            "[3][459/1562] Elapsed: 7m 50s (remain 18m 48s) Loss: 0.0819(0.1067) Grad: 330025.65625 LR: 0.00019767 \n",
            "[3][479/1562] Elapsed: 8m 11s (remain 18m 27s) Loss: 0.1157(0.1068) Grad: 242025.9375 LR: 0.00019844 \n",
            "[3][499/1562] Elapsed: 8m 31s (remain 18m 7s) Loss: 0.0802(0.1078) Grad: 120724.3515625 LR: 0.00019921 \n",
            "[3][519/1562] Elapsed: 8m 52s (remain 17m 46s) Loss: 0.0896(0.1074) Grad: 188652.3125 LR: 0.00019997 \n",
            "[3][539/1562] Elapsed: 9m 12s (remain 17m 26s) Loss: 0.2185(0.1081) Grad: 282809.0 LR: 0.00020074 \n",
            "[3][559/1562] Elapsed: 9m 33s (remain 17m 5s) Loss: 0.0556(0.1080) Grad: 107105.0546875 LR: 0.00020151 \n",
            "[3][579/1562] Elapsed: 9m 54s (remain 16m 45s) Loss: 0.0621(0.1085) Grad: 201855.546875 LR: 0.00020228 \n",
            "[3][599/1562] Elapsed: 10m 14s (remain 16m 25s) Loss: 0.0667(0.1076) Grad: 114589.0625 LR: 0.00020305 \n",
            "[3][619/1562] Elapsed: 10m 34s (remain 16m 4s) Loss: 0.0557(0.1075) Grad: 115019.4140625 LR: 0.00020382 \n",
            "[3][639/1562] Elapsed: 10m 55s (remain 15m 44s) Loss: 0.0867(0.1067) Grad: 195786.171875 LR: 0.00020458 \n",
            "[3][659/1562] Elapsed: 11m 15s (remain 15m 23s) Loss: 0.0507(0.1070) Grad: 185295.25 LR: 0.00020535 \n",
            "[3][679/1562] Elapsed: 11m 36s (remain 15m 3s) Loss: 0.1228(0.1068) Grad: 312214.90625 LR: 0.00020612 \n",
            "[3][699/1562] Elapsed: 11m 56s (remain 14m 42s) Loss: 0.0604(0.1067) Grad: 151293.203125 LR: 0.00020689 \n",
            "[3][719/1562] Elapsed: 12m 17s (remain 14m 22s) Loss: 0.3191(0.1065) Grad: 297259.90625 LR: 0.00020766 \n",
            "[3][739/1562] Elapsed: 12m 37s (remain 14m 1s) Loss: 0.3047(0.1065) Grad: 520476.53125 LR: 0.00020843 \n",
            "[3][759/1562] Elapsed: 12m 58s (remain 13m 41s) Loss: 0.0469(0.1065) Grad: 188906.484375 LR: 0.00020919 \n",
            "[3][779/1562] Elapsed: 13m 18s (remain 13m 20s) Loss: 0.0690(0.1065) Grad: 199274.421875 LR: 0.00020996 \n",
            "[3][799/1562] Elapsed: 13m 39s (remain 13m 0s) Loss: 0.1046(0.1067) Grad: 284995.65625 LR: 0.00021073 \n",
            "[3][819/1562] Elapsed: 13m 59s (remain 12m 39s) Loss: 0.0790(0.1067) Grad: 156650.9375 LR: 0.00021150 \n",
            "[3][839/1562] Elapsed: 14m 20s (remain 12m 19s) Loss: 0.1722(0.1070) Grad: 253403.875 LR: 0.00021227 \n",
            "[3][859/1562] Elapsed: 14m 40s (remain 11m 58s) Loss: 0.0662(0.1072) Grad: 210503.265625 LR: 0.00021303 \n",
            "[3][879/1562] Elapsed: 15m 1s (remain 11m 38s) Loss: 0.1261(0.1071) Grad: 402822.625 LR: 0.00021380 \n",
            "[3][899/1562] Elapsed: 15m 21s (remain 11m 17s) Loss: 0.1350(0.1074) Grad: 186461.5 LR: 0.00021457 \n",
            "[3][919/1562] Elapsed: 15m 42s (remain 10m 57s) Loss: 0.0955(0.1074) Grad: 134451.3125 LR: 0.00021534 \n",
            "[3][939/1562] Elapsed: 16m 2s (remain 10m 36s) Loss: 0.0887(0.1076) Grad: 122957.46875 LR: 0.00021611 \n",
            "[3][959/1562] Elapsed: 16m 23s (remain 10m 16s) Loss: 0.1495(0.1082) Grad: 335891.375 LR: 0.00021688 \n",
            "[3][979/1562] Elapsed: 16m 43s (remain 9m 55s) Loss: 0.0313(0.1084) Grad: 113811.3984375 LR: 0.00021764 \n",
            "[3][999/1562] Elapsed: 17m 3s (remain 9m 35s) Loss: 0.2113(0.1080) Grad: 281062.28125 LR: 0.00021841 \n",
            "[3][1019/1562] Elapsed: 17m 24s (remain 9m 14s) Loss: 0.1478(0.1079) Grad: 234196.03125 LR: 0.00021918 \n",
            "[3][1039/1562] Elapsed: 17m 44s (remain 8m 54s) Loss: 0.0413(0.1079) Grad: 103247.640625 LR: 0.00021995 \n",
            "[3][1059/1562] Elapsed: 18m 5s (remain 8m 33s) Loss: 0.2587(0.1080) Grad: 485230.1875 LR: 0.00022072 \n",
            "[3][1079/1562] Elapsed: 18m 25s (remain 8m 13s) Loss: 0.0525(0.1079) Grad: 188620.890625 LR: 0.00022149 \n",
            "[3][1099/1562] Elapsed: 18m 46s (remain 7m 53s) Loss: 0.1751(0.1081) Grad: 217970.625 LR: 0.00022225 \n",
            "[3][1119/1562] Elapsed: 19m 6s (remain 7m 32s) Loss: 0.2629(0.1083) Grad: 346638.4375 LR: 0.00022302 \n",
            "[3][1139/1562] Elapsed: 19m 27s (remain 7m 12s) Loss: 0.1764(0.1085) Grad: 503222.03125 LR: 0.00022379 \n",
            "[3][1159/1562] Elapsed: 19m 47s (remain 6m 51s) Loss: 0.0777(0.1088) Grad: 228048.265625 LR: 0.00022456 \n",
            "[3][1179/1562] Elapsed: 20m 8s (remain 6m 31s) Loss: 0.0755(0.1086) Grad: 293304.21875 LR: 0.00022533 \n",
            "[3][1199/1562] Elapsed: 20m 28s (remain 6m 10s) Loss: 0.0429(0.1086) Grad: 116124.4765625 LR: 0.00022609 \n",
            "[3][1219/1562] Elapsed: 20m 49s (remain 5m 50s) Loss: 0.3426(0.1087) Grad: 917214.75 LR: 0.00022686 \n",
            "[3][1239/1562] Elapsed: 21m 9s (remain 5m 29s) Loss: 0.1293(0.1084) Grad: 300564.78125 LR: 0.00022763 \n",
            "[3][1259/1562] Elapsed: 21m 29s (remain 5m 9s) Loss: 0.0564(0.1083) Grad: 150258.0 LR: 0.00022840 \n",
            "[3][1279/1562] Elapsed: 21m 50s (remain 4m 48s) Loss: 0.1055(0.1085) Grad: 353075.96875 LR: 0.00022917 \n",
            "[3][1299/1562] Elapsed: 22m 10s (remain 4m 28s) Loss: 0.1660(0.1089) Grad: 345762.46875 LR: 0.00022994 \n",
            "[3][1319/1562] Elapsed: 22m 31s (remain 4m 7s) Loss: 0.1137(0.1088) Grad: 402666.71875 LR: 0.00023070 \n",
            "[3][1339/1562] Elapsed: 22m 51s (remain 3m 47s) Loss: 0.0615(0.1088) Grad: 125955.75 LR: 0.00023147 \n",
            "[3][1359/1562] Elapsed: 23m 12s (remain 3m 26s) Loss: 0.1367(0.1091) Grad: 432753.625 LR: 0.00023224 \n",
            "[3][1379/1562] Elapsed: 23m 32s (remain 3m 6s) Loss: 0.0892(0.1093) Grad: 163849.0 LR: 0.00023301 \n",
            "[3][1399/1562] Elapsed: 23m 53s (remain 2m 45s) Loss: 0.1253(0.1091) Grad: 281553.21875 LR: 0.00023378 \n",
            "[3][1419/1562] Elapsed: 24m 13s (remain 2m 25s) Loss: 0.0748(0.1093) Grad: 314215.4375 LR: 0.00023455 \n",
            "[3][1439/1562] Elapsed: 24m 34s (remain 2m 4s) Loss: 0.1189(0.1096) Grad: 215389.9375 LR: 0.00023531 \n",
            "[3][1459/1562] Elapsed: 24m 54s (remain 1m 44s) Loss: 0.0706(0.1097) Grad: 155078.546875 LR: 0.00023608 \n",
            "[3][1479/1562] Elapsed: 25m 14s (remain 1m 23s) Loss: 0.0642(0.1099) Grad: 149629.515625 LR: 0.00023685 \n",
            "[3][1499/1562] Elapsed: 25m 35s (remain 1m 3s) Loss: 0.0422(0.1097) Grad: 102111.140625 LR: 0.00023762 \n",
            "[3][1519/1562] Elapsed: 25m 55s (remain 0m 42s) Loss: 0.1031(0.1097) Grad: 155860.5 LR: 0.00023839 \n",
            "[3][1539/1562] Elapsed: 26m 16s (remain 0m 22s) Loss: 0.0360(0.1098) Grad: 124643.71875 LR: 0.00023915 \n",
            "[3][1559/1562] Elapsed: 26m 36s (remain 0m 2s) Loss: 0.4726(0.1102) Grad: 599245.5 LR: 0.00023992 \n",
            "[3][19/393] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.0718(0.1578) \n",
            "[3][39/393] Elapsed: 0m 26s (remain 3m 49s) Loss: 0.2279(0.1583) \n",
            "[3][59/393] Elapsed: 0m 38s (remain 3m 36s) Loss: 0.6019(0.1588) \n",
            "[3][79/393] Elapsed: 0m 51s (remain 3m 23s) Loss: 0.0900(0.1530) \n",
            "[3][99/393] Elapsed: 1m 4s (remain 3m 10s) Loss: 0.0567(0.1478) \n",
            "[3][119/393] Elapsed: 1m 17s (remain 2m 57s) Loss: 0.0840(0.1462) \n",
            "[3][139/393] Elapsed: 1m 30s (remain 2m 44s) Loss: 0.1396(0.1433) \n",
            "[3][159/393] Elapsed: 1m 43s (remain 2m 30s) Loss: 0.1613(0.1458) \n",
            "[3][179/393] Elapsed: 1m 56s (remain 2m 17s) Loss: 0.2546(0.1453) \n",
            "[3][199/393] Elapsed: 2m 9s (remain 2m 5s) Loss: 0.1970(0.1442) \n",
            "[3][219/393] Elapsed: 2m 22s (remain 1m 52s) Loss: 0.1031(0.1424) \n",
            "[3][239/393] Elapsed: 2m 35s (remain 1m 39s) Loss: 0.0532(0.1421) \n",
            "[3][259/393] Elapsed: 2m 48s (remain 1m 26s) Loss: 0.1377(0.1424) \n",
            "[3][279/393] Elapsed: 3m 1s (remain 1m 13s) Loss: 0.1741(0.1422) \n",
            "[3][299/393] Elapsed: 3m 14s (remain 1m 0s) Loss: 0.1463(0.1414) \n",
            "[3][319/393] Elapsed: 3m 27s (remain 0m 47s) Loss: 0.2063(0.1429) \n",
            "[3][339/393] Elapsed: 3m 40s (remain 0m 34s) Loss: 0.2119(0.1449) \n",
            "[3][359/393] Elapsed: 3m 53s (remain 0m 21s) Loss: 0.1418(0.1450) \n",
            "[3][379/393] Elapsed: 4m 5s (remain 0m 8s) Loss: 0.1088(0.1456) \n",
            "[3][393/393] Elapsed: 4m 14s (remain -1m 59s) Loss: 0.0630(0.1451) \n",
            "Epoch: 4 - avg_train_loss = 0.1102 avg_val_loss: 0.1451 time: 1853.8550s\n",
            "Epoch: 4 - Score: 0.5383, Scores: [0.6802866193601257, 0.48063225633627626, 0.5219589727482485, 0.4728287782475975, 0.5398293977520184, 0.5341529355639341]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4][19/1562] Elapsed: 0m 20s (remain 26m 27s) Loss: 0.1178(0.0959) Grad: 324035.75 LR: 0.00024077 \n",
            "[4][39/1562] Elapsed: 0m 41s (remain 26m 2s) Loss: 0.1291(0.1048) Grad: 177356.75 LR: 0.00024154 \n",
            "[4][59/1562] Elapsed: 1m 1s (remain 25m 41s) Loss: 0.0852(0.1131) Grad: 156685.03125 LR: 0.00024230 \n",
            "[4][79/1562] Elapsed: 1m 22s (remain 25m 19s) Loss: 0.0959(0.1099) Grad: 162044.1875 LR: 0.00024307 \n",
            "[4][99/1562] Elapsed: 1m 42s (remain 24m 59s) Loss: 0.1848(0.1140) Grad: 386182.0 LR: 0.00024384 \n",
            "[4][119/1562] Elapsed: 2m 3s (remain 24m 38s) Loss: 0.1241(0.1192) Grad: 198847.515625 LR: 0.00024461 \n",
            "[4][139/1562] Elapsed: 2m 23s (remain 24m 17s) Loss: 0.0856(0.1161) Grad: 164805.3125 LR: 0.00024538 \n",
            "[4][159/1562] Elapsed: 2m 43s (remain 23m 56s) Loss: 0.1367(0.1152) Grad: 344367.8125 LR: 0.00024615 \n",
            "[4][179/1562] Elapsed: 3m 4s (remain 23m 35s) Loss: 0.3160(0.1155) Grad: 707532.8125 LR: 0.00024691 \n",
            "[4][199/1562] Elapsed: 3m 24s (remain 23m 15s) Loss: 0.0820(0.1154) Grad: 136909.703125 LR: 0.00024768 \n",
            "[4][219/1562] Elapsed: 3m 45s (remain 22m 54s) Loss: 0.0855(0.1167) Grad: 227849.078125 LR: 0.00024845 \n",
            "[4][239/1562] Elapsed: 4m 5s (remain 22m 34s) Loss: 0.0672(0.1164) Grad: 141343.09375 LR: 0.00024922 \n",
            "[4][259/1562] Elapsed: 4m 26s (remain 22m 13s) Loss: 0.0930(0.1147) Grad: 124999.34375 LR: 0.00024999 \n",
            "[4][279/1562] Elapsed: 4m 46s (remain 21m 52s) Loss: 0.0893(0.1127) Grad: 274711.59375 LR: 0.00025076 \n",
            "[4][299/1562] Elapsed: 5m 7s (remain 21m 32s) Loss: 0.0986(0.1120) Grad: 260428.828125 LR: 0.00025152 \n",
            "[4][319/1562] Elapsed: 5m 27s (remain 21m 11s) Loss: 0.0918(0.1111) Grad: 190353.75 LR: 0.00025229 \n",
            "[4][339/1562] Elapsed: 5m 48s (remain 20m 51s) Loss: 0.0343(0.1111) Grad: 64890.7734375 LR: 0.00025306 \n",
            "[4][359/1562] Elapsed: 6m 8s (remain 20m 30s) Loss: 0.1185(0.1118) Grad: 162965.484375 LR: 0.00025383 \n",
            "[4][379/1562] Elapsed: 6m 29s (remain 20m 10s) Loss: 0.2518(0.1134) Grad: 640560.5625 LR: 0.00025460 \n",
            "[4][399/1562] Elapsed: 6m 49s (remain 19m 49s) Loss: 0.1857(0.1141) Grad: 262036.625 LR: 0.00025536 \n",
            "[4][419/1562] Elapsed: 7m 10s (remain 19m 29s) Loss: 0.0735(0.1135) Grad: 204047.640625 LR: 0.00025613 \n",
            "[4][439/1562] Elapsed: 7m 30s (remain 19m 9s) Loss: 0.2020(0.1129) Grad: 498976.5625 LR: 0.00025690 \n",
            "[4][459/1562] Elapsed: 7m 51s (remain 18m 48s) Loss: 0.1224(0.1129) Grad: 481665.0625 LR: 0.00025767 \n",
            "[4][479/1562] Elapsed: 8m 11s (remain 18m 28s) Loss: 0.1874(0.1128) Grad: 494927.78125 LR: 0.00025844 \n",
            "[4][499/1562] Elapsed: 8m 32s (remain 18m 7s) Loss: 0.0824(0.1135) Grad: 274398.34375 LR: 0.00025921 \n",
            "[4][519/1562] Elapsed: 8m 52s (remain 17m 47s) Loss: 0.1529(0.1143) Grad: 409375.6875 LR: 0.00025997 \n",
            "[4][539/1562] Elapsed: 9m 13s (remain 17m 26s) Loss: 0.0410(0.1143) Grad: 172997.453125 LR: 0.00026074 \n",
            "[4][559/1562] Elapsed: 9m 33s (remain 17m 6s) Loss: 0.0662(0.1143) Grad: 151247.421875 LR: 0.00026151 \n",
            "[4][579/1562] Elapsed: 9m 53s (remain 16m 45s) Loss: 0.0941(0.1134) Grad: 146531.359375 LR: 0.00026228 \n",
            "[4][599/1562] Elapsed: 10m 14s (remain 16m 25s) Loss: 0.1556(0.1124) Grad: 394604.21875 LR: 0.00026305 \n",
            "[4][619/1562] Elapsed: 10m 34s (remain 16m 4s) Loss: 0.0842(0.1128) Grad: 160176.78125 LR: 0.00026382 \n",
            "[4][639/1562] Elapsed: 10m 55s (remain 15m 44s) Loss: 0.1925(0.1126) Grad: 552602.5 LR: 0.00026458 \n",
            "[4][659/1562] Elapsed: 11m 15s (remain 15m 23s) Loss: 0.1168(0.1120) Grad: 265636.28125 LR: 0.00026535 \n",
            "[4][679/1562] Elapsed: 11m 36s (remain 15m 3s) Loss: 0.1259(0.1117) Grad: 367731.75 LR: 0.00026612 \n",
            "[4][699/1562] Elapsed: 11m 56s (remain 14m 42s) Loss: 0.0827(0.1114) Grad: 287154.84375 LR: 0.00026689 \n",
            "[4][719/1562] Elapsed: 12m 17s (remain 14m 22s) Loss: 0.0878(0.1116) Grad: 198404.296875 LR: 0.00026766 \n",
            "[4][739/1562] Elapsed: 12m 37s (remain 14m 1s) Loss: 0.0916(0.1112) Grad: 240867.625 LR: 0.00026843 \n",
            "[4][759/1562] Elapsed: 12m 58s (remain 13m 41s) Loss: 0.1007(0.1113) Grad: 157435.234375 LR: 0.00026919 \n",
            "[4][779/1562] Elapsed: 13m 18s (remain 13m 20s) Loss: 0.2895(0.1114) Grad: 570523.75 LR: 0.00026996 \n",
            "[4][799/1562] Elapsed: 13m 39s (remain 13m 0s) Loss: 0.0906(0.1113) Grad: 266729.5625 LR: 0.00027073 \n",
            "[4][819/1562] Elapsed: 13m 59s (remain 12m 39s) Loss: 0.0648(0.1111) Grad: 153844.5625 LR: 0.00027150 \n",
            "[4][839/1562] Elapsed: 14m 20s (remain 12m 19s) Loss: 0.1162(0.1110) Grad: 173584.765625 LR: 0.00027227 \n",
            "[4][859/1562] Elapsed: 14m 40s (remain 11m 58s) Loss: 0.2672(0.1110) Grad: 287726.90625 LR: 0.00027303 \n",
            "[4][879/1562] Elapsed: 15m 1s (remain 11m 38s) Loss: 0.0598(0.1111) Grad: 146593.25 LR: 0.00027380 \n",
            "[4][899/1562] Elapsed: 15m 21s (remain 11m 17s) Loss: 0.2493(0.1116) Grad: 476413.15625 LR: 0.00027457 \n",
            "[4][919/1562] Elapsed: 15m 42s (remain 10m 57s) Loss: 0.0877(0.1120) Grad: 201159.4375 LR: 0.00027534 \n",
            "[4][939/1562] Elapsed: 16m 2s (remain 10m 36s) Loss: 0.2816(0.1124) Grad: 593392.8125 LR: 0.00027611 \n",
            "[4][959/1562] Elapsed: 16m 22s (remain 10m 16s) Loss: 0.0322(0.1121) Grad: 101978.9453125 LR: 0.00027688 \n",
            "[4][979/1562] Elapsed: 16m 43s (remain 9m 55s) Loss: 0.1626(0.1124) Grad: 462023.375 LR: 0.00027764 \n",
            "[4][999/1562] Elapsed: 17m 3s (remain 9m 35s) Loss: 0.0995(0.1126) Grad: 139959.578125 LR: 0.00027841 \n",
            "[4][1019/1562] Elapsed: 17m 24s (remain 9m 14s) Loss: 0.2251(0.1128) Grad: 535069.8125 LR: 0.00027918 \n",
            "[4][1039/1562] Elapsed: 17m 44s (remain 8m 54s) Loss: 0.0605(0.1125) Grad: 64852.10546875 LR: 0.00027995 \n",
            "[4][1059/1562] Elapsed: 18m 5s (remain 8m 33s) Loss: 0.0425(0.1126) Grad: 92191.9765625 LR: 0.00028072 \n",
            "[4][1079/1562] Elapsed: 18m 25s (remain 8m 13s) Loss: 0.1018(0.1123) Grad: 244225.5 LR: 0.00028149 \n",
            "[4][1099/1562] Elapsed: 18m 46s (remain 7m 52s) Loss: 0.0781(0.1120) Grad: 131720.25 LR: 0.00028225 \n",
            "[4][1119/1562] Elapsed: 19m 6s (remain 7m 32s) Loss: 0.0409(0.1116) Grad: 100070.6328125 LR: 0.00028302 \n",
            "[4][1139/1562] Elapsed: 19m 27s (remain 7m 12s) Loss: 0.1471(0.1120) Grad: 271643.3125 LR: 0.00028379 \n",
            "[4][1159/1562] Elapsed: 19m 47s (remain 6m 51s) Loss: 0.0290(0.1118) Grad: 122137.8828125 LR: 0.00028456 \n",
            "[4][1179/1562] Elapsed: 20m 8s (remain 6m 31s) Loss: 0.2429(0.1120) Grad: 371731.5625 LR: 0.00028533 \n",
            "[4][1199/1562] Elapsed: 20m 28s (remain 6m 10s) Loss: 0.1692(0.1121) Grad: 486031.71875 LR: 0.00028609 \n",
            "[4][1219/1562] Elapsed: 20m 49s (remain 5m 50s) Loss: 0.0619(0.1117) Grad: 137002.25 LR: 0.00028686 \n",
            "[4][1239/1562] Elapsed: 21m 9s (remain 5m 29s) Loss: 0.1009(0.1118) Grad: 261093.234375 LR: 0.00028763 \n",
            "[4][1259/1562] Elapsed: 21m 30s (remain 5m 9s) Loss: 0.1118(0.1117) Grad: 218796.078125 LR: 0.00028840 \n",
            "[4][1279/1562] Elapsed: 21m 50s (remain 4m 48s) Loss: 0.0648(0.1114) Grad: 167834.03125 LR: 0.00028917 \n",
            "[4][1299/1562] Elapsed: 22m 10s (remain 4m 28s) Loss: 0.0570(0.1115) Grad: 187187.953125 LR: 0.00028994 \n",
            "[4][1319/1562] Elapsed: 22m 31s (remain 4m 7s) Loss: 0.0411(0.1111) Grad: 127117.75 LR: 0.00029070 \n",
            "[4][1339/1562] Elapsed: 22m 51s (remain 3m 47s) Loss: 0.1582(0.1117) Grad: 214687.671875 LR: 0.00029147 \n",
            "[4][1359/1562] Elapsed: 23m 12s (remain 3m 26s) Loss: 0.0793(0.1114) Grad: 129241.671875 LR: 0.00029224 \n",
            "[4][1379/1562] Elapsed: 23m 32s (remain 3m 6s) Loss: 0.2446(0.1118) Grad: 465784.03125 LR: 0.00029301 \n",
            "[4][1399/1562] Elapsed: 23m 53s (remain 2m 45s) Loss: 0.0582(0.1119) Grad: 124844.2578125 LR: 0.00029378 \n",
            "[4][1419/1562] Elapsed: 24m 13s (remain 2m 25s) Loss: 0.0822(0.1120) Grad: 111063.578125 LR: 0.00029455 \n",
            "[4][1439/1562] Elapsed: 24m 34s (remain 2m 4s) Loss: 0.0604(0.1121) Grad: 152943.734375 LR: 0.00029531 \n",
            "[4][1459/1562] Elapsed: 24m 54s (remain 1m 44s) Loss: 0.0893(0.1118) Grad: 293763.34375 LR: 0.00029608 \n",
            "[4][1479/1562] Elapsed: 25m 15s (remain 1m 23s) Loss: 0.0994(0.1117) Grad: 360248.09375 LR: 0.00029685 \n",
            "[4][1499/1562] Elapsed: 25m 35s (remain 1m 3s) Loss: 0.0520(0.1116) Grad: 133957.375 LR: 0.00029762 \n",
            "[4][1519/1562] Elapsed: 25m 56s (remain 0m 42s) Loss: 0.0584(0.1114) Grad: 131753.8125 LR: 0.00029839 \n",
            "[4][1539/1562] Elapsed: 26m 16s (remain 0m 22s) Loss: 0.0967(0.1115) Grad: 156331.375 LR: 0.00029915 \n",
            "[4][1559/1562] Elapsed: 26m 37s (remain 0m 2s) Loss: 0.0585(0.1115) Grad: 93251.5390625 LR: 0.00029992 \n",
            "[4][19/393] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0488(0.0910) \n",
            "[4][39/393] Elapsed: 0m 25s (remain 3m 49s) Loss: 0.1139(0.0987) \n",
            "[4][59/393] Elapsed: 0m 38s (remain 3m 36s) Loss: 0.3840(0.1010) \n",
            "[4][79/393] Elapsed: 0m 51s (remain 3m 22s) Loss: 0.0254(0.0958) \n",
            "[4][99/393] Elapsed: 1m 4s (remain 3m 9s) Loss: 0.0597(0.0950) \n",
            "[4][119/393] Elapsed: 1m 17s (remain 2m 56s) Loss: 0.0510(0.0948) \n",
            "[4][139/393] Elapsed: 1m 30s (remain 2m 43s) Loss: 0.0693(0.0934) \n",
            "[4][159/393] Elapsed: 1m 43s (remain 2m 30s) Loss: 0.0641(0.0932) \n",
            "[4][179/393] Elapsed: 1m 56s (remain 2m 17s) Loss: 0.1451(0.0960) \n",
            "[4][199/393] Elapsed: 2m 9s (remain 2m 4s) Loss: 0.1779(0.0965) \n",
            "[4][219/393] Elapsed: 2m 22s (remain 1m 51s) Loss: 0.1265(0.0962) \n",
            "[4][239/393] Elapsed: 2m 35s (remain 1m 38s) Loss: 0.0454(0.0963) \n",
            "[4][259/393] Elapsed: 2m 48s (remain 1m 26s) Loss: 0.0770(0.0972) \n",
            "[4][279/393] Elapsed: 3m 1s (remain 1m 13s) Loss: 0.0813(0.0960) \n",
            "[4][299/393] Elapsed: 3m 13s (remain 1m 0s) Loss: 0.0656(0.0965) \n",
            "[4][319/393] Elapsed: 3m 26s (remain 0m 47s) Loss: 0.1319(0.0979) \n",
            "[4][339/393] Elapsed: 3m 39s (remain 0m 34s) Loss: 0.1095(0.0977) \n",
            "[4][359/393] Elapsed: 3m 52s (remain 0m 21s) Loss: 0.0684(0.0982) \n",
            "[4][379/393] Elapsed: 4m 5s (remain 0m 8s) Loss: 0.0460(0.0980) \n",
            "[4][393/393] Elapsed: 4m 14s (remain -1m 59s) Loss: 0.1327(0.0978) \n",
            "Epoch: 5 - avg_train_loss = 0.1114 avg_val_loss: 0.0978 time: 1853.8525s\n",
            "Epoch: 5 - Score: 0.4431, Scores: [0.4738576408543372, 0.43751247087825546, 0.418176621997103, 0.44383966013535486, 0.4594298609359638, 0.4256108358542924]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2301: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at microsoft/deberta-v3-large were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.classifier.weight', 'mask_predictions.dense.weight', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.bias', 'mask_predictions.LayerNorm.weight', 'mask_predictions.classifier.bias']\n",
            "- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "names:  ['.embeddings.']\n",
            "names:  ['encoder.LayerNorm', 'rel_embeddings']\n",
            "names:  ['.0.', '.1.', '.2.', '.3.', '.4.', '.5.']\n",
            "names:  ['.6.', '.7.', '.8.', '.9.', '.10.', '.11.']\n",
            "names:  ['.12.', '.13.', '.14.', '.15.', '.16.', '.17.', '.18.', '.19.', '.20.', '.21.', '.22.']\n",
            "names:  ['.23.']\n",
            "warmup start:  7815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0][19/1563] Elapsed: 0m 20s (remain 26m 39s) Loss: 2.3902(2.6684) Grad: 417293.1875 LR: 0.00000077 \n",
            "[0][39/1563] Elapsed: 0m 41s (remain 26m 18s) Loss: 2.2176(2.6371) Grad: 425452.375 LR: 0.00000154 \n",
            "[0][59/1563] Elapsed: 1m 1s (remain 25m 52s) Loss: 2.3879(2.5949) Grad: 420452.40625 LR: 0.00000230 \n",
            "[0][79/1563] Elapsed: 1m 22s (remain 25m 26s) Loss: 2.4751(2.5634) Grad: 415011.21875 LR: 0.00000307 \n",
            "[0][99/1563] Elapsed: 1m 42s (remain 25m 4s) Loss: 2.8357(2.5895) Grad: 420136.0625 LR: 0.00000384 \n",
            "[0][119/1563] Elapsed: 2m 3s (remain 24m 42s) Loss: 1.8722(2.5649) Grad: 420157.8125 LR: 0.00000461 \n",
            "[0][139/1563] Elapsed: 2m 23s (remain 24m 21s) Loss: 2.1325(2.5393) Grad: 419238.34375 LR: 0.00000537 \n",
            "[0][159/1563] Elapsed: 2m 44s (remain 24m 0s) Loss: 2.2890(2.5024) Grad: 426907.03125 LR: 0.00000614 \n",
            "[0][179/1563] Elapsed: 3m 4s (remain 23m 38s) Loss: 3.2267(2.4874) Grad: 442663.75 LR: 0.00000691 \n",
            "[0][199/1563] Elapsed: 3m 25s (remain 23m 18s) Loss: 2.0252(2.4412) Grad: 422285.21875 LR: 0.00000768 \n",
            "[0][219/1563] Elapsed: 3m 45s (remain 22m 56s) Loss: 1.3458(2.3808) Grad: 442303.6875 LR: 0.00000845 \n",
            "[0][239/1563] Elapsed: 4m 5s (remain 22m 35s) Loss: 1.2975(2.3207) Grad: 451864.375 LR: 0.00000921 \n",
            "[0][259/1563] Elapsed: 4m 26s (remain 22m 15s) Loss: 1.2490(2.2584) Grad: 460457.25 LR: 0.00000998 \n",
            "[0][279/1563] Elapsed: 4m 46s (remain 21m 54s) Loss: 1.5181(2.1940) Grad: 482794.90625 LR: 0.00001075 \n",
            "[0][299/1563] Elapsed: 5m 7s (remain 21m 34s) Loss: 0.7624(2.1263) Grad: 417948.625 LR: 0.00001152 \n",
            "[0][319/1563] Elapsed: 5m 27s (remain 21m 13s) Loss: 1.1375(2.0602) Grad: 501532.40625 LR: 0.00001228 \n",
            "[0][339/1563] Elapsed: 5m 48s (remain 20m 52s) Loss: 0.4566(1.9826) Grad: 447544.0625 LR: 0.00001305 \n",
            "[0][359/1563] Elapsed: 6m 8s (remain 20m 32s) Loss: 0.9658(1.9131) Grad: 489144.6875 LR: 0.00001382 \n",
            "[0][379/1563] Elapsed: 6m 29s (remain 20m 11s) Loss: 0.5895(1.8390) Grad: 515063.46875 LR: 0.00001459 \n",
            "[0][399/1563] Elapsed: 6m 49s (remain 19m 51s) Loss: 0.5307(1.7633) Grad: 536052.25 LR: 0.00001536 \n",
            "[0][419/1563] Elapsed: 7m 10s (remain 19m 30s) Loss: 0.1766(1.6902) Grad: 303218.96875 LR: 0.00001612 \n",
            "[0][439/1563] Elapsed: 7m 30s (remain 19m 9s) Loss: 0.3449(1.6242) Grad: 214246.609375 LR: 0.00001689 \n",
            "[0][459/1563] Elapsed: 7m 51s (remain 18m 49s) Loss: 0.1913(1.5633) Grad: 331303.0 LR: 0.00001766 \n",
            "[0][479/1563] Elapsed: 8m 11s (remain 18m 28s) Loss: 0.0394(1.5064) Grad: 91354.21875 LR: 0.00001843 \n",
            "[0][499/1563] Elapsed: 8m 31s (remain 18m 8s) Loss: 0.4194(1.4526) Grad: 355358.25 LR: 0.00001919 \n",
            "[0][519/1563] Elapsed: 8m 52s (remain 17m 47s) Loss: 0.0867(1.4043) Grad: 124364.2265625 LR: 0.00001996 \n",
            "[0][539/1563] Elapsed: 9m 12s (remain 17m 27s) Loss: 0.1321(1.3595) Grad: 186227.59375 LR: 0.00002073 \n",
            "[0][559/1563] Elapsed: 9m 33s (remain 17m 6s) Loss: 0.1583(1.3163) Grad: 308185.65625 LR: 0.00002150 \n",
            "[0][579/1563] Elapsed: 9m 53s (remain 16m 46s) Loss: 0.2003(1.2772) Grad: 199075.71875 LR: 0.00002226 \n",
            "[0][599/1563] Elapsed: 10m 14s (remain 16m 25s) Loss: 0.2061(1.2413) Grad: 400512.1875 LR: 0.00002303 \n",
            "[0][619/1563] Elapsed: 10m 34s (remain 16m 5s) Loss: 0.2467(1.2076) Grad: 231439.359375 LR: 0.00002380 \n",
            "[0][639/1563] Elapsed: 10m 55s (remain 15m 44s) Loss: 0.0872(1.1760) Grad: 194752.875 LR: 0.00002457 \n",
            "[0][659/1563] Elapsed: 11m 15s (remain 15m 24s) Loss: 0.2471(1.1463) Grad: 161009.578125 LR: 0.00002534 \n",
            "[0][679/1563] Elapsed: 11m 35s (remain 15m 3s) Loss: 0.1830(1.1192) Grad: 213217.359375 LR: 0.00002610 \n",
            "[0][699/1563] Elapsed: 11m 56s (remain 14m 43s) Loss: 0.2680(1.0920) Grad: 416360.375 LR: 0.00002687 \n",
            "[0][719/1563] Elapsed: 12m 16s (remain 14m 22s) Loss: 0.1173(1.0662) Grad: 179089.65625 LR: 0.00002764 \n",
            "[0][739/1563] Elapsed: 12m 37s (remain 14m 2s) Loss: 0.0320(1.0429) Grad: 108788.90625 LR: 0.00002841 \n",
            "[0][759/1563] Elapsed: 12m 57s (remain 13m 41s) Loss: 0.1848(1.0198) Grad: 182704.296875 LR: 0.00002917 \n",
            "[0][779/1563] Elapsed: 13m 18s (remain 13m 21s) Loss: 0.2325(0.9967) Grad: 303327.125 LR: 0.00002994 \n",
            "[0][799/1563] Elapsed: 13m 38s (remain 13m 0s) Loss: 0.1878(0.9758) Grad: 225829.859375 LR: 0.00003071 \n",
            "[0][819/1563] Elapsed: 13m 59s (remain 12m 40s) Loss: 0.0941(0.9560) Grad: 100592.953125 LR: 0.00003148 \n",
            "[0][839/1563] Elapsed: 14m 19s (remain 12m 19s) Loss: 0.1165(0.9369) Grad: 245969.9375 LR: 0.00003225 \n",
            "[0][859/1563] Elapsed: 14m 40s (remain 11m 59s) Loss: 0.1591(0.9194) Grad: 177907.921875 LR: 0.00003301 \n",
            "[0][879/1563] Elapsed: 15m 0s (remain 11m 38s) Loss: 0.0947(0.9023) Grad: 164448.421875 LR: 0.00003378 \n",
            "[0][899/1563] Elapsed: 15m 21s (remain 11m 18s) Loss: 0.0672(0.8856) Grad: 113992.65625 LR: 0.00003455 \n",
            "[0][919/1563] Elapsed: 15m 41s (remain 10m 57s) Loss: 0.1050(0.8690) Grad: 137015.0 LR: 0.00003532 \n",
            "[0][939/1563] Elapsed: 16m 1s (remain 10m 37s) Loss: 0.1293(0.8539) Grad: 257865.015625 LR: 0.00003608 \n",
            "[0][959/1563] Elapsed: 16m 22s (remain 10m 17s) Loss: 0.2898(0.8398) Grad: 257051.515625 LR: 0.00003685 \n",
            "[0][979/1563] Elapsed: 16m 43s (remain 9m 56s) Loss: 0.1414(0.8249) Grad: 154172.640625 LR: 0.00003762 \n",
            "[0][999/1563] Elapsed: 17m 3s (remain 9m 36s) Loss: 0.0936(0.8108) Grad: 139424.28125 LR: 0.00003839 \n",
            "[0][1019/1563] Elapsed: 17m 24s (remain 9m 15s) Loss: 0.0538(0.7976) Grad: 178164.21875 LR: 0.00003916 \n",
            "[0][1039/1563] Elapsed: 17m 44s (remain 8m 55s) Loss: 0.0448(0.7853) Grad: 124379.4765625 LR: 0.00003992 \n",
            "[0][1059/1563] Elapsed: 18m 5s (remain 8m 34s) Loss: 0.0971(0.7730) Grad: 177340.140625 LR: 0.00004069 \n",
            "[0][1079/1563] Elapsed: 18m 25s (remain 8m 14s) Loss: 0.2079(0.7617) Grad: 310372.09375 LR: 0.00004146 \n",
            "[0][1099/1563] Elapsed: 18m 45s (remain 7m 53s) Loss: 0.2106(0.7503) Grad: 438373.0625 LR: 0.00004223 \n",
            "[0][1119/1563] Elapsed: 19m 6s (remain 7m 33s) Loss: 0.1765(0.7391) Grad: 197462.453125 LR: 0.00004299 \n",
            "[0][1139/1563] Elapsed: 19m 26s (remain 7m 12s) Loss: 0.1819(0.7284) Grad: 305758.4375 LR: 0.00004376 \n",
            "[0][1159/1563] Elapsed: 19m 47s (remain 6m 52s) Loss: 0.0884(0.7180) Grad: 274700.4375 LR: 0.00004453 \n",
            "[0][1179/1563] Elapsed: 20m 7s (remain 6m 32s) Loss: 0.1236(0.7087) Grad: 213678.765625 LR: 0.00004530 \n",
            "[0][1199/1563] Elapsed: 20m 28s (remain 6m 11s) Loss: 0.1278(0.6995) Grad: 131947.296875 LR: 0.00004607 \n",
            "[0][1219/1563] Elapsed: 20m 48s (remain 5m 51s) Loss: 0.0344(0.6909) Grad: 88029.390625 LR: 0.00004683 \n",
            "[0][1239/1563] Elapsed: 21m 9s (remain 5m 30s) Loss: 0.1020(0.6814) Grad: 224095.59375 LR: 0.00004760 \n",
            "[0][1259/1563] Elapsed: 21m 29s (remain 5m 10s) Loss: 0.0665(0.6722) Grad: 170606.71875 LR: 0.00004837 \n",
            "[0][1279/1563] Elapsed: 21m 49s (remain 4m 49s) Loss: 0.1198(0.6636) Grad: 341654.15625 LR: 0.00004914 \n",
            "[0][1299/1563] Elapsed: 22m 10s (remain 4m 29s) Loss: 0.0778(0.6557) Grad: 224829.78125 LR: 0.00004990 \n",
            "[0][1319/1563] Elapsed: 22m 30s (remain 4m 8s) Loss: 0.1004(0.6474) Grad: 264872.375 LR: 0.00005067 \n",
            "[0][1339/1563] Elapsed: 22m 51s (remain 3m 48s) Loss: 0.1327(0.6392) Grad: 213826.015625 LR: 0.00005144 \n",
            "[0][1359/1563] Elapsed: 23m 11s (remain 3m 27s) Loss: 0.1745(0.6319) Grad: 307266.5 LR: 0.00005221 \n",
            "[0][1379/1563] Elapsed: 23m 32s (remain 3m 7s) Loss: 0.0972(0.6245) Grad: 278373.21875 LR: 0.00005298 \n",
            "[0][1399/1563] Elapsed: 23m 52s (remain 2m 46s) Loss: 0.0591(0.6175) Grad: 216866.109375 LR: 0.00005374 \n",
            "[0][1419/1563] Elapsed: 24m 13s (remain 2m 26s) Loss: 0.1202(0.6106) Grad: 210143.03125 LR: 0.00005451 \n",
            "[0][1439/1563] Elapsed: 24m 33s (remain 2m 5s) Loss: 0.0898(0.6039) Grad: 176281.390625 LR: 0.00005528 \n",
            "[0][1459/1563] Elapsed: 24m 54s (remain 1m 45s) Loss: 0.1253(0.5971) Grad: 176362.84375 LR: 0.00005605 \n",
            "[0][1479/1563] Elapsed: 25m 14s (remain 1m 24s) Loss: 0.2478(0.5906) Grad: 523784.875 LR: 0.00005681 \n",
            "[0][1499/1563] Elapsed: 25m 35s (remain 1m 4s) Loss: 0.3377(0.5844) Grad: 463900.96875 LR: 0.00005758 \n",
            "[0][1519/1563] Elapsed: 25m 55s (remain 0m 44s) Loss: 0.1195(0.5787) Grad: 231895.578125 LR: 0.00005835 \n",
            "[0][1539/1563] Elapsed: 26m 16s (remain 0m 23s) Loss: 0.0763(0.5725) Grad: 271806.46875 LR: 0.00005912 \n",
            "[0][1559/1563] Elapsed: 26m 36s (remain 0m 3s) Loss: 0.0361(0.5669) Grad: 75646.796875 LR: 0.00005988 \n",
            "[0][19/392] Elapsed: 0m 13s (remain 4m 4s) Loss: 0.0990(0.1052) \n",
            "[0][39/392] Elapsed: 0m 26s (remain 3m 48s) Loss: 0.1784(0.1204) \n",
            "[0][59/392] Elapsed: 0m 38s (remain 3m 35s) Loss: 0.0884(0.1133) \n",
            "[0][79/392] Elapsed: 0m 51s (remain 3m 21s) Loss: 0.1373(0.1220) \n",
            "[0][99/392] Elapsed: 1m 4s (remain 3m 8s) Loss: 0.0652(0.1186) \n",
            "[0][119/392] Elapsed: 1m 17s (remain 2m 55s) Loss: 0.1153(0.1199) \n",
            "[0][139/392] Elapsed: 1m 30s (remain 2m 42s) Loss: 0.0562(0.1238) \n",
            "[0][159/392] Elapsed: 1m 43s (remain 2m 29s) Loss: 0.1630(0.1189) \n",
            "[0][179/392] Elapsed: 1m 56s (remain 2m 16s) Loss: 0.0975(0.1208) \n",
            "[0][199/392] Elapsed: 2m 9s (remain 2m 3s) Loss: 0.0911(0.1200) \n",
            "[0][219/392] Elapsed: 2m 21s (remain 1m 50s) Loss: 0.2477(0.1219) \n",
            "[0][239/392] Elapsed: 2m 34s (remain 1m 38s) Loss: 0.1008(0.1221) \n",
            "[0][259/392] Elapsed: 2m 47s (remain 1m 25s) Loss: 0.0532(0.1220) \n",
            "[0][279/392] Elapsed: 3m 0s (remain 1m 12s) Loss: 0.2004(0.1216) \n",
            "[0][299/392] Elapsed: 3m 13s (remain 0m 59s) Loss: 0.0735(0.1213) \n",
            "[0][319/392] Elapsed: 3m 26s (remain 0m 46s) Loss: 0.0755(0.1204) \n",
            "[0][339/392] Elapsed: 3m 39s (remain 0m 33s) Loss: 0.0548(0.1206) \n",
            "[0][359/392] Elapsed: 3m 51s (remain 0m 20s) Loss: 0.0993(0.1203) \n",
            "[0][379/392] Elapsed: 4m 4s (remain 0m 7s) Loss: 0.0925(0.1207) \n",
            "[0][392/392] Elapsed: 4m 13s (remain -1m 59s) Loss: 0.1737(0.1208) \n",
            "Epoch: 1 - avg_train_loss = 0.5660 avg_val_loss: 0.1208 time: 1853.0750s\n",
            "Epoch: 1 - Score: 0.4928, Scores: [0.5409139493286217, 0.48173513309639965, 0.45241538093728295, 0.5092670290147505, 0.4758172244013826, 0.49651424276201905]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1][19/1563] Elapsed: 0m 20s (remain 26m 40s) Loss: 0.1015(0.1312) Grad: 304177.4375 LR: 0.00006077 \n",
            "[1][39/1563] Elapsed: 0m 41s (remain 26m 15s) Loss: 0.1321(0.1303) Grad: 282355.78125 LR: 0.00006154 \n",
            "[1][59/1563] Elapsed: 1m 1s (remain 25m 51s) Loss: 0.1165(0.1239) Grad: 165624.734375 LR: 0.00006230 \n",
            "[1][79/1563] Elapsed: 1m 22s (remain 25m 26s) Loss: 0.2344(0.1251) Grad: 500392.84375 LR: 0.00006307 \n",
            "[1][99/1563] Elapsed: 1m 42s (remain 25m 4s) Loss: 0.1202(0.1224) Grad: 173147.0 LR: 0.00006384 \n",
            "[1][119/1563] Elapsed: 2m 3s (remain 24m 43s) Loss: 0.0626(0.1236) Grad: 210051.03125 LR: 0.00006461 \n",
            "[1][139/1563] Elapsed: 2m 23s (remain 24m 21s) Loss: 0.1253(0.1234) Grad: 289981.625 LR: 0.00006537 \n",
            "[1][159/1563] Elapsed: 2m 44s (remain 24m 0s) Loss: 0.1231(0.1246) Grad: 331749.71875 LR: 0.00006614 \n",
            "[1][179/1563] Elapsed: 3m 4s (remain 23m 39s) Loss: 0.1693(0.1241) Grad: 500883.0 LR: 0.00006691 \n",
            "[1][199/1563] Elapsed: 3m 25s (remain 23m 18s) Loss: 0.1509(0.1227) Grad: 446165.0625 LR: 0.00006768 \n",
            "[1][219/1563] Elapsed: 3m 45s (remain 22m 57s) Loss: 0.0714(0.1213) Grad: 145252.90625 LR: 0.00006845 \n",
            "[1][239/1563] Elapsed: 4m 6s (remain 22m 36s) Loss: 0.0780(0.1199) Grad: 274254.90625 LR: 0.00006921 \n",
            "[1][259/1563] Elapsed: 4m 26s (remain 22m 16s) Loss: 0.1564(0.1210) Grad: 252876.0 LR: 0.00006998 \n",
            "[1][279/1563] Elapsed: 4m 47s (remain 21m 55s) Loss: 0.0614(0.1194) Grad: 165244.59375 LR: 0.00007075 \n",
            "[1][299/1563] Elapsed: 5m 7s (remain 21m 34s) Loss: 0.1041(0.1182) Grad: 335734.75 LR: 0.00007152 \n",
            "[1][319/1563] Elapsed: 5m 28s (remain 21m 14s) Loss: 0.0290(0.1175) Grad: 94845.0703125 LR: 0.00007228 \n",
            "[1][339/1563] Elapsed: 5m 48s (remain 20m 53s) Loss: 0.0689(0.1166) Grad: 224182.125 LR: 0.00007305 \n",
            "[1][359/1563] Elapsed: 6m 9s (remain 20m 33s) Loss: 0.0540(0.1160) Grad: 164006.0625 LR: 0.00007382 \n",
            "[1][379/1563] Elapsed: 6m 29s (remain 20m 12s) Loss: 0.0842(0.1163) Grad: 161957.96875 LR: 0.00007459 \n",
            "[1][399/1563] Elapsed: 6m 49s (remain 19m 52s) Loss: 0.0984(0.1163) Grad: 206794.015625 LR: 0.00007536 \n",
            "[1][419/1563] Elapsed: 7m 10s (remain 19m 31s) Loss: 0.0838(0.1163) Grad: 277374.21875 LR: 0.00007612 \n",
            "[1][439/1563] Elapsed: 7m 30s (remain 19m 10s) Loss: 0.2501(0.1175) Grad: 435675.9375 LR: 0.00007689 \n",
            "[1][459/1563] Elapsed: 7m 51s (remain 18m 50s) Loss: 0.0789(0.1175) Grad: 279993.3125 LR: 0.00007766 \n",
            "[1][479/1563] Elapsed: 8m 11s (remain 18m 29s) Loss: 0.0823(0.1174) Grad: 243029.765625 LR: 0.00007843 \n",
            "[1][499/1563] Elapsed: 8m 32s (remain 18m 9s) Loss: 0.0649(0.1167) Grad: 130910.6171875 LR: 0.00007919 \n",
            "[1][519/1563] Elapsed: 8m 52s (remain 17m 48s) Loss: 0.1953(0.1171) Grad: 450165.03125 LR: 0.00007996 \n",
            "[1][539/1563] Elapsed: 9m 13s (remain 17m 28s) Loss: 0.1281(0.1173) Grad: 500093.03125 LR: 0.00008073 \n",
            "[1][559/1563] Elapsed: 9m 33s (remain 17m 7s) Loss: 0.1015(0.1172) Grad: 327725.46875 LR: 0.00008150 \n",
            "[1][579/1563] Elapsed: 9m 54s (remain 16m 47s) Loss: 0.0786(0.1173) Grad: 335811.40625 LR: 0.00008226 \n",
            "[1][599/1563] Elapsed: 10m 14s (remain 16m 26s) Loss: 0.1184(0.1169) Grad: 169896.65625 LR: 0.00008303 \n",
            "[1][619/1563] Elapsed: 10m 35s (remain 16m 5s) Loss: 0.0805(0.1169) Grad: 197886.609375 LR: 0.00008380 \n",
            "[1][639/1563] Elapsed: 10m 55s (remain 15m 45s) Loss: 0.0792(0.1160) Grad: 174210.640625 LR: 0.00008457 \n",
            "[1][659/1563] Elapsed: 11m 15s (remain 15m 24s) Loss: 0.0321(0.1157) Grad: 88921.015625 LR: 0.00008534 \n",
            "[1][679/1563] Elapsed: 11m 36s (remain 15m 4s) Loss: 0.0414(0.1157) Grad: 153836.21875 LR: 0.00008610 \n",
            "[1][699/1563] Elapsed: 11m 56s (remain 14m 43s) Loss: 0.1409(0.1158) Grad: 474334.84375 LR: 0.00008687 \n",
            "[1][719/1563] Elapsed: 12m 17s (remain 14m 23s) Loss: 0.1310(0.1164) Grad: 488385.125 LR: 0.00008764 \n",
            "[1][739/1563] Elapsed: 12m 37s (remain 14m 2s) Loss: 0.1094(0.1161) Grad: 300142.84375 LR: 0.00008841 \n",
            "[1][759/1563] Elapsed: 12m 58s (remain 13m 42s) Loss: 0.0660(0.1159) Grad: 134849.25 LR: 0.00008917 \n",
            "[1][779/1563] Elapsed: 13m 18s (remain 13m 21s) Loss: 0.1002(0.1157) Grad: 252980.78125 LR: 0.00008994 \n",
            "[1][799/1563] Elapsed: 13m 39s (remain 13m 1s) Loss: 0.2012(0.1157) Grad: 273499.09375 LR: 0.00009071 \n",
            "[1][819/1563] Elapsed: 13m 59s (remain 12m 40s) Loss: 0.1023(0.1159) Grad: 419603.90625 LR: 0.00009148 \n",
            "[1][839/1563] Elapsed: 14m 20s (remain 12m 20s) Loss: 0.1055(0.1159) Grad: 410050.125 LR: 0.00009225 \n",
            "[1][859/1563] Elapsed: 14m 40s (remain 11m 59s) Loss: 0.1290(0.1156) Grad: 243873.515625 LR: 0.00009301 \n",
            "[1][879/1563] Elapsed: 15m 1s (remain 11m 39s) Loss: 0.1083(0.1153) Grad: 268394.09375 LR: 0.00009378 \n",
            "[1][899/1563] Elapsed: 15m 21s (remain 11m 18s) Loss: 0.0660(0.1153) Grad: 283828.3125 LR: 0.00009455 \n",
            "[1][919/1563] Elapsed: 15m 42s (remain 10m 58s) Loss: 0.1507(0.1151) Grad: 205548.953125 LR: 0.00009532 \n",
            "[1][939/1563] Elapsed: 16m 2s (remain 10m 37s) Loss: 0.3530(0.1152) Grad: 705554.75 LR: 0.00009608 \n",
            "[1][959/1563] Elapsed: 16m 22s (remain 10m 17s) Loss: 0.1248(0.1152) Grad: 164406.75 LR: 0.00009685 \n",
            "[1][979/1563] Elapsed: 16m 43s (remain 9m 56s) Loss: 0.1881(0.1157) Grad: 450464.71875 LR: 0.00009762 \n",
            "[1][999/1563] Elapsed: 17m 3s (remain 9m 36s) Loss: 0.0468(0.1156) Grad: 137794.859375 LR: 0.00009839 \n",
            "[1][1019/1563] Elapsed: 17m 24s (remain 9m 15s) Loss: 0.1765(0.1159) Grad: 475741.875 LR: 0.00009916 \n",
            "[1][1039/1563] Elapsed: 17m 44s (remain 8m 55s) Loss: 0.1630(0.1162) Grad: 396158.5625 LR: 0.00009992 \n",
            "[1][1059/1563] Elapsed: 18m 5s (remain 8m 34s) Loss: 0.0291(0.1160) Grad: 118318.75 LR: 0.00010069 \n",
            "[1][1079/1563] Elapsed: 18m 25s (remain 8m 14s) Loss: 0.1314(0.1159) Grad: 370378.46875 LR: 0.00010146 \n",
            "[1][1099/1563] Elapsed: 18m 46s (remain 7m 54s) Loss: 0.1104(0.1160) Grad: 399951.46875 LR: 0.00010223 \n",
            "[1][1119/1563] Elapsed: 19m 6s (remain 7m 33s) Loss: 0.1142(0.1159) Grad: 253020.03125 LR: 0.00010299 \n",
            "[1][1139/1563] Elapsed: 19m 27s (remain 7m 13s) Loss: 0.0490(0.1153) Grad: 189965.671875 LR: 0.00010376 \n",
            "[1][1159/1563] Elapsed: 19m 47s (remain 6m 52s) Loss: 0.1319(0.1152) Grad: 565954.1875 LR: 0.00010453 \n",
            "[1][1179/1563] Elapsed: 20m 8s (remain 6m 32s) Loss: 0.0706(0.1150) Grad: 136526.9375 LR: 0.00010530 \n",
            "[1][1199/1563] Elapsed: 20m 28s (remain 6m 11s) Loss: 0.0724(0.1148) Grad: 346649.875 LR: 0.00010607 \n",
            "[1][1219/1563] Elapsed: 20m 49s (remain 5m 51s) Loss: 0.0709(0.1147) Grad: 164240.09375 LR: 0.00010683 \n",
            "[1][1239/1563] Elapsed: 21m 9s (remain 5m 30s) Loss: 0.1729(0.1154) Grad: 601761.3125 LR: 0.00010760 \n",
            "[1][1259/1563] Elapsed: 21m 29s (remain 5m 10s) Loss: 0.1719(0.1158) Grad: 219563.8125 LR: 0.00010837 \n",
            "[1][1279/1563] Elapsed: 21m 50s (remain 4m 49s) Loss: 0.1380(0.1158) Grad: 391437.90625 LR: 0.00010914 \n",
            "[1][1299/1563] Elapsed: 22m 10s (remain 4m 29s) Loss: 0.0800(0.1152) Grad: 232042.65625 LR: 0.00010990 \n",
            "[1][1319/1563] Elapsed: 22m 31s (remain 4m 8s) Loss: 0.0818(0.1154) Grad: 166986.328125 LR: 0.00011067 \n",
            "[1][1339/1563] Elapsed: 22m 51s (remain 3m 48s) Loss: 0.1954(0.1153) Grad: 638453.9375 LR: 0.00011144 \n",
            "[1][1359/1563] Elapsed: 23m 12s (remain 3m 27s) Loss: 0.0909(0.1152) Grad: 195678.828125 LR: 0.00011221 \n",
            "[1][1379/1563] Elapsed: 23m 32s (remain 3m 7s) Loss: 0.0780(0.1149) Grad: 127600.6875 LR: 0.00011298 \n",
            "[1][1399/1563] Elapsed: 23m 53s (remain 2m 46s) Loss: 0.0865(0.1147) Grad: 124831.3046875 LR: 0.00011374 \n",
            "[1][1419/1563] Elapsed: 24m 13s (remain 2m 26s) Loss: 0.2061(0.1146) Grad: 542322.0 LR: 0.00011451 \n",
            "[1][1439/1563] Elapsed: 24m 34s (remain 2m 5s) Loss: 0.1933(0.1151) Grad: 593447.5 LR: 0.00011528 \n",
            "[1][1459/1563] Elapsed: 24m 54s (remain 1m 45s) Loss: 0.0863(0.1151) Grad: 294738.40625 LR: 0.00011605 \n",
            "[1][1479/1563] Elapsed: 25m 15s (remain 1m 24s) Loss: 0.0454(0.1149) Grad: 226430.375 LR: 0.00011681 \n",
            "[1][1499/1563] Elapsed: 25m 35s (remain 1m 4s) Loss: 0.1613(0.1150) Grad: 429493.875 LR: 0.00011758 \n",
            "[1][1519/1563] Elapsed: 25m 56s (remain 0m 44s) Loss: 0.1265(0.1150) Grad: 431240.46875 LR: 0.00011835 \n",
            "[1][1539/1563] Elapsed: 26m 16s (remain 0m 23s) Loss: 0.1469(0.1152) Grad: 417437.3125 LR: 0.00011912 \n",
            "[1][1559/1563] Elapsed: 26m 36s (remain 0m 3s) Loss: 0.0745(0.1152) Grad: 134732.59375 LR: 0.00011988 \n",
            "[1][19/392] Elapsed: 0m 13s (remain 4m 5s) Loss: 0.1240(0.1120) \n",
            "[1][39/392] Elapsed: 0m 26s (remain 3m 49s) Loss: 0.1785(0.1235) \n",
            "[1][59/392] Elapsed: 0m 39s (remain 3m 36s) Loss: 0.1032(0.1189) \n",
            "[1][79/392] Elapsed: 0m 51s (remain 3m 22s) Loss: 0.1581(0.1294) \n",
            "[1][99/392] Elapsed: 1m 4s (remain 3m 9s) Loss: 0.0553(0.1257) \n",
            "[1][119/392] Elapsed: 1m 17s (remain 2m 56s) Loss: 0.1082(0.1245) \n",
            "[1][139/392] Elapsed: 1m 30s (remain 2m 43s) Loss: 0.0710(0.1266) \n",
            "[1][159/392] Elapsed: 1m 43s (remain 2m 30s) Loss: 0.2203(0.1226) \n",
            "[1][179/392] Elapsed: 1m 56s (remain 2m 17s) Loss: 0.0905(0.1259) \n",
            "[1][199/392] Elapsed: 2m 9s (remain 2m 4s) Loss: 0.1476(0.1252) \n",
            "[1][219/392] Elapsed: 2m 22s (remain 1m 51s) Loss: 0.2633(0.1272) \n",
            "[1][239/392] Elapsed: 2m 35s (remain 1m 38s) Loss: 0.1389(0.1275) \n",
            "[1][259/392] Elapsed: 2m 48s (remain 1m 25s) Loss: 0.0678(0.1281) \n",
            "[1][279/392] Elapsed: 3m 1s (remain 1m 12s) Loss: 0.1521(0.1284) \n",
            "[1][299/392] Elapsed: 3m 13s (remain 0m 59s) Loss: 0.1061(0.1280) \n",
            "[1][319/392] Elapsed: 3m 26s (remain 0m 46s) Loss: 0.0588(0.1275) \n",
            "[1][339/392] Elapsed: 3m 39s (remain 0m 33s) Loss: 0.0569(0.1274) \n",
            "[1][359/392] Elapsed: 3m 52s (remain 0m 20s) Loss: 0.1094(0.1271) \n",
            "[1][379/392] Elapsed: 4m 5s (remain 0m 7s) Loss: 0.0819(0.1280) \n",
            "[1][392/392] Elapsed: 4m 13s (remain -1m 59s) Loss: 0.0941(0.1279) \n",
            "Epoch: 2 - avg_train_loss = 0.1151 avg_val_loss: 0.1279 time: 1854.0397s\n",
            "Epoch: 2 - Score: 0.5056, Scores: [0.6198438623219978, 0.45837453904007913, 0.45967140591604966, 0.4815267400735743, 0.48984791401121636, 0.5244597190373305]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:249: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2][19/1563] Elapsed: 0m 20s (remain 26m 33s) Loss: 0.0805(0.1308) Grad: 244140.953125 LR: 0.00012077 \n",
            "[2][39/1563] Elapsed: 0m 41s (remain 26m 5s) Loss: 0.1591(0.1211) Grad: 306919.90625 LR: 0.00012154 \n",
            "[2][59/1563] Elapsed: 1m 1s (remain 25m 42s) Loss: 0.2723(0.1227) Grad: 621501.125 LR: 0.00012230 \n",
            "[2][79/1563] Elapsed: 1m 22s (remain 25m 20s) Loss: 0.0687(0.1246) Grad: 137161.484375 LR: 0.00012307 \n",
            "[2][99/1563] Elapsed: 1m 42s (remain 24m 59s) Loss: 0.0655(0.1241) Grad: 191670.859375 LR: 0.00012384 \n",
            "[2][119/1563] Elapsed: 2m 2s (remain 24m 38s) Loss: 0.1266(0.1235) Grad: 352398.90625 LR: 0.00012461 \n",
            "[2][139/1563] Elapsed: 2m 23s (remain 24m 17s) Loss: 0.0931(0.1215) Grad: 329270.84375 LR: 0.00012537 \n",
            "[2][159/1563] Elapsed: 2m 43s (remain 23m 56s) Loss: 0.1328(0.1218) Grad: 432949.125 LR: 0.00012614 \n",
            "[2][179/1563] Elapsed: 3m 4s (remain 23m 35s) Loss: 0.1233(0.1200) Grad: 204926.59375 LR: 0.00012691 \n",
            "[2][199/1563] Elapsed: 3m 24s (remain 23m 15s) Loss: 0.0635(0.1182) Grad: 184977.578125 LR: 0.00012768 \n",
            "[2][219/1563] Elapsed: 3m 45s (remain 22m 54s) Loss: 0.1387(0.1161) Grad: 266838.1875 LR: 0.00012845 \n",
            "[2][239/1563] Elapsed: 4m 5s (remain 22m 34s) Loss: 0.0612(0.1159) Grad: 142421.359375 LR: 0.00012921 \n",
            "[2][259/1563] Elapsed: 4m 26s (remain 22m 14s) Loss: 0.0603(0.1139) Grad: 188672.40625 LR: 0.00012998 \n",
            "[2][279/1563] Elapsed: 4m 46s (remain 21m 53s) Loss: 0.0576(0.1140) Grad: 282074.0 LR: 0.00013075 \n",
            "[2][299/1563] Elapsed: 5m 7s (remain 21m 33s) Loss: 0.0781(0.1129) Grad: 182768.734375 LR: 0.00013152 \n",
            "[2][319/1563] Elapsed: 5m 27s (remain 21m 12s) Loss: 0.0892(0.1134) Grad: 248054.296875 LR: 0.00013228 \n",
            "[2][339/1563] Elapsed: 5m 48s (remain 20m 52s) Loss: 0.1512(0.1147) Grad: 285355.6875 LR: 0.00013305 \n",
            "[2][359/1563] Elapsed: 6m 8s (remain 20m 31s) Loss: 0.0876(0.1142) Grad: 205875.171875 LR: 0.00013382 \n",
            "[2][379/1563] Elapsed: 6m 29s (remain 20m 11s) Loss: 0.0403(0.1135) Grad: 118474.421875 LR: 0.00013459 \n",
            "[2][399/1563] Elapsed: 6m 49s (remain 19m 50s) Loss: 0.2080(0.1134) Grad: 601253.0 LR: 0.00013536 \n",
            "[2][419/1563] Elapsed: 7m 10s (remain 19m 30s) Loss: 0.1223(0.1141) Grad: 386268.5625 LR: 0.00013612 \n",
            "[2][439/1563] Elapsed: 7m 30s (remain 19m 9s) Loss: 0.1523(0.1139) Grad: 201083.15625 LR: 0.00013689 \n",
            "[2][459/1563] Elapsed: 7m 51s (remain 18m 49s) Loss: 0.0480(0.1141) Grad: 130053.2578125 LR: 0.00013766 \n",
            "[2][479/1563] Elapsed: 8m 11s (remain 18m 28s) Loss: 0.0882(0.1140) Grad: 279912.5625 LR: 0.00013843 \n",
            "[2][499/1563] Elapsed: 8m 31s (remain 18m 8s) Loss: 0.0506(0.1140) Grad: 193278.40625 LR: 0.00013919 \n",
            "[2][519/1563] Elapsed: 8m 52s (remain 17m 47s) Loss: 0.1424(0.1135) Grad: 382681.96875 LR: 0.00013996 \n",
            "[2][539/1563] Elapsed: 9m 12s (remain 17m 27s) Loss: 0.1162(0.1132) Grad: 313046.90625 LR: 0.00014073 \n",
            "[2][559/1563] Elapsed: 9m 33s (remain 17m 7s) Loss: 0.1801(0.1134) Grad: 441139.125 LR: 0.00014150 \n",
            "[2][579/1563] Elapsed: 9m 53s (remain 16m 46s) Loss: 0.0965(0.1128) Grad: 187927.078125 LR: 0.00014226 \n",
            "[2][599/1563] Elapsed: 10m 14s (remain 16m 26s) Loss: 0.0596(0.1139) Grad: 203660.390625 LR: 0.00014303 \n",
            "[2][619/1563] Elapsed: 10m 34s (remain 16m 5s) Loss: 0.0947(0.1139) Grad: 283254.03125 LR: 0.00014380 \n",
            "[2][639/1563] Elapsed: 10m 55s (remain 15m 44s) Loss: 0.0536(0.1138) Grad: 136596.125 LR: 0.00014457 \n",
            "[2][659/1563] Elapsed: 11m 15s (remain 15m 24s) Loss: 0.0597(0.1142) Grad: 153826.828125 LR: 0.00014534 \n",
            "[2][679/1563] Elapsed: 11m 36s (remain 15m 3s) Loss: 0.1042(0.1140) Grad: 299722.375 LR: 0.00014610 \n",
            "[2][699/1563] Elapsed: 11m 56s (remain 14m 43s) Loss: 0.1298(0.1137) Grad: 386697.84375 LR: 0.00014687 \n",
            "[2][719/1563] Elapsed: 12m 17s (remain 14m 23s) Loss: 0.0473(0.1137) Grad: 177200.46875 LR: 0.00014764 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d0lWOzgsVHnP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}