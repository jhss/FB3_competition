{"cells":[{"cell_type":"markdown","source":["Reference: https://www.kaggle.com/code/kojimar/fb3-single-pytorch-model-train"],"metadata":{"id":"KHIseMig86R1"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","%cd drive/MyDrive/kaggle/notebook"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nzta-USbvri1","executionInfo":{"status":"ok","timestamp":1669533050181,"user_tz":-540,"elapsed":29080,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"}},"outputId":"231a8bfc-af76-4a97-de8b-f2e21ed6adca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/kaggle/notebook\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dAPPw2PODeXy"},"outputs":[],"source":["%%capture\n","!pip install transformers==4.20.1\n","!pip install tokenizers==0.12.1\n","!pip install sentencepiece==0.1.97"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5041,"status":"ok","timestamp":1669533072955,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"},"user_tz":-540},"id":"Wj84kHj_sE41","outputId":"12bec91f-bdce-48dc-c0c3-4267bddd2b9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["transformers.__version__: 4.20.1\n","tokenizers.__version__: 0.12.1\n"]}],"source":["import os\n","import gc\n","import sys\n","\n","import time\n","import datetime\n","import math\n","import random\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from sklearn.metrics import mean_squared_error\n","\n","import torch\n","import torch.nn as nn\n","from torch.nn import Parameter\n","import torch.nn.functional as F\n","from torch.optim import AdamW\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.checkpoint import checkpoint\n","\n","import transformers\n","import tokenizers\n","print(f'transformers.__version__: {transformers.__version__}')\n","print(f'tokenizers.__version__: {tokenizers.__version__}')\n","from transformers import AutoTokenizer, AutoModel, AutoConfig\n","from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n","os.environ['TOKENIZERS_PARALLELISM']='true'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxZ1NXIwsIyo"},"outputs":[],"source":["class CFG:\n","    str_now = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n","    train = True\n","\n","    models_path = 'FB3-models'\n","    epochs = 5\n","    save_all_models = False\n","    competition = 'FB3'\n","    apex = True\n","    print_freq = 20\n","    num_workers = 4\n","    model = 'microsoft/deberta-v3-base' #If you want to train on the kaggle platform, v3-base is realistic. v3-large will time out.\n","\n","    gradient_checkpointing = True\n","    scheduler = 'cosine'\n","    batch_scheduler = True\n","    num_cycles = 0.5\n","    num_warmup_steps = 0\n","\n","    #Layer-Wise Learning Rate Decay\n","    llrd = True\n","    layerwise_lr = 5e-5\n","    layerwise_lr_decay = 0.9\n","    layerwise_weight_decay = 0.01\n","    layerwise_adam_epsilon = 1e-6\n","    layerwise_use_bertadam = False\n","    #pooling\n","    pooling = 'mean' # mean, max, min, attention, weightedlayer\n","\n","    #init_weight\n","    init_weight = 'normal' # normal, xavier_uniform, xavier_normal, kaiming_uniform, kaiming_normal, orthogonal\n","    #re-init\n","    reinit = True\n","    reinit_n = 1\n","\n","    unscale = False\n","    eps = 1e-6\n","    betas = (0.9, 0.999)\n","    max_len = 512\n","    weight_decay = 0.01\n","    gradient_accumulation_steps = 1\n","    max_grad_norm = 1000\n","    target_cols = ['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions']\n","    seed = 42\n","    cv_seed = 42\n","    n_fold = 5\n","    trn_fold = list(range(n_fold))\n","    batch_size = 8\n","    n_targets = 6\n","    gpu_id = 0\n","    device = f'cuda:{gpu_id}'\n","    train_file = '../input/feedback-prize-english-language-learning/train.csv'\n","    pseudo_files  = [f'../input/fb1-pseudo-label/{i}_pseudo_label.csv' for i in range(n_fold)]\n","    test_file = '../input/feedback-prize-english-language-learning/test.csv'\n","    submission_file = '../input/feedback-prize-english-language-learning/sample_submission.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1669533072956,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"},"user_tz":-540},"id":"fcSFEe6usJ9B","outputId":"8f8f5552-fb5b-4f00-ac28-2fe1659d99fd"},"outputs":[{"output_type":"stream","name":"stdout","text":["20221127-071113-deberta-v3-base-pseudo-simul\n"]}],"source":["#Unique model name\n","plus = 'pseudo-simul'\n","if len(CFG.model.split(\"/\")) == 2:\n","    CFG.identifier = f'{CFG.str_now}-{CFG.model.split(\"/\")[1]}-{plus}' \n","else:\n","    CFG.identifier = f'{CFG.str_now}-{CFG.model}-{plus}'\n","    \n","print(CFG.identifier)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11785,"status":"ok","timestamp":1669533084737,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"},"user_tz":-540},"id":"Y3topYVYsLOG","outputId":"4eac811a-d621-433e-aa58-860c16141a05"},"outputs":[{"output_type":"stream","name":"stdout","text":["./20221127-071113-deberta-v3-base-pseudo-simul/\n"]}],"source":["\n","CFG.df_train = pd.read_csv(CFG.train_file)\n","CFG.pseudo_train_list = [pd.read_csv(file_name) for file_name in CFG.pseudo_files]\n","CFG.OUTPUT_DIR = f'./{CFG.identifier}/'\n","CFG.log_filename = CFG.OUTPUT_DIR + 'train'\n","\n","os.system('pip install iterative-stratification==0.1.7')\n","#CV\n","from iterstrat.ml_stratifiers import MultilabelStratifiedKFold    \n","Fold = MultilabelStratifiedKFold(n_splits = CFG.n_fold, shuffle = True, random_state = CFG.cv_seed)\n","for n, (train_index, val_index) in enumerate(Fold.split(CFG.df_train, CFG.df_train[CFG.target_cols])):\n","    CFG.df_train.loc[val_index, 'fold'] = int(n)\n","CFG.df_train['fold'] = CFG.df_train['fold'].astype(int)\n","\n","\n","os.makedirs(CFG.OUTPUT_DIR, exist_ok=True)    \n","print(CFG.OUTPUT_DIR)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1669533084738,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"},"user_tz":-540},"id":"QnWrtZ6csMGf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"557a3788-95be-418d-8201-7aff5f1914f7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Before:  15594  After:  15142  each fold:  3129\n","Final:  19053\n","Before:  15594  After:  15142  each fold:  3129\n","Final:  19053\n","Before:  15594  After:  15142  each fold:  3129\n","Final:  19053\n","Before:  15594  After:  15142  each fold:  3129\n","Final:  19053\n","Before:  15594  After:  15142  each fold:  3129\n","Final:  19053\n"]}],"source":["pseudo_folds = []\n","df_train_set = set(CFG.df_train['text_id'].values)\n","for fold in range(5):\n","    cur_pseudo_df  = CFG.pseudo_train_list[fold]\n","    pseudo_set = set(cur_pseudo_df['text_id'].values)\n","    drop_list  = list(df_train_set & pseudo_set)\n","\n","    new_df = cur_pseudo_df[~cur_pseudo_df['text_id'].isin(drop_list)]\n","    new_df = new_df.drop([cur_pseudo_df.columns[0], cur_pseudo_df.columns[9]], axis = 1)\n","    print(\"Before: \", len(cur_pseudo_df), \" After: \", len(new_df), \" each fold: \", len(CFG.df_train[CFG.df_train['fold'] != 0]))\n","    new_df['fold'] = fold+1\n","    pseudo_folds.append(pd.concat([CFG.df_train, new_df], axis=0))\n","    del cur_pseudo_df, pseudo_set, drop_list\n","    print(\"Final: \", len(pseudo_folds[fold]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6sUZywhAXEzT"},"outputs":[],"source":["def MCRMSE(y_trues, y_preds):\n","    scores = []\n","    idxes = y_trues.shape[1]\n","    for i in range(idxes):\n","        y_true = y_trues[:, i]\n","        y_pred = y_preds[:, i]\n","        score = mean_squared_error(y_true, y_pred, squared = False)\n","        scores.append(score)\n","    mcrmse_score = np.mean(scores)\n","    return mcrmse_score, scores\n","\n","def get_score(y_trues, y_preds):\n","    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n","    return mcrmse_score, scores\n","\n","def get_logger(filename = CFG.log_filename):\n","    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n","    logger = getLogger(__name__)\n","    logger.setLevel(INFO)\n","    handler1 = StreamHandler()\n","    handler1.setFormatter(Formatter('%(message)s'))\n","    handler2 = FileHandler(filename = f'{filename}.log')\n","    handler2.setFormatter(Formatter('%(message)s'))\n","    logger.addHandler(handler1)\n","    logger.addHandler(handler2)\n","    return logger\n","\n","def prepare_input(cfg, text):\n","    inputs = cfg.tokenizer.encode_plus(\n","        text,\n","        return_tensors = None,\n","        add_special_tokens = True,\n","        max_length = cfg.max_len,\n","        pad_to_max_length = True,\n","        truncation = True\n","    )\n","    for k, v in inputs.items():\n","        inputs[k] = torch.tensor(v, dtype = torch.long)\n","    return inputs    \n","\n","def collate(inputs):\n","    mask_len = int(inputs['attention_mask'].sum(axis = 1).max())\n","    for k, v in inputs.items():\n","        inputs[k] = inputs[k][:, :mask_len]\n","    return inputs\n","\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","        \n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","        \n","    def update(self, val, n = 1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count\n","\n","def asMinutes(s):\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return f'{int(m)}m {int(s)}s'\n","\n","def timeSince(since, percent):\n","    now = time.time()\n","    s = now - since\n","    es = s / (percent)\n","    rs = es - s\n","    return f'{str(asMinutes(s))} (remain {str(asMinutes(rs))})'\n","\n","def seed_everything(seed = 42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    \n","seed_everything(CFG.seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"paCHTDL3sPfW"},"outputs":[],"source":["class MeanPooling(nn.Module):\n","    def __init__(self):\n","        super(MeanPooling, self).__init__()\n","        \n","    def forward(self, last_hidden_state, attention_mask):\n","        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n","        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n","        sum_mask = input_mask_expanded.sum(1)\n","        sum_mask = torch.clamp(sum_mask, min = 1e-9)\n","        mean_embeddings = sum_embeddings/sum_mask\n","        return mean_embeddings\n","\n","class MaxPooling(nn.Module):\n","    def __init__(self):\n","        super(MaxPooling, self).__init__()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nvmj0EZEsSTN"},"outputs":[],"source":["def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n","    losses = AverageMeter()\n","    model.train()\n","    scaler = torch.cuda.amp.GradScaler(enabled = CFG.apex)\n","    start = end = time.time()\n","    global_step = 0\n","  \n","    for step, (inputs, labels) in enumerate(train_loader):\n","        attention_mask = inputs['attention_mask'].to(device)\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.cuda.amp.autocast(enabled = CFG.apex):\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        scaler.scale(loss).backward()\n","        if CFG.unscale:\n","            scaler.unscale_(optimizer)\n","        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n","                \n","        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n","            scaler.step(optimizer)\n","            scaler.update()\n","            optimizer.zero_grad()\n","            global_step += 1\n","            if CFG.batch_scheduler:\n","                scheduler.step()\n","                \n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n","            print('Epoch: [{0}][{1}/{2}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  'Grad: {grad_norm:.4f} '\n","                  'LR: {lr:.8f} '\n","                  .format(epoch + 1, step, len(train_loader), remain = timeSince(start, float(step + 1)/len(train_loader)),\n","                          loss = losses,\n","                          grad_norm = grad_norm,\n","                          lr = scheduler.get_lr()[0]\n","                         )\n","                 )\n","    return losses.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eveetuNTsUMT"},"outputs":[],"source":["def valid_fn(valid_loader, model, criterion, device):\n","    losses = AverageMeter()\n","    model.eval()\n","    preds = []\n","    start = end = time.time()\n","    for step, (inputs, labels) in enumerate(valid_loader):\n","        inputs = collate(inputs)\n","        for k, v in inputs.items():\n","            inputs[k] = v.to(device)\n","        labels = labels.to(device)\n","        batch_size = labels.size(0)\n","        with torch.no_grad():\n","            y_preds = model(inputs)\n","            loss = criterion(y_preds, labels)\n","        if CFG.gradient_accumulation_steps > 1:\n","            loss = loss / CFG.gradient_accumulation_steps\n","        losses.update(loss.item(), batch_size)\n","        preds.append(y_preds.to('cpu').numpy())\n","        end = time.time()\n","        if step % CFG.print_freq == 0 or step == (len(valid_loader) - 1):\n","            print('EVAL: [{0}/{1}] '\n","                  'Elapsed {remain:s} '\n","                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n","                  .format(step, len(valid_loader),\n","                          loss = losses,\n","                          remain = timeSince(start, float(step + 1) / len(valid_loader))\n","                         )\n","                 )\n","    return losses.avg, np.concatenate(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1669533203212,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"},"user_tz":-540},"id":"_mFrZMpisVVx","outputId":"df9b5f49-fd96-40eb-9abb-abc15519b732"},"outputs":[{"output_type":"stream","name":"stderr","text":["OUTPUT_DIR: ./20221127-071113-deberta-v3-base-pseudo-simul/\n","INFO:__main__:OUTPUT_DIR: ./20221127-071113-deberta-v3-base-pseudo-simul/\n"]}],"source":["LOGGER = get_logger()\n","LOGGER.info(f'OUTPUT_DIR: {CFG.OUTPUT_DIR}')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":181,"referenced_widgets":["7efbfe0d1cd34447bd0338531d37a586","613ec2c892b540659b20f9b92ee4f909","8f3f39737324486d8eb85fe15f1db1f4","aa9e0120539d40ee8e26a743a54221ff","96c6272749d44b24a3212a248b063b85","a12c98b5b0bc4877a00a37b5bd9b9e69","e0486a18d1c94f189501792e29e52801","b49270a1543e40169d3f6f2b918028f7","2ad630bb6a83427383267b223745ec67","bd634d3456244396a2396c268855d447","b8ef5e33dbf44e5f829629a1dc6b588b","156cd32e9b89448191b2780377b429ca","bacaa87f83fe4da0a7bb72bd6890f171","56c256a530e4496487738d7a0a6b464b","9d06adad548f4eb3b1b4cabb0d7e895e","f8459e2ec95a4553ba63e99957b2bc39","a5ba438b26de4b749eafeb817ff2b760","47075a447a4f4ecca7d4ca54b71f4e3f","7f7ff21ebc584bb0a930367c8800c725","89826e44350a4d06a29cfa550b47f4b8","069c4f984a17498eb7efdc2fe7fdad30","76a328f3820a44319869ef522d6800f8","49bbfb5b8e9d41faaf615a74ddf8fd3d","99cbf61819bc455589373aab8affc394","1b023e98e7fa4f42abdd38d063df1c6a","0b71660d22fe48c2ade053b405b29da7","97368996b675473dac55d0782a80bef2","6c2fbb665b1b40b1b6760c576aceeb16","cbb30ef6f5284415bb1a73fce3ccba23","cad992e8f4764cce89c7523a36f743c6","9e8fc1669c02489eb3ab399da537b04d","71f3d04483d143e197fb24060e276d04","a0f01b7b366c4b1bbdf3f8b2ac519191","fe4372ac29c04d0dabdea7f0be95a6b0","34c578370f204b8ca27415c2cc87b066","d12fe568157e42dca0ca52be23d456c1","41142207aca34cb380dca8a2158f1a19","fee57ad07e95465182eb6724cfbd4731","2196d9c5df604a1198fd172d60016cdc","941db153172d4744b4c1757fa4c871ba","49eb567e4fec41e1bc218074655bcd40","75bb6db35a844cd295f0b5bf87e5fc87","e4f67557b2a54b84b9cce58bb18ce5a1","296049ceaf1347309036c56f87dc9490"]},"id":"SM0K1HNTtyF_","executionInfo":{"status":"ok","timestamp":1669533218805,"user_tz":-540,"elapsed":15186,"user":{"displayName":"hhog ssap","userId":"13037467103569678500"}},"outputId":"4f33b81c-e19c-4f91-f340-cdd68449960d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7efbfe0d1cd34447bd0338531d37a586"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/579 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"156cd32e9b89448191b2780377b429ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49bbfb5b8e9d41faaf615a74ddf8fd3d"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3911 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4372ac29c04d0dabdea7f0be95a6b0"}},"metadata":{}}],"source":["CFG.tokenizer = AutoTokenizer.from_pretrained(CFG.model)\n","CFG.tokenizer.save_pretrained(CFG.OUTPUT_DIR + 'tokenizer')\n","\n","#max_len\n","lengths = []\n","tk0 = tqdm(CFG.df_train['full_text'].fillna('').values, total = len(CFG.df_train))\n","for text in tk0:\n","    length = len(CFG.tokenizer(text, add_special_tokens = False)['input_ids'])\n","    lengths.append(length)\n","CFG.max_len = max(lengths) + 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"byh-s8DVsWEH"},"outputs":[],"source":["class FB3TrainDataset(Dataset):\n","    def __init__(self, cfg, df):\n","        self.cfg = cfg\n","        self.texts = df['full_text'].values\n","        self.labels = df[cfg.target_cols].values\n","        \n","    def __len__(self):\n","        return len(self.texts)\n","    \n","    def __getitem__(self, item):\n","        inputs = prepare_input(self.cfg, self.texts[item])\n","        label = torch.tensor(self.labels[item], dtype = torch.float)\n","        return inputs, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E6kEHQK5sW9Q"},"outputs":[],"source":["class FB3Model(nn.Module):\n","    def __init__(self, CFG, config_path = None, pretrained = False):\n","        super().__init__()\n","        self.CFG = CFG\n","        if config_path is None:\n","            self.config = AutoConfig.from_pretrained(CFG.model, ouput_hidden_states = True)\n","            self.config.save_pretrained(CFG.OUTPUT_DIR + 'config')\n","            self.config.hidden_dropout = 0.\n","            self.config.hidden_dropout_prob = 0.\n","            self.config.attention_dropout = 0.\n","            self.config.attention_probs_dropout_prob = 0.\n","        else:\n","            self.config = torch.load(config_path)\n","            \n","        LOGGER.info(self.config)\n","        \n","        if pretrained:\n","            self.model = AutoModel.from_pretrained(CFG.model, config=self.config)\n","            self.model.save_pretrained(CFG.OUTPUT_DIR + 'model')\n","        else:\n","            self.model = AutoModel(self.config)\n","            \n","        if self.CFG.gradient_checkpointing:\n","            self.model.gradient_checkpointing_enable()\n","            \n","        self.pool = MeanPooling()\n","        self.fc = nn.Linear(self.config.hidden_size, self.CFG.n_targets)\n","        self._init_weights(self.fc)\n","        \n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n","                \n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean = 0.0, std = self.config.initializer_range)\n","               \n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","    \n","    def feature(self, inputs):\n","        outputs = self.model(**inputs)\n","\n","        last_hidden_states = outputs[0]\n","        feature = self.pool(last_hidden_states, inputs['attention_mask'])\n","            \n","        return feature\n","    \n","    def forward(self, inputs):\n","        feature = self.feature(inputs)\n","        outout = self.fc(feature)\n","        return outout"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tJfqSMMusYVX"},"outputs":[],"source":["def re_initializing_layer(model, config, layer_num):\n","    for module in model.model.encoder.layer[-layer_num:].modules():\n","        if isinstance(module, nn.Linear)::\n","            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n","     \n","            if module.bias is not None:\n","                module.bias.data.zero_()\n","        elif isinstance(module, nn.Embedding):\n","            module.weight.data.normal_(mean=0.0, std=config.initializer_range)\n","     \n","            if module.padding_idx is not None:\n","                module.weight.data[module.padding_idx].zero_()\n","        elif isinstance(module, nn.LayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","\n","    return model   \n","\n","def train_loop(folds, org_folds, fold):\n","    LOGGER.info(f\"========== fold: {fold} training ==========\")\n","    print(\"len pseudo folds: \", len(folds))\n","    print(\"len_org_folds: \", len(org_folds))\n","\n","    train_folds = folds[folds['fold'] != fold].reset_index(drop = True)\n","    valid_folds = org_folds[org_folds['fold'] == fold].reset_index(drop = True)\n","\n","\n","    print(\"len_train_folds: \", len(train_folds))\n","    print(\"len_valid_folds: \", len(valid_folds))\n","    valid_labels = valid_folds[CFG.target_cols].values\n","    \n","    train_dataset = FB3TrainDataset(CFG, train_folds)\n","    valid_dataset = FB3TrainDataset(CFG, valid_folds)\n","    \n","    train_loader = DataLoader(train_dataset,\n","                              batch_size = CFG.batch_size,\n","                              shuffle = True, \n","                              num_workers = CFG.num_workers,\n","                              pin_memory = True, \n","                              drop_last = True\n","                             )\n","    valid_loader = DataLoader(valid_dataset,\n","                              batch_size = CFG.batch_size * 2,\n","                              shuffle=False,\n","                              num_workers=CFG.num_workers,\n","                              pin_memory=True, \n","                              drop_last=False)\n","\n","    model = FB3Model(CFG, config_path = None, pretrained = True)\n","    if CFG.reinit:\n","        model = re_initializing_layer(model, model.config, CFG.reinit_n)\n","        \n","    #os.makedirs(CFG.OUTPUT_DIR + 'config/', exist_ok = True)\n","    #torch.save(model.config, CFG.OUTPUT_DIR + 'config/config.pth')\n","    model.to(CFG.device)\n","    \n","    #llrd\n","    def get_optimizer_grouped_parameters(model, \n","                                         layerwise_lr,\n","                                         layerwise_weight_decay,\n","                                         layerwise_lr_decay):\n","        \n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        # initialize lr for task specific layer\n","        optimizer_grouped_parameters = [{\"params\": [p for n, p in model.named_parameters() if \"model\" not in n],\n","                                         \"weight_decay\": 0.0,\n","                                         \"lr\": layerwise_lr,\n","                                        },]\n","        # initialize lrs for every layer\n","        layers = [model.model.embeddings] + list(model.model.encoder.layer)\n","        layers.reverse()\n","        lr = layerwise_lr\n","        for layer in layers:\n","            optimizer_grouped_parameters += [{\"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n","                                              \"weight_decay\": layerwise_weight_decay,\n","                                              \"lr\": lr,\n","                                             },\n","                                             {\"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n","                                              \"weight_decay\": 0.0,\n","                                              \"lr\": lr,\n","                                             },]\n","            lr *= layerwise_lr_decay\n","        return optimizer_grouped_parameters\n","    \n","\n","    from transformers import AdamW\n","    grouped_optimizer_params = get_optimizer_grouped_parameters(model, \n","                                                                CFG.layerwise_lr, \n","                                                                CFG.layerwise_weight_decay, \n","                                                                CFG.layerwise_lr_decay)\n","    optimizer = AdamW(grouped_optimizer_params,\n","                      lr = CFG.layerwise_lr,\n","                      eps = CFG.layerwise_adam_epsilon,\n","                      correct_bias = not CFG.layerwise_use_bertadam)\n","\n","    \n","    def get_scheduler(cfg, optimizer, num_train_steps):\n","        if cfg.scheduler == 'linear':\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer, \n","                num_warmup_steps = cfg.num_warmup_steps, \n","                num_training_steps = num_train_steps\n","            )\n","        elif cfg.scheduler == 'cosine':\n","            scheduler = get_cosine_schedule_with_warmup(\n","                optimizer, \n","                num_warmup_steps = cfg.num_warmup_steps, \n","                num_training_steps = num_train_steps,\n","                num_cycles = cfg.num_cycles\n","            )\n","        return scheduler\n","    \n","    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n","    scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n","    \n","    criterion = nn.SmoothL1Loss(reduction='mean')\n","\n","    best_score = np.inf\n","    best_train_loss = np.inf\n","    best_val_loss = np.inf\n","    \n","    epoch_list = []\n","    epoch_avg_loss_list = []\n","    epoch_avg_val_loss_list = []\n","    epoch_score_list = []\n","    epoch_scores_list = []\n","\n","    for epoch in range(CFG.epochs):\n","        start_time = time.time()\n","\n","        # train\n","        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, CFG.device)\n","\n","        # eval\n","        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, CFG.device)\n","        \n","        # scoring\n","        score, scores = get_score(valid_labels, predictions)\n","\n","        elapsed = time.time() - start_time\n","        \n","        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n","        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}  Scores: {scores}')\n","        \n","        epoch_list.append(epoch+1)\n","        epoch_avg_loss_list.append(avg_loss)\n","        epoch_avg_val_loss_list.append(avg_val_loss)\n","        epoch_score_list.append(score)\n","        epoch_scores_list.append(scores)\n","        \n","        if best_score > score:\n","            best_score = score\n","            best_train_loss = avg_loss\n","            best_val_loss = avg_val_loss\n","            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\")\n","            \n","        if CFG.save_all_models:\n","            torch.save({'model': model.state_dict(),\n","                        'predictions': predictions},\n","                        CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_epoch{epoch + 1}.pth\")\n","\n","    predictions = torch.load(CFG.OUTPUT_DIR + f\"{CFG.model.replace('/', '-')}_fold{fold}_best.pth\", \n","                             map_location = torch.device('cpu'))['predictions']\n","    valid_folds[[f\"pred_{c}\" for c in CFG.target_cols]] = predictions\n","    \n","    df_epoch = pd.DataFrame({'epoch' : epoch_list,\n","                             'MCRMSE' : epoch_score_list,\n","                             'train_loss' : epoch_avg_loss_list, \n","                             'val_loss' : epoch_avg_val_loss_list})\n","    df_scores = pd.DataFrame(epoch_scores_list)\n","    df_scores.columns = CFG.target_cols\n","    \n","    \n","    torch.cuda.empty_cache()\n","    gc.collect()\n","    \n","    return best_train_loss, best_val_loss, valid_folds, pd.concat([df_epoch, df_scores], axis = 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d42e8ebe3d6745bc95fdce7a47863020","727a9524d87546a7ac04533a907dc881","17d24812066f439fad49a1723e918f26","cb5e9ba57c2447bfa0e02c88f4e3d4fa","73a6c566db9c4f2b842c0915890324f8","5284b5f50a694ecc903e0b3bbf12731c","1f1da28d4f3b40359f2df04e1e2643fe","882a8341586b4c85bb3ecec49a41e494","bc76904ee1b04a3fba5782579ab4bf7f","07b09498b31d4c5f92b30ca4fd294d48","3a21254af5e64078853b337a241e133e"]},"id":"rbXR2ZnasZ89","outputId":"885a7208-8d0e-4835-93fb-b7710889b955"},"outputs":[{"output_type":"stream","name":"stderr","text":["========== fold: 0 training ==========\n","INFO:__main__:========== fold: 0 training ==========\n"]},{"output_type":"stream","name":"stdout","text":["len pseudo folds:  19053\n","len_org_folds:  3911\n","len_train_folds:  18271\n","len_valid_folds:  782\n"]},{"output_type":"stream","name":"stderr","text":["DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n","INFO:__main__:DebertaV2Config {\n","  \"_name_or_path\": \"microsoft/deberta-v3-base\",\n","  \"attention_dropout\": 0.0,\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout\": 0.0,\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-07,\n","  \"max_position_embeddings\": 512,\n","  \"max_relative_positions\": -1,\n","  \"model_type\": \"deberta-v2\",\n","  \"norm_rel_ebd\": \"layer_norm\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 0,\n","  \"pooler_dropout\": 0,\n","  \"pooler_hidden_act\": \"gelu\",\n","  \"pooler_hidden_size\": 768,\n","  \"pos_att_type\": [\n","    \"p2c\",\n","    \"c2p\"\n","  ],\n","  \"position_biased_input\": false,\n","  \"position_buckets\": 256,\n","  \"relative_attention\": true,\n","  \"share_att_key\": true,\n","  \"transformers_version\": \"4.20.1\",\n","  \"type_vocab_size\": 0,\n","  \"vocab_size\": 128100\n","}\n","\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/354M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d42e8ebe3d6745bc95fdce7a47863020"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at microsoft/deberta-v3-base were not used when initializing DebertaV2Model: ['mask_predictions.LayerNorm.bias', 'mask_predictions.dense.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'mask_predictions.dense.weight', 'mask_predictions.classifier.bias', 'lm_predictions.lm_head.dense.bias', 'mask_predictions.classifier.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight']\n","- This IS expected if you are initializing DebertaV2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing DebertaV2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"output_type":"stream","name":"stdout","text":["Epoch: [1][0/2283] Elapsed 0m 6s (remain 252m 9s) Loss: 3.4162(3.4162) Grad: inf LR: 0.00005000 \n","Epoch: [1][20/2283] Elapsed 0m 49s (remain 88m 21s) Loss: 0.3282(1.8460) Grad: 225819.2500 LR: 0.00005000 \n","Epoch: [1][40/2283] Elapsed 1m 18s (remain 71m 18s) Loss: 0.2303(1.0411) Grad: 91145.9219 LR: 0.00005000 \n","Epoch: [1][60/2283] Elapsed 1m 56s (remain 70m 48s) Loss: 0.0373(0.7294) Grad: 93803.4062 LR: 0.00005000 \n","Epoch: [1][80/2283] Elapsed 2m 33s (remain 69m 40s) Loss: 0.0839(0.5594) Grad: 163021.0938 LR: 0.00004999 \n","Epoch: [1][100/2283] Elapsed 3m 8s (remain 68m 0s) Loss: 0.0210(0.4566) Grad: 59573.4453 LR: 0.00004999 \n","Epoch: [1][120/2283] Elapsed 3m 45s (remain 67m 14s) Loss: 0.0169(0.3874) Grad: 50552.6211 LR: 0.00004999 \n","Epoch: [1][140/2283] Elapsed 4m 23s (remain 66m 49s) Loss: 0.0194(0.3372) Grad: 56247.7383 LR: 0.00004998 \n","Epoch: [1][160/2283] Elapsed 4m 57s (remain 65m 25s) Loss: 0.0146(0.2986) Grad: 38078.0469 LR: 0.00004998 \n","Epoch: [1][180/2283] Elapsed 5m 30s (remain 63m 57s) Loss: 0.0591(0.2691) Grad: 109620.0078 LR: 0.00004997 \n","Epoch: [1][200/2283] Elapsed 6m 4s (remain 62m 51s) Loss: 0.0278(0.2438) Grad: 49308.5898 LR: 0.00004996 \n","Epoch: [1][220/2283] Elapsed 6m 39s (remain 62m 3s) Loss: 0.0186(0.2254) Grad: 22511.3477 LR: 0.00004995 \n","Epoch: [1][240/2283] Elapsed 7m 17s (remain 61m 46s) Loss: 0.0814(0.2090) Grad: 85547.9609 LR: 0.00004995 \n","Epoch: [1][260/2283] Elapsed 7m 55s (remain 61m 20s) Loss: 0.0044(0.1950) Grad: 39439.0977 LR: 0.00004994 \n","Epoch: [1][280/2283] Elapsed 8m 33s (remain 60m 55s) Loss: 0.0091(0.1826) Grad: 32465.9531 LR: 0.00004993 \n","Epoch: [1][300/2283] Elapsed 9m 10s (remain 60m 27s) Loss: 0.0146(0.1726) Grad: 137985.2969 LR: 0.00004991 \n","Epoch: [1][320/2283] Elapsed 9m 48s (remain 59m 54s) Loss: 0.0683(0.1635) Grad: 120356.9141 LR: 0.00004990 \n","Epoch: [1][340/2283] Elapsed 10m 23s (remain 59m 8s) Loss: 0.0112(0.1553) Grad: 25585.1992 LR: 0.00004989 \n","Epoch: [1][360/2283] Elapsed 10m 56s (remain 58m 15s) Loss: 0.0632(0.1486) Grad: 84528.5078 LR: 0.00004988 \n","Epoch: [1][380/2283] Elapsed 11m 35s (remain 57m 54s) Loss: 0.0296(0.1418) Grad: 86273.9297 LR: 0.00004986 \n","Epoch: [1][400/2283] Elapsed 12m 11s (remain 57m 12s) Loss: 0.0513(0.1358) Grad: 77084.3516 LR: 0.00004985 \n","Epoch: [1][420/2283] Elapsed 12m 55s (remain 57m 9s) Loss: 0.0223(0.1306) Grad: 57994.5117 LR: 0.00004983 \n","Epoch: [1][440/2283] Elapsed 13m 28s (remain 56m 17s) Loss: 0.0475(0.1257) Grad: 53852.0039 LR: 0.00004982 \n","Epoch: [1][460/2283] Elapsed 14m 12s (remain 56m 10s) Loss: 0.0212(0.1214) Grad: 75414.4453 LR: 0.00004980 \n","Epoch: [1][480/2283] Elapsed 14m 56s (remain 55m 59s) Loss: 0.0288(0.1174) Grad: 55378.2734 LR: 0.00004978 \n","Epoch: [1][500/2283] Elapsed 15m 37s (remain 55m 34s) Loss: 0.0754(0.1140) Grad: 250826.8906 LR: 0.00004976 \n","Epoch: [1][520/2283] Elapsed 16m 14s (remain 54m 55s) Loss: 0.0183(0.1110) Grad: 68209.8281 LR: 0.00004974 \n","Epoch: [1][540/2283] Elapsed 16m 54s (remain 54m 28s) Loss: 0.0112(0.1076) Grad: 34157.1992 LR: 0.00004972 \n","Epoch: [1][560/2283] Elapsed 17m 38s (remain 54m 8s) Loss: 0.0145(0.1048) Grad: 96442.0625 LR: 0.00004970 \n","Epoch: [1][580/2283] Elapsed 18m 14s (remain 53m 27s) Loss: 0.0309(0.1021) Grad: 77687.9844 LR: 0.00004968 \n","Epoch: [1][600/2283] Elapsed 18m 53s (remain 52m 53s) Loss: 0.0429(0.0998) Grad: 65639.0547 LR: 0.00004966 \n","Epoch: [1][620/2283] Elapsed 19m 35s (remain 52m 27s) Loss: 0.0417(0.0974) Grad: 40700.2812 LR: 0.00004964 \n","Epoch: [1][640/2283] Elapsed 20m 11s (remain 51m 43s) Loss: 0.0077(0.0953) Grad: 24226.9590 LR: 0.00004961 \n","Epoch: [1][660/2283] Elapsed 20m 45s (remain 50m 57s) Loss: 0.0362(0.0932) Grad: 49055.0195 LR: 0.00004959 \n","Epoch: [1][680/2283] Elapsed 21m 19s (remain 50m 10s) Loss: 0.0060(0.0909) Grad: 31643.2012 LR: 0.00004956 \n","Epoch: [1][700/2283] Elapsed 22m 0s (remain 49m 39s) Loss: 0.0311(0.0891) Grad: 34483.6133 LR: 0.00004954 \n","Epoch: [1][720/2283] Elapsed 22m 38s (remain 49m 3s) Loss: 0.0325(0.0876) Grad: 41976.6016 LR: 0.00004951 \n","Epoch: [1][740/2283] Elapsed 23m 11s (remain 48m 16s) Loss: 0.0046(0.0860) Grad: 29551.0508 LR: 0.00004948 \n","Epoch: [1][760/2283] Elapsed 23m 48s (remain 47m 37s) Loss: 0.0049(0.0843) Grad: 27107.4551 LR: 0.00004945 \n","Epoch: [1][780/2283] Elapsed 24m 25s (remain 46m 58s) Loss: 0.0054(0.0829) Grad: 62707.5625 LR: 0.00004943 \n","Epoch: [1][800/2283] Elapsed 25m 0s (remain 46m 16s) Loss: 0.0430(0.0813) Grad: 40032.4336 LR: 0.00004940 \n","Epoch: [1][820/2283] Elapsed 25m 38s (remain 45m 38s) Loss: 0.0294(0.0800) Grad: 43779.5000 LR: 0.00004936 \n","Epoch: [1][840/2283] Elapsed 26m 16s (remain 45m 3s) Loss: 0.0416(0.0787) Grad: 62996.5430 LR: 0.00004933 \n","Epoch: [1][860/2283] Elapsed 26m 55s (remain 44m 27s) Loss: 0.0309(0.0774) Grad: 52725.2773 LR: 0.00004930 \n","Epoch: [1][880/2283] Elapsed 27m 29s (remain 43m 45s) Loss: 0.0562(0.0762) Grad: 63224.2305 LR: 0.00004927 \n","Epoch: [1][900/2283] Elapsed 28m 9s (remain 43m 11s) Loss: 0.0193(0.0750) Grad: 45666.6758 LR: 0.00004924 \n","Epoch: [1][920/2283] Elapsed 28m 44s (remain 42m 29s) Loss: 0.0191(0.0737) Grad: 31222.4023 LR: 0.00004920 \n","Epoch: [1][940/2283] Elapsed 29m 20s (remain 41m 51s) Loss: 0.0096(0.0727) Grad: 77778.6250 LR: 0.00004917 \n","Epoch: [1][960/2283] Elapsed 29m 56s (remain 41m 11s) Loss: 0.0068(0.0717) Grad: 44871.8828 LR: 0.00004913 \n","Epoch: [1][980/2283] Elapsed 30m 34s (remain 40m 34s) Loss: 0.0070(0.0707) Grad: 80569.1172 LR: 0.00004909 \n","Epoch: [1][1000/2283] Elapsed 31m 12s (remain 39m 58s) Loss: 0.0104(0.0696) Grad: 40793.1641 LR: 0.00004906 \n","Epoch: [1][1020/2283] Elapsed 31m 53s (remain 39m 24s) Loss: 0.0327(0.0688) Grad: 35661.3242 LR: 0.00004902 \n","Epoch: [1][1040/2283] Elapsed 32m 37s (remain 38m 55s) Loss: 0.0145(0.0680) Grad: 64447.1133 LR: 0.00004898 \n","Epoch: [1][1060/2283] Elapsed 33m 12s (remain 38m 15s) Loss: 0.0354(0.0672) Grad: 166022.7031 LR: 0.00004894 \n","Epoch: [1][1080/2283] Elapsed 33m 49s (remain 37m 37s) Loss: 0.0466(0.0664) Grad: 60969.7461 LR: 0.00004890 \n","Epoch: [1][1100/2283] Elapsed 34m 30s (remain 37m 2s) Loss: 0.0246(0.0657) Grad: 29323.9258 LR: 0.00004886 \n","Epoch: [1][1120/2283] Elapsed 35m 11s (remain 36m 29s) Loss: 0.0340(0.0649) Grad: 46596.6445 LR: 0.00004882 \n","Epoch: [1][1140/2283] Elapsed 35m 46s (remain 35m 47s) Loss: 0.0050(0.0642) Grad: 34966.9492 LR: 0.00004878 \n","Epoch: [1][1160/2283] Elapsed 36m 27s (remain 35m 13s) Loss: 0.0534(0.0634) Grad: 73051.6094 LR: 0.00004874 \n","Epoch: [1][1180/2283] Elapsed 37m 8s (remain 34m 39s) Loss: 0.0246(0.0628) Grad: 49569.1484 LR: 0.00004869 \n","Epoch: [1][1200/2283] Elapsed 37m 50s (remain 34m 5s) Loss: 0.0274(0.0622) Grad: 28010.9746 LR: 0.00004865 \n","Epoch: [1][1220/2283] Elapsed 38m 33s (remain 33m 32s) Loss: 0.0254(0.0615) Grad: 33907.4023 LR: 0.00004860 \n","Epoch: [1][1240/2283] Elapsed 39m 13s (remain 32m 56s) Loss: 0.0130(0.0609) Grad: 29138.1289 LR: 0.00004856 \n","Epoch: [1][1260/2283] Elapsed 39m 49s (remain 32m 16s) Loss: 0.0125(0.0604) Grad: 67869.0000 LR: 0.00004851 \n","Epoch: [1][1280/2283] Elapsed 40m 30s (remain 31m 41s) Loss: 0.0100(0.0598) Grad: 17610.8340 LR: 0.00004846 \n","Epoch: [1][1300/2283] Elapsed 41m 17s (remain 31m 9s) Loss: 0.0697(0.0594) Grad: 90943.5078 LR: 0.00004842 \n","Epoch: [1][1320/2283] Elapsed 41m 54s (remain 30m 30s) Loss: 0.0602(0.0589) Grad: 88231.1875 LR: 0.00004837 \n","Epoch: [1][1340/2283] Elapsed 42m 23s (remain 29m 46s) Loss: 0.0081(0.0584) Grad: 41745.4648 LR: 0.00004832 \n","Epoch: [1][1360/2283] Elapsed 42m 57s (remain 29m 5s) Loss: 0.0350(0.0579) Grad: 23608.6855 LR: 0.00004827 \n","Epoch: [1][1380/2283] Elapsed 43m 37s (remain 28m 29s) Loss: 0.0021(0.0574) Grad: 20280.2617 LR: 0.00004822 \n","Epoch: [1][1400/2283] Elapsed 44m 14s (remain 27m 51s) Loss: 0.0192(0.0570) Grad: 66258.1406 LR: 0.00004817 \n","Epoch: [1][1420/2283] Elapsed 44m 49s (remain 27m 11s) Loss: 0.0513(0.0564) Grad: 62376.6445 LR: 0.00004811 \n","Epoch: [1][1440/2283] Elapsed 45m 24s (remain 26m 32s) Loss: 0.0038(0.0560) Grad: 26012.5312 LR: 0.00004806 \n","Epoch: [1][1460/2283] Elapsed 46m 1s (remain 25m 53s) Loss: 0.0089(0.0555) Grad: 26474.8906 LR: 0.00004801 \n","Epoch: [1][1480/2283] Elapsed 46m 39s (remain 25m 16s) Loss: 0.0293(0.0551) Grad: 41815.7539 LR: 0.00004795 \n","Epoch: [1][1500/2283] Elapsed 47m 16s (remain 24m 37s) Loss: 0.0154(0.0546) Grad: 66423.2812 LR: 0.00004790 \n","Epoch: [1][1520/2283] Elapsed 47m 58s (remain 24m 2s) Loss: 0.0049(0.0541) Grad: 28855.0566 LR: 0.00004784 \n","Epoch: [1][1540/2283] Elapsed 48m 38s (remain 23m 25s) Loss: 0.0159(0.0536) Grad: 31221.5723 LR: 0.00004779 \n","Epoch: [1][1560/2283] Elapsed 49m 20s (remain 22m 49s) Loss: 0.0058(0.0532) Grad: 33760.2070 LR: 0.00004773 \n","Epoch: [1][1580/2283] Elapsed 50m 3s (remain 22m 13s) Loss: 0.0223(0.0528) Grad: 93357.2500 LR: 0.00004767 \n","Epoch: [1][1600/2283] Elapsed 50m 40s (remain 21m 35s) Loss: 0.0356(0.0525) Grad: 39962.7383 LR: 0.00004761 \n","Epoch: [1][1620/2283] Elapsed 51m 17s (remain 20m 56s) Loss: 0.0422(0.0521) Grad: 42097.3906 LR: 0.00004755 \n","Epoch: [1][1640/2283] Elapsed 51m 52s (remain 20m 17s) Loss: 0.0100(0.0517) Grad: 52415.8047 LR: 0.00004750 \n","Epoch: [1][1660/2283] Elapsed 52m 30s (remain 19m 39s) Loss: 0.0374(0.0514) Grad: 95982.1953 LR: 0.00004743 \n","Epoch: [1][1680/2283] Elapsed 53m 7s (remain 19m 1s) Loss: 0.0164(0.0512) Grad: 38923.4180 LR: 0.00004737 \n","Epoch: [1][1700/2283] Elapsed 53m 46s (remain 18m 23s) Loss: 0.0152(0.0508) Grad: 19321.8965 LR: 0.00004731 \n","Epoch: [1][1720/2283] Elapsed 54m 19s (remain 17m 44s) Loss: 0.0330(0.0506) Grad: 62433.7812 LR: 0.00004725 \n","Epoch: [1][1740/2283] Elapsed 54m 57s (remain 17m 6s) Loss: 0.0345(0.0503) Grad: 44166.4570 LR: 0.00004719 \n","Epoch: [1][1760/2283] Elapsed 55m 34s (remain 16m 28s) Loss: 0.0299(0.0500) Grad: 36005.2305 LR: 0.00004712 \n","Epoch: [1][1780/2283] Elapsed 56m 17s (remain 15m 51s) Loss: 0.0439(0.0496) Grad: 74174.1328 LR: 0.00004706 \n","Epoch: [1][1800/2283] Elapsed 56m 56s (remain 15m 14s) Loss: 0.0326(0.0493) Grad: 29473.8652 LR: 0.00004699 \n","Epoch: [1][1820/2283] Elapsed 57m 35s (remain 14m 36s) Loss: 0.0195(0.0490) Grad: 41499.8398 LR: 0.00004693 \n","Epoch: [1][1840/2283] Elapsed 58m 13s (remain 13m 58s) Loss: 0.0118(0.0487) Grad: 49365.2031 LR: 0.00004686 \n","Epoch: [1][1860/2283] Elapsed 58m 52s (remain 13m 21s) Loss: 0.0115(0.0484) Grad: 25352.7637 LR: 0.00004679 \n","Epoch: [1][1880/2283] Elapsed 59m 29s (remain 12m 42s) Loss: 0.0211(0.0481) Grad: 43843.8516 LR: 0.00004673 \n","Epoch: [1][1900/2283] Elapsed 60m 0s (remain 12m 3s) Loss: 0.0218(0.0478) Grad: 36099.1719 LR: 0.00004666 \n","Epoch: [1][1920/2283] Elapsed 60m 34s (remain 11m 24s) Loss: 0.0129(0.0476) Grad: 46606.0273 LR: 0.00004659 \n","Epoch: [1][1940/2283] Elapsed 61m 10s (remain 10m 46s) Loss: 0.0138(0.0473) Grad: 36145.0000 LR: 0.00004652 \n","Epoch: [1][1960/2283] Elapsed 61m 55s (remain 10m 10s) Loss: 0.0281(0.0471) Grad: 46013.7617 LR: 0.00004645 \n","Epoch: [1][1980/2283] Elapsed 62m 40s (remain 9m 33s) Loss: 0.0303(0.0469) Grad: 80698.0234 LR: 0.00004638 \n","Epoch: [1][2000/2283] Elapsed 63m 19s (remain 8m 55s) Loss: 0.0142(0.0467) Grad: 34708.1680 LR: 0.00004631 \n","Epoch: [1][2020/2283] Elapsed 63m 55s (remain 8m 17s) Loss: 0.0015(0.0465) Grad: 26742.1289 LR: 0.00004623 \n","Epoch: [1][2040/2283] Elapsed 64m 33s (remain 7m 39s) Loss: 0.0242(0.0462) Grad: 90835.9062 LR: 0.00004616 \n","Epoch: [1][2060/2283] Elapsed 65m 13s (remain 7m 1s) Loss: 0.0112(0.0459) Grad: 82831.9453 LR: 0.00004609 \n","Epoch: [1][2080/2283] Elapsed 65m 44s (remain 6m 22s) Loss: 0.0444(0.0457) Grad: 134870.3125 LR: 0.00004601 \n","Epoch: [1][2100/2283] Elapsed 66m 25s (remain 5m 45s) Loss: 0.0179(0.0454) Grad: 36946.4727 LR: 0.00004594 \n","Epoch: [1][2120/2283] Elapsed 66m 56s (remain 5m 6s) Loss: 0.0018(0.0452) Grad: 45171.1133 LR: 0.00004586 \n","Epoch: [1][2140/2283] Elapsed 67m 34s (remain 4m 28s) Loss: 0.0283(0.0449) Grad: 60237.2969 LR: 0.00004579 \n","Epoch: [1][2160/2283] Elapsed 68m 23s (remain 3m 51s) Loss: 0.0805(0.0448) Grad: 176277.7344 LR: 0.00004571 \n","Epoch: [1][2180/2283] Elapsed 69m 1s (remain 3m 13s) Loss: 0.0203(0.0445) Grad: 89392.5234 LR: 0.00004563 \n","Epoch: [1][2200/2283] Elapsed 69m 30s (remain 2m 35s) Loss: 0.0243(0.0443) Grad: 133462.7656 LR: 0.00004555 \n","Epoch: [1][2220/2283] Elapsed 70m 12s (remain 1m 57s) Loss: 0.0760(0.0441) Grad: 101463.8203 LR: 0.00004548 \n","Epoch: [1][2240/2283] Elapsed 70m 47s (remain 1m 19s) Loss: 0.0166(0.0438) Grad: 32372.3535 LR: 0.00004540 \n","Epoch: [1][2260/2283] Elapsed 71m 17s (remain 0m 41s) Loss: 0.0284(0.0436) Grad: 44064.9492 LR: 0.00004532 \n","Epoch: [1][2280/2283] Elapsed 71m 46s (remain 0m 3s) Loss: 0.0513(0.0434) Grad: 90276.3906 LR: 0.00004524 \n","Epoch: [1][2282/2283] Elapsed 71m 52s (remain 0m 0s) Loss: 0.0686(0.0434) Grad: 464562.9688 LR: 0.00004523 \n","EVAL: [0/49] Elapsed 0m 2s (remain 1m 43s) Loss: 0.1104(0.1104) \n","EVAL: [20/49] Elapsed 0m 44s (remain 0m 59s) Loss: 0.1049(0.1112) \n","EVAL: [40/49] Elapsed 1m 30s (remain 0m 17s) Loss: 0.1162(0.1105) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1 - avg_train_loss: 0.0434  avg_val_loss: 0.1116  time: 4421s\n","INFO:__main__:Epoch 1 - avg_train_loss: 0.0434  avg_val_loss: 0.1116  time: 4421s\n","Epoch 1 - Score: 0.4725  Scores: [0.5307354763645933, 0.478920852627887, 0.41890723674078406, 0.4876202599791557, 0.4693006827046313, 0.44923968411073834]\n","INFO:__main__:Epoch 1 - Score: 0.4725  Scores: [0.5307354763645933, 0.478920852627887, 0.41890723674078406, 0.4876202599791557, 0.4693006827046313, 0.44923968411073834]\n","Epoch 1 - Save Best Score: 0.4725 Model\n","INFO:__main__:Epoch 1 - Save Best Score: 0.4725 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [48/49] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1184(0.1116) \n","Epoch: [2][0/2283] Elapsed 0m 1s (remain 72m 52s) Loss: 0.0370(0.0370) Grad: 97320.7891 LR: 0.00004522 \n","Epoch: [2][20/2283] Elapsed 0m 39s (remain 70m 8s) Loss: 0.0027(0.0155) Grad: 37061.3008 LR: 0.00004514 \n","Epoch: [2][40/2283] Elapsed 1m 16s (remain 69m 41s) Loss: 0.0160(0.0146) Grad: 54252.5625 LR: 0.00004506 \n","Epoch: [2][60/2283] Elapsed 1m 54s (remain 69m 33s) Loss: 0.0013(0.0155) Grad: 25746.4141 LR: 0.00004498 \n","Epoch: [2][80/2283] Elapsed 2m 32s (remain 69m 3s) Loss: 0.0125(0.0168) Grad: 62234.1836 LR: 0.00004490 \n","Epoch: [2][100/2283] Elapsed 3m 13s (remain 69m 39s) Loss: 0.0013(0.0166) Grad: 50134.9844 LR: 0.00004481 \n","Epoch: [2][120/2283] Elapsed 3m 52s (remain 69m 13s) Loss: 0.0359(0.0174) Grad: 262531.0312 LR: 0.00004473 \n","Epoch: [2][140/2283] Elapsed 4m 34s (remain 69m 22s) Loss: 0.0584(0.0178) Grad: 217219.8438 LR: 0.00004464 \n","Epoch: [2][160/2283] Elapsed 5m 11s (remain 68m 23s) Loss: 0.0274(0.0184) Grad: 115238.5781 LR: 0.00004456 \n","Epoch: [2][180/2283] Elapsed 5m 40s (remain 65m 59s) Loss: 0.0027(0.0181) Grad: 42551.4102 LR: 0.00004447 \n","Epoch: [2][200/2283] Elapsed 6m 19s (remain 65m 35s) Loss: 0.0016(0.0180) Grad: 40607.3789 LR: 0.00004439 \n","Epoch: [2][220/2283] Elapsed 7m 2s (remain 65m 39s) Loss: 0.0017(0.0178) Grad: 25007.0879 LR: 0.00004430 \n","Epoch: [2][240/2283] Elapsed 7m 45s (remain 65m 45s) Loss: 0.0862(0.0186) Grad: 152333.4219 LR: 0.00004421 \n","Epoch: [2][260/2283] Elapsed 8m 27s (remain 65m 31s) Loss: 0.0204(0.0184) Grad: 84370.9766 LR: 0.00004412 \n","Epoch: [2][280/2283] Elapsed 9m 9s (remain 65m 12s) Loss: 0.0170(0.0183) Grad: 63614.1484 LR: 0.00004403 \n","Epoch: [2][300/2283] Elapsed 9m 43s (remain 64m 4s) Loss: 0.0180(0.0183) Grad: 66330.3203 LR: 0.00004394 \n","Epoch: [2][320/2283] Elapsed 10m 20s (remain 63m 10s) Loss: 0.0113(0.0185) Grad: 50944.3750 LR: 0.00004385 \n","Epoch: [2][340/2283] Elapsed 10m 57s (remain 62m 23s) Loss: 0.0149(0.0186) Grad: 26910.1016 LR: 0.00004376 \n","Epoch: [2][360/2283] Elapsed 11m 32s (remain 61m 27s) Loss: 0.0058(0.0182) Grad: 26897.4668 LR: 0.00004367 \n","Epoch: [2][380/2283] Elapsed 12m 12s (remain 60m 56s) Loss: 0.0401(0.0181) Grad: 97651.3359 LR: 0.00004358 \n","Epoch: [2][400/2283] Elapsed 12m 52s (remain 60m 26s) Loss: 0.0142(0.0180) Grad: 37109.6758 LR: 0.00004349 \n","Epoch: [2][420/2283] Elapsed 13m 26s (remain 59m 28s) Loss: 0.0024(0.0178) Grad: 51780.5273 LR: 0.00004340 \n","Epoch: [2][440/2283] Elapsed 14m 6s (remain 58m 55s) Loss: 0.0022(0.0177) Grad: 47319.9648 LR: 0.00004330 \n","Epoch: [2][460/2283] Elapsed 14m 46s (remain 58m 23s) Loss: 0.0081(0.0177) Grad: 37677.0352 LR: 0.00004321 \n","Epoch: [2][480/2283] Elapsed 15m 21s (remain 57m 30s) Loss: 0.0257(0.0175) Grad: 65724.0625 LR: 0.00004311 \n","Epoch: [2][500/2283] Elapsed 16m 1s (remain 56m 59s) Loss: 0.0147(0.0175) Grad: 114612.4375 LR: 0.00004302 \n","Epoch: [2][520/2283] Elapsed 16m 37s (remain 56m 12s) Loss: 0.0342(0.0176) Grad: 119159.7891 LR: 0.00004292 \n","Epoch: [2][540/2283] Elapsed 17m 15s (remain 55m 33s) Loss: 0.0057(0.0178) Grad: 69996.4375 LR: 0.00004283 \n","Epoch: [2][560/2283] Elapsed 17m 51s (remain 54m 50s) Loss: 0.0234(0.0179) Grad: 70680.7891 LR: 0.00004273 \n","Epoch: [2][580/2283] Elapsed 18m 20s (remain 53m 44s) Loss: 0.0107(0.0181) Grad: 46629.8984 LR: 0.00004263 \n","Epoch: [2][600/2283] Elapsed 18m 55s (remain 52m 58s) Loss: 0.0299(0.0182) Grad: 45661.7852 LR: 0.00004253 \n","Epoch: [2][620/2283] Elapsed 19m 33s (remain 52m 20s) Loss: 0.0262(0.0184) Grad: 75944.0312 LR: 0.00004244 \n","Epoch: [2][640/2283] Elapsed 20m 9s (remain 51m 37s) Loss: 0.0236(0.0187) Grad: 142789.9219 LR: 0.00004234 \n","Epoch: [2][660/2283] Elapsed 20m 49s (remain 51m 7s) Loss: 0.0306(0.0186) Grad: 60253.1016 LR: 0.00004224 \n","Epoch: [2][680/2283] Elapsed 21m 34s (remain 50m 45s) Loss: 0.0069(0.0186) Grad: 46251.1133 LR: 0.00004214 \n","Epoch: [2][700/2283] Elapsed 22m 6s (remain 49m 53s) Loss: 0.0053(0.0185) Grad: 22908.0488 LR: 0.00004204 \n","Epoch: [2][720/2283] Elapsed 22m 40s (remain 49m 6s) Loss: 0.0025(0.0184) Grad: 51597.4062 LR: 0.00004194 \n","Epoch: [2][740/2283] Elapsed 23m 15s (remain 48m 25s) Loss: 0.0217(0.0185) Grad: 37325.0000 LR: 0.00004184 \n","Epoch: [2][760/2283] Elapsed 23m 50s (remain 47m 41s) Loss: 0.0221(0.0184) Grad: 143391.8125 LR: 0.00004173 \n","Epoch: [2][780/2283] Elapsed 24m 24s (remain 46m 55s) Loss: 0.0011(0.0184) Grad: 21375.7988 LR: 0.00004163 \n","Epoch: [2][800/2283] Elapsed 24m 59s (remain 46m 14s) Loss: 0.0191(0.0184) Grad: 78576.4062 LR: 0.00004153 \n","Epoch: [2][820/2283] Elapsed 25m 42s (remain 45m 45s) Loss: 0.0174(0.0185) Grad: 45675.3359 LR: 0.00004142 \n","Epoch: [2][840/2283] Elapsed 26m 19s (remain 45m 8s) Loss: 0.0269(0.0184) Grad: 79234.6797 LR: 0.00004132 \n","Epoch: [2][860/2283] Elapsed 26m 55s (remain 44m 27s) Loss: 0.0177(0.0182) Grad: 96169.5156 LR: 0.00004122 \n","Epoch: [2][880/2283] Elapsed 27m 32s (remain 43m 50s) Loss: 0.0526(0.0183) Grad: 297430.9375 LR: 0.00004111 \n","Epoch: [2][900/2283] Elapsed 28m 16s (remain 43m 21s) Loss: 0.0159(0.0183) Grad: 143505.0312 LR: 0.00004101 \n","Epoch: [2][920/2283] Elapsed 28m 47s (remain 42m 35s) Loss: 0.0104(0.0182) Grad: 82566.4922 LR: 0.00004090 \n","Epoch: [2][940/2283] Elapsed 29m 30s (remain 42m 5s) Loss: 0.0261(0.0182) Grad: 80360.9219 LR: 0.00004079 \n","Epoch: [2][960/2283] Elapsed 30m 20s (remain 41m 43s) Loss: 0.0290(0.0181) Grad: 52094.3438 LR: 0.00004069 \n","Epoch: [2][980/2283] Elapsed 31m 0s (remain 41m 8s) Loss: 0.0039(0.0181) Grad: 32936.6211 LR: 0.00004058 \n","Epoch: [2][1000/2283] Elapsed 31m 39s (remain 40m 32s) Loss: 0.0133(0.0182) Grad: 78916.9141 LR: 0.00004047 \n","Epoch: [2][1020/2283] Elapsed 32m 15s (remain 39m 51s) Loss: 0.0497(0.0183) Grad: 220474.4688 LR: 0.00004036 \n","Epoch: [2][1040/2283] Elapsed 32m 53s (remain 39m 14s) Loss: 0.0063(0.0183) Grad: 34682.8008 LR: 0.00004025 \n","Epoch: [2][1060/2283] Elapsed 33m 34s (remain 38m 39s) Loss: 0.0077(0.0183) Grad: 50507.1328 LR: 0.00004015 \n","Epoch: [2][1080/2283] Elapsed 34m 10s (remain 37m 59s) Loss: 0.0252(0.0184) Grad: 133567.3750 LR: 0.00004004 \n","Epoch: [2][1100/2283] Elapsed 34m 49s (remain 37m 23s) Loss: 0.0108(0.0184) Grad: 65209.8594 LR: 0.00003993 \n","Epoch: [2][1120/2283] Elapsed 35m 21s (remain 36m 39s) Loss: 0.0046(0.0183) Grad: 28815.3945 LR: 0.00003982 \n","Epoch: [2][1140/2283] Elapsed 36m 1s (remain 36m 3s) Loss: 0.0038(0.0183) Grad: 46254.5859 LR: 0.00003970 \n","Epoch: [2][1160/2283] Elapsed 36m 36s (remain 35m 22s) Loss: 0.0124(0.0183) Grad: 79661.5000 LR: 0.00003959 \n","Epoch: [2][1180/2283] Elapsed 37m 13s (remain 34m 43s) Loss: 0.0280(0.0182) Grad: 58875.2344 LR: 0.00003948 \n","Epoch: [2][1200/2283] Elapsed 37m 46s (remain 34m 1s) Loss: 0.0083(0.0183) Grad: 81346.0703 LR: 0.00003937 \n","Epoch: [2][1220/2283] Elapsed 38m 20s (remain 33m 20s) Loss: 0.0336(0.0183) Grad: 166639.7344 LR: 0.00003926 \n","Epoch: [2][1240/2283] Elapsed 38m 56s (remain 32m 42s) Loss: 0.0110(0.0183) Grad: 48780.2773 LR: 0.00003914 \n","Epoch: [2][1260/2283] Elapsed 39m 30s (remain 32m 1s) Loss: 0.0940(0.0183) Grad: 157199.9375 LR: 0.00003903 \n","Epoch: [2][1280/2283] Elapsed 40m 13s (remain 31m 27s) Loss: 0.0100(0.0183) Grad: 74873.2266 LR: 0.00003891 \n","Epoch: [2][1300/2283] Elapsed 40m 46s (remain 30m 46s) Loss: 0.0478(0.0184) Grad: 98248.9297 LR: 0.00003880 \n","Epoch: [2][1320/2283] Elapsed 41m 28s (remain 30m 12s) Loss: 0.1475(0.0187) Grad: 408408.7188 LR: 0.00003869 \n","Epoch: [2][1340/2283] Elapsed 42m 3s (remain 29m 32s) Loss: 0.0253(0.0186) Grad: 65726.7109 LR: 0.00003857 \n","Epoch: [2][1360/2283] Elapsed 42m 38s (remain 28m 53s) Loss: 0.0126(0.0186) Grad: 33983.4844 LR: 0.00003845 \n","Epoch: [2][1380/2283] Elapsed 43m 15s (remain 28m 15s) Loss: 0.0265(0.0187) Grad: 86723.1953 LR: 0.00003834 \n","Epoch: [2][1400/2283] Elapsed 43m 52s (remain 27m 37s) Loss: 0.0152(0.0187) Grad: 46904.9727 LR: 0.00003822 \n","Epoch: [2][1420/2283] Elapsed 44m 30s (remain 26m 59s) Loss: 0.0304(0.0186) Grad: 75450.4688 LR: 0.00003810 \n","Epoch: [2][1440/2283] Elapsed 45m 5s (remain 26m 20s) Loss: 0.0100(0.0185) Grad: 38458.3008 LR: 0.00003799 \n","Epoch: [2][1460/2283] Elapsed 45m 41s (remain 25m 42s) Loss: 0.0663(0.0184) Grad: 114909.4531 LR: 0.00003787 \n","Epoch: [2][1480/2283] Elapsed 46m 19s (remain 25m 5s) Loss: 0.0206(0.0184) Grad: 44371.3711 LR: 0.00003775 \n","Epoch: [2][1500/2283] Elapsed 46m 56s (remain 24m 27s) Loss: 0.0035(0.0184) Grad: 33721.9141 LR: 0.00003763 \n","Epoch: [2][1520/2283] Elapsed 47m 40s (remain 23m 53s) Loss: 0.0550(0.0184) Grad: 469592.0312 LR: 0.00003751 \n","Epoch: [2][1540/2283] Elapsed 48m 25s (remain 23m 19s) Loss: 0.0026(0.0184) Grad: 62760.1562 LR: 0.00003739 \n","Epoch: [2][1560/2283] Elapsed 49m 6s (remain 22m 42s) Loss: 0.0572(0.0184) Grad: 119166.9062 LR: 0.00003727 \n","Epoch: [2][1580/2283] Elapsed 49m 45s (remain 22m 5s) Loss: 0.0274(0.0184) Grad: 50391.0859 LR: 0.00003715 \n","Epoch: [2][1600/2283] Elapsed 50m 18s (remain 21m 25s) Loss: 0.0087(0.0184) Grad: 41759.5781 LR: 0.00003703 \n","Epoch: [2][1620/2283] Elapsed 50m 54s (remain 20m 47s) Loss: 0.0483(0.0183) Grad: 83033.1406 LR: 0.00003691 \n","Epoch: [2][1640/2283] Elapsed 51m 32s (remain 20m 9s) Loss: 0.0092(0.0184) Grad: 52157.8945 LR: 0.00003679 \n","Epoch: [2][1660/2283] Elapsed 52m 9s (remain 19m 32s) Loss: 0.0142(0.0183) Grad: 49803.5469 LR: 0.00003667 \n","Epoch: [2][1680/2283] Elapsed 52m 46s (remain 18m 54s) Loss: 0.0079(0.0184) Grad: 45338.5820 LR: 0.00003655 \n","Epoch: [2][1700/2283] Elapsed 53m 26s (remain 18m 17s) Loss: 0.0024(0.0183) Grad: 36173.3828 LR: 0.00003643 \n","Epoch: [2][1720/2283] Elapsed 53m 59s (remain 17m 37s) Loss: 0.0119(0.0183) Grad: 36167.3086 LR: 0.00003630 \n","Epoch: [2][1740/2283] Elapsed 54m 36s (remain 17m 0s) Loss: 0.0062(0.0183) Grad: 21895.2832 LR: 0.00003618 \n","Epoch: [2][1760/2283] Elapsed 55m 7s (remain 16m 20s) Loss: 0.0555(0.0183) Grad: 96012.4062 LR: 0.00003606 \n","Epoch: [2][1780/2283] Elapsed 55m 51s (remain 15m 44s) Loss: 0.0264(0.0183) Grad: 650271.2500 LR: 0.00003594 \n","Epoch: [2][1800/2283] Elapsed 56m 26s (remain 15m 6s) Loss: 0.0036(0.0183) Grad: 52286.4570 LR: 0.00003581 \n","Epoch: [2][1820/2283] Elapsed 57m 9s (remain 14m 30s) Loss: 0.0279(0.0183) Grad: 117260.8672 LR: 0.00003569 \n","Epoch: [2][1840/2283] Elapsed 57m 46s (remain 13m 52s) Loss: 0.0188(0.0183) Grad: 56998.4727 LR: 0.00003556 \n","Epoch: [2][1860/2283] Elapsed 58m 33s (remain 13m 16s) Loss: 0.0167(0.0183) Grad: 21280.4609 LR: 0.00003544 \n","Epoch: [2][1880/2283] Elapsed 59m 11s (remain 12m 39s) Loss: 0.0252(0.0183) Grad: 51452.4219 LR: 0.00003531 \n","Epoch: [2][1900/2283] Elapsed 59m 41s (remain 11m 59s) Loss: 0.0177(0.0184) Grad: 56768.6523 LR: 0.00003519 \n","Epoch: [2][1920/2283] Elapsed 60m 22s (remain 11m 22s) Loss: 0.0160(0.0184) Grad: 109533.3672 LR: 0.00003506 \n","Epoch: [2][1940/2283] Elapsed 60m 56s (remain 10m 44s) Loss: 0.0140(0.0184) Grad: 53892.8984 LR: 0.00003494 \n","Epoch: [2][1960/2283] Elapsed 61m 28s (remain 10m 5s) Loss: 0.0021(0.0184) Grad: 30553.3613 LR: 0.00003481 \n","Epoch: [2][1980/2283] Elapsed 62m 12s (remain 9m 29s) Loss: 0.0148(0.0184) Grad: 28671.9004 LR: 0.00003468 \n","Epoch: [2][2000/2283] Elapsed 62m 59s (remain 8m 52s) Loss: 0.0112(0.0184) Grad: 64326.9492 LR: 0.00003456 \n","Epoch: [2][2020/2283] Elapsed 63m 29s (remain 8m 13s) Loss: 0.0154(0.0183) Grad: 134113.4219 LR: 0.00003443 \n","Epoch: [2][2040/2283] Elapsed 64m 3s (remain 7m 35s) Loss: 0.0143(0.0184) Grad: 76276.2656 LR: 0.00003430 \n","Epoch: [2][2060/2283] Elapsed 64m 41s (remain 6m 58s) Loss: 0.0217(0.0184) Grad: 270374.2188 LR: 0.00003417 \n","Epoch: [2][2080/2283] Elapsed 65m 19s (remain 6m 20s) Loss: 0.1011(0.0184) Grad: 344702.5938 LR: 0.00003404 \n","Epoch: [2][2100/2283] Elapsed 65m 53s (remain 5m 42s) Loss: 0.0088(0.0185) Grad: 62846.6016 LR: 0.00003392 \n","Epoch: [2][2120/2283] Elapsed 66m 34s (remain 5m 5s) Loss: 0.0194(0.0184) Grad: 130675.6875 LR: 0.00003379 \n","Epoch: [2][2140/2283] Elapsed 67m 11s (remain 4m 27s) Loss: 0.0219(0.0184) Grad: 112371.4375 LR: 0.00003366 \n","Epoch: [2][2160/2283] Elapsed 67m 50s (remain 3m 49s) Loss: 0.0075(0.0184) Grad: 69592.9219 LR: 0.00003353 \n","Epoch: [2][2180/2283] Elapsed 68m 31s (remain 3m 12s) Loss: 0.0245(0.0184) Grad: 143263.0781 LR: 0.00003340 \n","Epoch: [2][2200/2283] Elapsed 69m 12s (remain 2m 34s) Loss: 0.0519(0.0184) Grad: 174360.0156 LR: 0.00003327 \n","Epoch: [2][2220/2283] Elapsed 69m 53s (remain 1m 57s) Loss: 0.0232(0.0184) Grad: 68199.1562 LR: 0.00003314 \n","Epoch: [2][2240/2283] Elapsed 70m 33s (remain 1m 19s) Loss: 0.0161(0.0184) Grad: 64115.9961 LR: 0.00003301 \n","Epoch: [2][2260/2283] Elapsed 71m 10s (remain 0m 41s) Loss: 0.0015(0.0184) Grad: 59492.3633 LR: 0.00003288 \n","Epoch: [2][2280/2283] Elapsed 71m 48s (remain 0m 3s) Loss: 0.0145(0.0183) Grad: 87871.6562 LR: 0.00003275 \n","Epoch: [2][2282/2283] Elapsed 71m 50s (remain 0m 0s) Loss: 0.0665(0.0183) Grad: 253534.7500 LR: 0.00003274 \n","EVAL: [0/49] Elapsed 0m 2s (remain 1m 44s) Loss: 0.1093(0.1093) \n","EVAL: [20/49] Elapsed 0m 44s (remain 0m 59s) Loss: 0.0971(0.0997) \n","EVAL: [40/49] Elapsed 1m 29s (remain 0m 17s) Loss: 0.0965(0.1013) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 2 - avg_train_loss: 0.0183  avg_val_loss: 0.1006  time: 4419s\n","INFO:__main__:Epoch 2 - avg_train_loss: 0.0183  avg_val_loss: 0.1006  time: 4419s\n","Epoch 2 - Score: 0.4489  Scores: [0.4796153913304552, 0.45654387101035676, 0.4048814693945213, 0.4530342112235974, 0.4585768023929215, 0.44056050101466127]\n","INFO:__main__:Epoch 2 - Score: 0.4489  Scores: [0.4796153913304552, 0.45654387101035676, 0.4048814693945213, 0.4530342112235974, 0.4585768023929215, 0.44056050101466127]\n","Epoch 2 - Save Best Score: 0.4489 Model\n","INFO:__main__:Epoch 2 - Save Best Score: 0.4489 Model\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [48/49] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1173(0.1006) \n","Epoch: [3][0/2283] Elapsed 0m 1s (remain 73m 15s) Loss: 0.0014(0.0014) Grad: 41786.2930 LR: 0.00003273 \n","Epoch: [3][20/2283] Elapsed 0m 38s (remain 68m 16s) Loss: 0.0081(0.0186) Grad: 188851.5781 LR: 0.00003260 \n","Epoch: [3][40/2283] Elapsed 1m 16s (remain 70m 2s) Loss: 0.0324(0.0198) Grad: 72616.4375 LR: 0.00003247 \n","Epoch: [3][60/2283] Elapsed 1m 51s (remain 67m 31s) Loss: 0.0025(0.0167) Grad: 33001.1406 LR: 0.00003234 \n","Epoch: [3][80/2283] Elapsed 2m 31s (remain 68m 31s) Loss: 0.0174(0.0173) Grad: 93571.1328 LR: 0.00003220 \n","Epoch: [3][100/2283] Elapsed 3m 4s (remain 66m 26s) Loss: 0.0287(0.0168) Grad: 56618.0156 LR: 0.00003207 \n","Epoch: [3][120/2283] Elapsed 3m 38s (remain 64m 59s) Loss: 0.0102(0.0173) Grad: 79624.3203 LR: 0.00003194 \n","Epoch: [3][140/2283] Elapsed 4m 16s (remain 65m 0s) Loss: 0.0048(0.0176) Grad: 47263.4609 LR: 0.00003181 \n","Epoch: [3][160/2283] Elapsed 4m 59s (remain 65m 42s) Loss: 0.0136(0.0173) Grad: 101225.7812 LR: 0.00003168 \n","Epoch: [3][180/2283] Elapsed 5m 39s (remain 65m 41s) Loss: 0.0294(0.0173) Grad: 35115.0977 LR: 0.00003154 \n","Epoch: [3][200/2283] Elapsed 6m 10s (remain 63m 59s) Loss: 0.0273(0.0172) Grad: 60236.6172 LR: 0.00003141 \n","Epoch: [3][220/2283] Elapsed 6m 45s (remain 62m 59s) Loss: 0.0044(0.0173) Grad: 60399.2070 LR: 0.00003128 \n","Epoch: [3][240/2283] Elapsed 7m 22s (remain 62m 32s) Loss: 0.0049(0.0171) Grad: 46901.3086 LR: 0.00003114 \n","Epoch: [3][260/2283] Elapsed 7m 58s (remain 61m 45s) Loss: 0.0109(0.0169) Grad: 30636.6855 LR: 0.00003101 \n","Epoch: [3][280/2283] Elapsed 8m 34s (remain 61m 3s) Loss: 0.0023(0.0165) Grad: 65852.2109 LR: 0.00003088 \n","Epoch: [3][300/2283] Elapsed 9m 19s (remain 61m 22s) Loss: 0.0477(0.0164) Grad: 275300.9688 LR: 0.00003074 \n","Epoch: [3][320/2283] Elapsed 9m 50s (remain 60m 11s) Loss: 0.0025(0.0164) Grad: 33229.2344 LR: 0.00003061 \n","Epoch: [3][340/2283] Elapsed 10m 32s (remain 60m 0s) Loss: 0.0172(0.0166) Grad: 209384.7500 LR: 0.00003047 \n","Epoch: [3][360/2283] Elapsed 11m 8s (remain 59m 20s) Loss: 0.0065(0.0166) Grad: 45364.1211 LR: 0.00003034 \n","Epoch: [3][380/2283] Elapsed 11m 43s (remain 58m 33s) Loss: 0.0366(0.0167) Grad: 87244.4531 LR: 0.00003021 \n","Epoch: [3][400/2283] Elapsed 12m 16s (remain 57m 38s) Loss: 0.0218(0.0169) Grad: 120852.3906 LR: 0.00003007 \n","Epoch: [3][420/2283] Elapsed 12m 55s (remain 57m 10s) Loss: 0.0115(0.0168) Grad: 31956.3672 LR: 0.00002994 \n","Epoch: [3][440/2283] Elapsed 13m 32s (remain 56m 35s) Loss: 0.0237(0.0167) Grad: 81392.0703 LR: 0.00002980 \n","Epoch: [3][460/2283] Elapsed 14m 13s (remain 56m 14s) Loss: 0.0233(0.0166) Grad: 72770.0547 LR: 0.00002967 \n","Epoch: [3][480/2283] Elapsed 14m 56s (remain 55m 58s) Loss: 0.0183(0.0168) Grad: 56923.1211 LR: 0.00002953 \n","Epoch: [3][500/2283] Elapsed 15m 46s (remain 56m 8s) Loss: 0.0055(0.0169) Grad: 137984.9531 LR: 0.00002940 \n","Epoch: [3][520/2283] Elapsed 16m 22s (remain 55m 23s) Loss: 0.0211(0.0166) Grad: 53938.0938 LR: 0.00002926 \n","Epoch: [3][540/2283] Elapsed 16m 59s (remain 54m 44s) Loss: 0.0123(0.0167) Grad: 63799.5508 LR: 0.00002913 \n","Epoch: [3][560/2283] Elapsed 17m 47s (remain 54m 37s) Loss: 0.0206(0.0169) Grad: 52399.5508 LR: 0.00002899 \n","Epoch: [3][580/2283] Elapsed 18m 22s (remain 53m 49s) Loss: 0.0080(0.0169) Grad: 34745.3477 LR: 0.00002885 \n","Epoch: [3][600/2283] Elapsed 19m 2s (remain 53m 18s) Loss: 0.0094(0.0169) Grad: 42270.2891 LR: 0.00002872 \n","Epoch: [3][620/2283] Elapsed 19m 42s (remain 52m 44s) Loss: 0.0214(0.0168) Grad: 66461.9141 LR: 0.00002858 \n","Epoch: [3][640/2283] Elapsed 20m 18s (remain 52m 0s) Loss: 0.0041(0.0167) Grad: 41986.4922 LR: 0.00002845 \n","Epoch: [3][660/2283] Elapsed 21m 0s (remain 51m 33s) Loss: 0.0150(0.0169) Grad: 86766.4766 LR: 0.00002831 \n","Epoch: [3][680/2283] Elapsed 21m 37s (remain 50m 53s) Loss: 0.0036(0.0169) Grad: 48384.7031 LR: 0.00002817 \n","Epoch: [3][700/2283] Elapsed 22m 16s (remain 50m 16s) Loss: 0.0195(0.0167) Grad: 42401.1016 LR: 0.00002804 \n","Epoch: [3][720/2283] Elapsed 22m 54s (remain 49m 36s) Loss: 0.0303(0.0168) Grad: 87435.8203 LR: 0.00002790 \n","Epoch: [3][740/2283] Elapsed 23m 33s (remain 49m 0s) Loss: 0.0242(0.0168) Grad: 88549.7266 LR: 0.00002776 \n","Epoch: [3][760/2283] Elapsed 24m 7s (remain 48m 15s) Loss: 0.0050(0.0166) Grad: 37432.4648 LR: 0.00002763 \n","Epoch: [3][780/2283] Elapsed 24m 43s (remain 47m 33s) Loss: 0.0211(0.0166) Grad: 107662.9297 LR: 0.00002749 \n","Epoch: [3][800/2283] Elapsed 25m 21s (remain 46m 55s) Loss: 0.0111(0.0165) Grad: 68929.0547 LR: 0.00002735 \n","Epoch: [3][820/2283] Elapsed 25m 54s (remain 46m 8s) Loss: 0.0265(0.0166) Grad: 91713.7812 LR: 0.00002722 \n","Epoch: [3][840/2283] Elapsed 26m 29s (remain 45m 26s) Loss: 0.0026(0.0165) Grad: 34179.7148 LR: 0.00002708 \n","Epoch: [3][860/2283] Elapsed 27m 10s (remain 44m 53s) Loss: 0.0104(0.0164) Grad: 34025.1562 LR: 0.00002694 \n","Epoch: [3][880/2283] Elapsed 27m 50s (remain 44m 17s) Loss: 0.0257(0.0164) Grad: 187180.0156 LR: 0.00002680 \n","Epoch: [3][900/2283] Elapsed 28m 26s (remain 43m 38s) Loss: 0.0230(0.0164) Grad: 73865.2031 LR: 0.00002667 \n","Epoch: [3][920/2283] Elapsed 29m 8s (remain 43m 6s) Loss: 0.0474(0.0163) Grad: 74863.1641 LR: 0.00002653 \n","Epoch: [3][940/2283] Elapsed 29m 43s (remain 42m 23s) Loss: 0.0087(0.0163) Grad: 66055.0234 LR: 0.00002639 \n","Epoch: [3][960/2283] Elapsed 30m 21s (remain 41m 45s) Loss: 0.0267(0.0162) Grad: 76819.8281 LR: 0.00002625 \n","Epoch: [3][980/2283] Elapsed 31m 1s (remain 41m 10s) Loss: 0.0348(0.0162) Grad: 49864.0469 LR: 0.00002612 \n","Epoch: [3][1000/2283] Elapsed 31m 42s (remain 40m 36s) Loss: 0.0176(0.0162) Grad: 52663.6484 LR: 0.00002598 \n","Epoch: [3][1020/2283] Elapsed 32m 23s (remain 40m 2s) Loss: 0.0070(0.0162) Grad: 27177.3574 LR: 0.00002584 \n","Epoch: [3][1040/2283] Elapsed 33m 0s (remain 39m 23s) Loss: 0.0119(0.0162) Grad: 47381.7773 LR: 0.00002570 \n","Epoch: [3][1060/2283] Elapsed 33m 38s (remain 38m 44s) Loss: 0.0601(0.0162) Grad: 362306.5000 LR: 0.00002557 \n","Epoch: [3][1080/2283] Elapsed 34m 12s (remain 38m 1s) Loss: 0.0183(0.0162) Grad: 56980.4883 LR: 0.00002543 \n","Epoch: [3][1100/2283] Elapsed 34m 51s (remain 37m 24s) Loss: 0.0267(0.0162) Grad: 141211.7500 LR: 0.00002529 \n","Epoch: [3][1120/2283] Elapsed 35m 27s (remain 36m 45s) Loss: 0.0010(0.0162) Grad: 21615.3281 LR: 0.00002515 \n","Epoch: [3][1140/2283] Elapsed 36m 7s (remain 36m 9s) Loss: 0.0126(0.0162) Grad: 41978.7617 LR: 0.00002502 \n","Epoch: [3][1160/2283] Elapsed 36m 42s (remain 35m 28s) Loss: 0.0564(0.0162) Grad: 109348.3984 LR: 0.00002488 \n","Epoch: [3][1180/2283] Elapsed 37m 15s (remain 34m 46s) Loss: 0.0176(0.0162) Grad: 64511.8477 LR: 0.00002474 \n","Epoch: [3][1200/2283] Elapsed 37m 56s (remain 34m 10s) Loss: 0.0293(0.0161) Grad: 78817.8359 LR: 0.00002460 \n","Epoch: [3][1220/2283] Elapsed 38m 36s (remain 33m 34s) Loss: 0.0052(0.0161) Grad: 83381.5703 LR: 0.00002447 \n","Epoch: [3][1240/2283] Elapsed 39m 10s (remain 32m 53s) Loss: 0.0238(0.0162) Grad: 84811.7109 LR: 0.00002433 \n","Epoch: [3][1260/2283] Elapsed 39m 53s (remain 32m 20s) Loss: 0.0211(0.0161) Grad: 58685.3398 LR: 0.00002419 \n","Epoch: [3][1280/2283] Elapsed 40m 27s (remain 31m 38s) Loss: 0.0150(0.0161) Grad: 42205.2695 LR: 0.00002405 \n","Epoch: [3][1300/2283] Elapsed 41m 4s (remain 31m 0s) Loss: 0.0151(0.0161) Grad: 82260.0859 LR: 0.00002392 \n","Epoch: [3][1320/2283] Elapsed 41m 48s (remain 30m 27s) Loss: 0.0043(0.0161) Grad: 27273.3105 LR: 0.00002378 \n","Epoch: [3][1340/2283] Elapsed 42m 20s (remain 29m 44s) Loss: 0.0124(0.0160) Grad: 31762.8516 LR: 0.00002364 \n","Epoch: [3][1360/2283] Elapsed 42m 58s (remain 29m 6s) Loss: 0.0241(0.0161) Grad: 215102.8438 LR: 0.00002350 \n","Epoch: [3][1380/2283] Elapsed 43m 28s (remain 28m 23s) Loss: 0.0132(0.0161) Grad: 52931.4453 LR: 0.00002337 \n","Epoch: [3][1400/2283] Elapsed 44m 9s (remain 27m 48s) Loss: 0.0028(0.0161) Grad: 85398.1094 LR: 0.00002323 \n","Epoch: [3][1420/2283] Elapsed 44m 46s (remain 27m 9s) Loss: 0.0234(0.0162) Grad: 54688.5742 LR: 0.00002309 \n","Epoch: [3][1440/2283] Elapsed 45m 25s (remain 26m 32s) Loss: 0.0104(0.0162) Grad: 133776.6094 LR: 0.00002296 \n","Epoch: [3][1460/2283] Elapsed 46m 8s (remain 25m 57s) Loss: 0.0026(0.0162) Grad: 145841.3906 LR: 0.00002282 \n","Epoch: [3][1480/2283] Elapsed 46m 57s (remain 25m 25s) Loss: 0.0102(0.0163) Grad: 59774.4727 LR: 0.00002268 \n","Epoch: [3][1500/2283] Elapsed 47m 32s (remain 24m 46s) Loss: 0.0286(0.0162) Grad: 53926.9805 LR: 0.00002255 \n","Epoch: [3][1520/2283] Elapsed 48m 11s (remain 24m 8s) Loss: 0.0078(0.0162) Grad: 53570.5156 LR: 0.00002241 \n","Epoch: [3][1540/2283] Elapsed 48m 51s (remain 23m 31s) Loss: 0.0076(0.0162) Grad: 29318.9453 LR: 0.00002227 \n","Epoch: [3][1560/2283] Elapsed 49m 26s (remain 22m 52s) Loss: 0.0332(0.0163) Grad: 60149.1641 LR: 0.00002213 \n","Epoch: [3][1580/2283] Elapsed 50m 4s (remain 22m 14s) Loss: 0.0052(0.0163) Grad: 33985.2695 LR: 0.00002200 \n","Epoch: [3][1600/2283] Elapsed 50m 40s (remain 21m 35s) Loss: 0.0375(0.0163) Grad: 291327.6562 LR: 0.00002186 \n","Epoch: [3][1620/2283] Elapsed 51m 20s (remain 20m 58s) Loss: 0.0240(0.0163) Grad: 53944.3281 LR: 0.00002173 \n","Epoch: [3][1640/2283] Elapsed 51m 53s (remain 20m 17s) Loss: 0.0010(0.0162) Grad: 19737.1699 LR: 0.00002159 \n","Epoch: [3][1660/2283] Elapsed 52m 31s (remain 19m 40s) Loss: 0.0085(0.0163) Grad: 71110.6250 LR: 0.00002145 \n","Epoch: [3][1680/2283] Elapsed 53m 9s (remain 19m 2s) Loss: 0.0364(0.0162) Grad: 71668.7031 LR: 0.00002132 \n","Epoch: [3][1700/2283] Elapsed 53m 37s (remain 18m 20s) Loss: 0.0171(0.0162) Grad: 44302.3750 LR: 0.00002118 \n","Epoch: [3][1720/2283] Elapsed 54m 14s (remain 17m 42s) Loss: 0.0273(0.0162) Grad: 69010.4922 LR: 0.00002104 \n","Epoch: [3][1740/2283] Elapsed 54m 48s (remain 17m 3s) Loss: 0.0338(0.0162) Grad: 48139.8750 LR: 0.00002091 \n","Epoch: [3][1760/2283] Elapsed 55m 34s (remain 16m 28s) Loss: 0.0153(0.0162) Grad: 79096.2344 LR: 0.00002077 \n","Epoch: [3][1780/2283] Elapsed 56m 9s (remain 15m 49s) Loss: 0.0088(0.0162) Grad: 41107.6797 LR: 0.00002064 \n","Epoch: [3][1800/2283] Elapsed 56m 50s (remain 15m 12s) Loss: 0.0162(0.0163) Grad: 41541.5078 LR: 0.00002050 \n","Epoch: [3][1820/2283] Elapsed 57m 32s (remain 14m 35s) Loss: 0.0130(0.0163) Grad: 32634.3574 LR: 0.00002037 \n","Epoch: [3][1840/2283] Elapsed 58m 8s (remain 13m 57s) Loss: 0.0005(0.0163) Grad: 11422.0059 LR: 0.00002023 \n","Epoch: [3][1860/2283] Elapsed 58m 55s (remain 13m 21s) Loss: 0.0232(0.0163) Grad: 51469.3008 LR: 0.00002010 \n","Epoch: [3][1880/2283] Elapsed 59m 40s (remain 12m 45s) Loss: 0.0193(0.0163) Grad: 63150.1836 LR: 0.00001996 \n","Epoch: [3][1900/2283] Elapsed 60m 21s (remain 12m 7s) Loss: 0.0112(0.0163) Grad: 33270.0938 LR: 0.00001983 \n","Epoch: [3][1920/2283] Elapsed 60m 53s (remain 11m 28s) Loss: 0.0211(0.0163) Grad: 54290.5234 LR: 0.00001969 \n","Epoch: [3][1940/2283] Elapsed 61m 22s (remain 10m 48s) Loss: 0.0051(0.0163) Grad: 24632.0977 LR: 0.00001956 \n","Epoch: [3][1960/2283] Elapsed 62m 12s (remain 10m 12s) Loss: 0.0012(0.0163) Grad: 14131.9004 LR: 0.00001942 \n","Epoch: [3][1980/2283] Elapsed 62m 51s (remain 9m 34s) Loss: 0.0024(0.0163) Grad: 33018.1445 LR: 0.00001929 \n","Epoch: [3][2000/2283] Elapsed 63m 39s (remain 8m 58s) Loss: 0.0257(0.0163) Grad: 276825.2500 LR: 0.00001916 \n","Epoch: [3][2020/2283] Elapsed 64m 16s (remain 8m 19s) Loss: 0.0430(0.0164) Grad: 1041302.2500 LR: 0.00001902 \n","Epoch: [3][2040/2283] Elapsed 64m 47s (remain 7m 40s) Loss: 0.0004(0.0164) Grad: 28978.5098 LR: 0.00001889 \n","Epoch: [3][2060/2283] Elapsed 65m 25s (remain 7m 2s) Loss: 0.0151(0.0164) Grad: 75678.8125 LR: 0.00001876 \n","Epoch: [3][2080/2283] Elapsed 66m 0s (remain 6m 24s) Loss: 0.0251(0.0164) Grad: 121878.9219 LR: 0.00001862 \n","Epoch: [3][2100/2283] Elapsed 66m 36s (remain 5m 46s) Loss: 0.0207(0.0163) Grad: 177380.1406 LR: 0.00001849 \n","Epoch: [3][2120/2283] Elapsed 67m 12s (remain 5m 7s) Loss: 0.0146(0.0163) Grad: 153840.2188 LR: 0.00001836 \n","Epoch: [3][2140/2283] Elapsed 67m 52s (remain 4m 30s) Loss: 0.0484(0.0163) Grad: 98904.1797 LR: 0.00001822 \n","Epoch: [3][2160/2283] Elapsed 68m 34s (remain 3m 52s) Loss: 0.0194(0.0163) Grad: 201823.6875 LR: 0.00001809 \n","Epoch: [3][2180/2283] Elapsed 69m 7s (remain 3m 13s) Loss: 0.0034(0.0163) Grad: 41823.7656 LR: 0.00001796 \n","Epoch: [3][2200/2283] Elapsed 69m 46s (remain 2m 35s) Loss: 0.0009(0.0162) Grad: 106184.4297 LR: 0.00001783 \n","Epoch: [3][2220/2283] Elapsed 70m 26s (remain 1m 57s) Loss: 0.0325(0.0162) Grad: 98416.5000 LR: 0.00001770 \n","Epoch: [3][2240/2283] Elapsed 70m 57s (remain 1m 19s) Loss: 0.0229(0.0162) Grad: 390764.0312 LR: 0.00001757 \n","Epoch: [3][2260/2283] Elapsed 71m 33s (remain 0m 41s) Loss: 0.0068(0.0162) Grad: 49291.9883 LR: 0.00001743 \n","Epoch: [3][2280/2283] Elapsed 72m 19s (remain 0m 3s) Loss: 0.0106(0.0162) Grad: 78925.5859 LR: 0.00001730 \n","Epoch: [3][2282/2283] Elapsed 72m 22s (remain 0m 0s) Loss: 0.0157(0.0161) Grad: 130418.5781 LR: 0.00001729 \n","EVAL: [0/49] Elapsed 0m 2s (remain 1m 44s) Loss: 0.1078(0.1078) \n","EVAL: [20/49] Elapsed 0m 44s (remain 0m 59s) Loss: 0.0964(0.1024) \n","EVAL: [40/49] Elapsed 1m 29s (remain 0m 17s) Loss: 0.0973(0.1027) \n"]},{"output_type":"stream","name":"stderr","text":["Epoch 3 - avg_train_loss: 0.0161  avg_val_loss: 0.1028  time: 4451s\n","INFO:__main__:Epoch 3 - avg_train_loss: 0.0161  avg_val_loss: 0.1028  time: 4451s\n","Epoch 3 - Score: 0.4536  Scores: [0.48852379142617874, 0.46346628803128415, 0.40550629548094663, 0.4571205414593978, 0.46311902336686317, 0.44376748304757735]\n","INFO:__main__:Epoch 3 - Score: 0.4536  Scores: [0.48852379142617874, 0.46346628803128415, 0.40550629548094663, 0.4571205414593978, 0.46311902336686317, 0.44376748304757735]\n"]},{"output_type":"stream","name":"stdout","text":["EVAL: [48/49] Elapsed 1m 48s (remain 0m 0s) Loss: 0.1168(0.1028) \n","Epoch: [4][0/2283] Elapsed 0m 1s (remain 50m 4s) Loss: 0.0529(0.0529) Grad: 56754.5117 LR: 0.00001728 \n","Epoch: [4][20/2283] Elapsed 0m 40s (remain 72m 57s) Loss: 0.0108(0.0095) Grad: 53837.6484 LR: 0.00001715 \n","Epoch: [4][40/2283] Elapsed 1m 14s (remain 67m 43s) Loss: 0.0169(0.0102) Grad: 38357.5742 LR: 0.00001702 \n","Epoch: [4][60/2283] Elapsed 1m 52s (remain 68m 16s) Loss: 0.0111(0.0117) Grad: 76140.6016 LR: 0.00001689 \n","Epoch: [4][80/2283] Elapsed 2m 32s (remain 69m 8s) Loss: 0.0161(0.0121) Grad: 52459.8047 LR: 0.00001676 \n","Epoch: [4][100/2283] Elapsed 3m 7s (remain 67m 39s) Loss: 0.0099(0.0126) Grad: 67546.1016 LR: 0.00001663 \n","Epoch: [4][120/2283] Elapsed 3m 44s (remain 66m 46s) Loss: 0.0148(0.0124) Grad: 21636.9082 LR: 0.00001650 \n","Epoch: [4][140/2283] Elapsed 4m 25s (remain 67m 11s) Loss: 0.0143(0.0130) Grad: 69288.8906 LR: 0.00001637 \n","Epoch: [4][160/2283] Elapsed 5m 2s (remain 66m 26s) Loss: 0.0152(0.0130) Grad: 54670.0000 LR: 0.00001624 \n","Epoch: [4][180/2283] Elapsed 5m 45s (remain 66m 51s) Loss: 0.0096(0.0132) Grad: 32376.0586 LR: 0.00001612 \n","Epoch: [4][200/2283] Elapsed 6m 22s (remain 66m 4s) Loss: 0.0315(0.0133) Grad: 282095.6250 LR: 0.00001599 \n","Epoch: [4][220/2283] Elapsed 7m 0s (remain 65m 26s) Loss: 0.0302(0.0136) Grad: 48054.8867 LR: 0.00001586 \n","Epoch: [4][240/2283] Elapsed 7m 43s (remain 65m 25s) Loss: 0.0083(0.0135) Grad: 24235.3457 LR: 0.00001573 \n","Epoch: [4][260/2283] Elapsed 8m 22s (remain 64m 56s) Loss: 0.0128(0.0134) Grad: 48280.8945 LR: 0.00001560 \n","Epoch: [4][280/2283] Elapsed 8m 57s (remain 63m 48s) Loss: 0.0073(0.0133) Grad: 26203.3613 LR: 0.00001548 \n","Epoch: [4][300/2283] Elapsed 9m 44s (remain 64m 8s) Loss: 0.0216(0.0136) Grad: 51725.9531 LR: 0.00001535 \n","Epoch: [4][320/2283] Elapsed 10m 21s (remain 63m 18s) Loss: 0.0098(0.0138) Grad: 29376.6680 LR: 0.00001522 \n","Epoch: [4][340/2283] Elapsed 10m 57s (remain 62m 26s) Loss: 0.0152(0.0137) Grad: 58334.4961 LR: 0.00001510 \n","Epoch: [4][360/2283] Elapsed 11m 41s (remain 62m 13s) Loss: 0.0123(0.0139) Grad: 34558.8945 LR: 0.00001497 \n","Epoch: [4][380/2283] Elapsed 12m 14s (remain 61m 7s) Loss: 0.0012(0.0139) Grad: 25795.8379 LR: 0.00001484 \n","Epoch: [4][400/2283] Elapsed 12m 50s (remain 60m 18s) Loss: 0.0820(0.0142) Grad: 102601.9766 LR: 0.00001472 \n","Epoch: [4][420/2283] Elapsed 13m 28s (remain 59m 36s) Loss: 0.0387(0.0144) Grad: 95771.5703 LR: 0.00001459 \n","Epoch: [4][440/2283] Elapsed 13m 57s (remain 58m 19s) Loss: 0.0144(0.0141) Grad: 88389.5859 LR: 0.00001447 \n","Epoch: [4][460/2283] Elapsed 14m 33s (remain 57m 34s) Loss: 0.0113(0.0140) Grad: 72957.8125 LR: 0.00001434 \n","Epoch: [4][480/2283] Elapsed 15m 4s (remain 56m 29s) Loss: 0.0097(0.0141) Grad: 29310.5938 LR: 0.00001422 \n","Epoch: [4][500/2283] Elapsed 15m 40s (remain 55m 46s) Loss: 0.0072(0.0140) Grad: 71696.3672 LR: 0.00001410 \n","Epoch: [4][520/2283] Elapsed 16m 14s (remain 54m 56s) Loss: 0.0235(0.0141) Grad: 66813.5000 LR: 0.00001397 \n","Epoch: [4][540/2283] Elapsed 16m 50s (remain 54m 13s) Loss: 0.0166(0.0142) Grad: 47143.0977 LR: 0.00001385 \n","Epoch: [4][560/2283] Elapsed 17m 21s (remain 53m 17s) Loss: 0.0234(0.0143) Grad: 102578.0547 LR: 0.00001373 \n","Epoch: [4][580/2283] Elapsed 18m 1s (remain 52m 47s) Loss: 0.0115(0.0143) Grad: 50193.5820 LR: 0.00001360 \n","Epoch: [4][600/2283] Elapsed 18m 32s (remain 51m 52s) Loss: 0.0223(0.0143) Grad: 64842.4102 LR: 0.00001348 \n","Epoch: [4][620/2283] Elapsed 19m 14s (remain 51m 29s) Loss: 0.0190(0.0144) Grad: 45453.9102 LR: 0.00001336 \n","Epoch: [4][640/2283] Elapsed 19m 51s (remain 50m 51s) Loss: 0.0076(0.0143) Grad: 24323.8633 LR: 0.00001324 \n","Epoch: [4][660/2283] Elapsed 20m 30s (remain 50m 20s) Loss: 0.0019(0.0142) Grad: 24093.5664 LR: 0.00001312 \n","Epoch: [4][680/2283] Elapsed 21m 13s (remain 49m 54s) Loss: 0.0103(0.0142) Grad: 33122.6445 LR: 0.00001300 \n","Epoch: [4][700/2283] Elapsed 21m 53s (remain 49m 24s) Loss: 0.0266(0.0142) Grad: 80367.3359 LR: 0.00001288 \n","Epoch: [4][720/2283] Elapsed 22m 25s (remain 48m 33s) Loss: 0.0078(0.0141) Grad: 33887.5859 LR: 0.00001276 \n","Epoch: [4][740/2283] Elapsed 23m 4s (remain 48m 0s) Loss: 0.0085(0.0141) Grad: 25911.6777 LR: 0.00001264 \n","Epoch: [4][760/2283] Elapsed 23m 35s (remain 47m 10s) Loss: 0.0099(0.0139) Grad: 49149.6602 LR: 0.00001252 \n","Epoch: [4][780/2283] Elapsed 24m 11s (remain 46m 31s) Loss: 0.0178(0.0139) Grad: 33016.9805 LR: 0.00001240 \n","Epoch: [4][800/2283] Elapsed 24m 49s (remain 45m 55s) Loss: 0.0007(0.0139) Grad: 18433.9805 LR: 0.00001228 \n","Epoch: [4][820/2283] Elapsed 25m 23s (remain 45m 13s) Loss: 0.0200(0.0140) Grad: 47098.6172 LR: 0.00001216 \n","Epoch: [4][840/2283] Elapsed 26m 9s (remain 44m 51s) Loss: 0.0046(0.0139) Grad: 52362.3320 LR: 0.00001204 \n","Epoch: [4][860/2283] Elapsed 26m 48s (remain 44m 16s) Loss: 0.0008(0.0139) Grad: 20346.3652 LR: 0.00001192 \n","Epoch: [4][880/2283] Elapsed 27m 26s (remain 43m 39s) Loss: 0.0049(0.0138) Grad: 40203.0859 LR: 0.00001181 \n","Epoch: [4][900/2283] Elapsed 28m 1s (remain 42m 59s) Loss: 0.0259(0.0138) Grad: 50399.5703 LR: 0.00001169 \n","Epoch: [4][920/2283] Elapsed 28m 32s (remain 42m 12s) Loss: 0.0006(0.0138) Grad: 12120.8730 LR: 0.00001157 \n","Epoch: [4][940/2283] Elapsed 29m 14s (remain 41m 42s) Loss: 0.0055(0.0138) Grad: 32626.9102 LR: 0.00001146 \n","Epoch: [4][960/2283] Elapsed 30m 2s (remain 41m 20s) Loss: 0.0048(0.0137) Grad: 20277.6992 LR: 0.00001134 \n","Epoch: [4][980/2283] Elapsed 30m 33s (remain 40m 33s) Loss: 0.0011(0.0137) Grad: 15165.3936 LR: 0.00001123 \n","Epoch: [4][1000/2283] Elapsed 31m 11s (remain 39m 57s) Loss: 0.0121(0.0136) Grad: 44567.2188 LR: 0.00001111 \n","Epoch: [4][1020/2283] Elapsed 31m 53s (remain 39m 25s) Loss: 0.0016(0.0136) Grad: 37169.0977 LR: 0.00001100 \n","Epoch: [4][1040/2283] Elapsed 32m 25s (remain 38m 41s) Loss: 0.0116(0.0135) Grad: 36056.2500 LR: 0.00001089 \n","Epoch: [4][1060/2283] Elapsed 33m 10s (remain 38m 12s) Loss: 0.0166(0.0136) Grad: 60893.1445 LR: 0.00001077 \n","Epoch: [4][1080/2283] Elapsed 33m 47s (remain 37m 34s) Loss: 0.0007(0.0136) Grad: 15348.1973 LR: 0.00001066 \n","Epoch: [4][1100/2283] Elapsed 34m 28s (remain 37m 1s) Loss: 0.0060(0.0136) Grad: 29503.9453 LR: 0.00001055 \n","Epoch: [4][1120/2283] Elapsed 35m 13s (remain 36m 30s) Loss: 0.0009(0.0137) Grad: 24248.0254 LR: 0.00001044 \n"]}],"source":["def get_result(oof_df, fold, best_train_loss, best_val_loss):\n","    labels = oof_df[CFG.target_cols].values\n","    preds = oof_df[[f\"pred_{c}\" for c in CFG.target_cols]].values\n","    score, scores = get_score(labels, preds)\n","    LOGGER.info(f'Score: {score:<.4f}  Scores: {scores}')\n","    _output_log = pd.DataFrame([CFG.identifier, CFG.model, CFG.cv_seed, CFG.seed, fold, 'best', score, best_train_loss, best_val_loss] + scores).T\n","    _output_log.columns = ['file', 'model', 'cv_seed', 'seed', 'fold', 'epoch', 'MCRMSE', 'train_loss', 'val_loss'] + CFG.target_cols\n","    return _output_log\n","\n","if CFG.train:\n","    output_log = pd.DataFrame()\n","    oof_df = pd.DataFrame()\n","    train_loss_list = []\n","    val_loss_list = []\n","    for fold in range(CFG.n_fold):\n","        if fold in CFG.trn_fold:\n","            best_train_loss, best_val_loss, _oof_df, df_epoch_scores = train_loop(folds = pseudo_folds[fold], org_folds = CFG.df_train, fold = fold)\n","            train_loss_list.append(best_train_loss)\n","            val_loss_list.append(best_val_loss)\n","            oof_df = pd.concat([oof_df, _oof_df])\n","            LOGGER.info(f\"========== fold: {fold} result ==========\")\n","\n","            df_epoch_scores['file'] = CFG.identifier\n","            df_epoch_scores['model'] = CFG.model\n","            df_epoch_scores['cv_seed'] = CFG.cv_seed\n","            df_epoch_scores['seed'] = CFG.seed\n","            df_epoch_scores['fold'] = fold\n","            df_epoch_scores = df_epoch_scores[['file', 'model', 'cv_seed', 'seed', 'fold', 'epoch', 'MCRMSE', 'train_loss', 'val_loss'] + CFG.target_cols]\n","\n","            _output_log = get_result(_oof_df, fold, best_train_loss, best_val_loss)\n","            output_log = pd.concat([output_log, df_epoch_scores, _output_log])\n","\n","    oof_df = oof_df.reset_index(drop=True)\n","    LOGGER.info(f\"========== CV ==========\")\n","    _output_log = get_result(oof_df, 'OOF', np.mean(train_loss_list), np.mean(val_loss_list))\n","    output_log = pd.concat([output_log, _output_log])\n","    output_log.to_csv(f'{CFG.identifier}.csv', index=False)\n","    oof_df.to_pickle(CFG.OUTPUT_DIR+'oof_df.pkl', protocol = 4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AlvppBKrW0mq"},"outputs":[],"source":["# epoch 7 / lr 5e-5 / len: max_len_of train / "]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[{"file_id":"1ZxIOspWAFVFIdpx64l0Jdf9Ips8OOTzv","timestamp":1669531918059}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7efbfe0d1cd34447bd0338531d37a586":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_613ec2c892b540659b20f9b92ee4f909","IPY_MODEL_8f3f39737324486d8eb85fe15f1db1f4","IPY_MODEL_aa9e0120539d40ee8e26a743a54221ff"],"layout":"IPY_MODEL_96c6272749d44b24a3212a248b063b85"}},"613ec2c892b540659b20f9b92ee4f909":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a12c98b5b0bc4877a00a37b5bd9b9e69","placeholder":"​","style":"IPY_MODEL_e0486a18d1c94f189501792e29e52801","value":"Downloading: 100%"}},"8f3f39737324486d8eb85fe15f1db1f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b49270a1543e40169d3f6f2b918028f7","max":52,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ad630bb6a83427383267b223745ec67","value":52}},"aa9e0120539d40ee8e26a743a54221ff":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd634d3456244396a2396c268855d447","placeholder":"​","style":"IPY_MODEL_b8ef5e33dbf44e5f829629a1dc6b588b","value":" 52.0/52.0 [00:00&lt;00:00, 1.44kB/s]"}},"96c6272749d44b24a3212a248b063b85":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a12c98b5b0bc4877a00a37b5bd9b9e69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0486a18d1c94f189501792e29e52801":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b49270a1543e40169d3f6f2b918028f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ad630bb6a83427383267b223745ec67":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd634d3456244396a2396c268855d447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b8ef5e33dbf44e5f829629a1dc6b588b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"156cd32e9b89448191b2780377b429ca":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bacaa87f83fe4da0a7bb72bd6890f171","IPY_MODEL_56c256a530e4496487738d7a0a6b464b","IPY_MODEL_9d06adad548f4eb3b1b4cabb0d7e895e"],"layout":"IPY_MODEL_f8459e2ec95a4553ba63e99957b2bc39"}},"bacaa87f83fe4da0a7bb72bd6890f171":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5ba438b26de4b749eafeb817ff2b760","placeholder":"​","style":"IPY_MODEL_47075a447a4f4ecca7d4ca54b71f4e3f","value":"Downloading: 100%"}},"56c256a530e4496487738d7a0a6b464b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f7ff21ebc584bb0a930367c8800c725","max":579,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89826e44350a4d06a29cfa550b47f4b8","value":579}},"9d06adad548f4eb3b1b4cabb0d7e895e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_069c4f984a17498eb7efdc2fe7fdad30","placeholder":"​","style":"IPY_MODEL_76a328f3820a44319869ef522d6800f8","value":" 579/579 [00:00&lt;00:00, 22.7kB/s]"}},"f8459e2ec95a4553ba63e99957b2bc39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a5ba438b26de4b749eafeb817ff2b760":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47075a447a4f4ecca7d4ca54b71f4e3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f7ff21ebc584bb0a930367c8800c725":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89826e44350a4d06a29cfa550b47f4b8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"069c4f984a17498eb7efdc2fe7fdad30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76a328f3820a44319869ef522d6800f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49bbfb5b8e9d41faaf615a74ddf8fd3d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_99cbf61819bc455589373aab8affc394","IPY_MODEL_1b023e98e7fa4f42abdd38d063df1c6a","IPY_MODEL_0b71660d22fe48c2ade053b405b29da7"],"layout":"IPY_MODEL_97368996b675473dac55d0782a80bef2"}},"99cbf61819bc455589373aab8affc394":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6c2fbb665b1b40b1b6760c576aceeb16","placeholder":"​","style":"IPY_MODEL_cbb30ef6f5284415bb1a73fce3ccba23","value":"Downloading: 100%"}},"1b023e98e7fa4f42abdd38d063df1c6a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cad992e8f4764cce89c7523a36f743c6","max":2464616,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9e8fc1669c02489eb3ab399da537b04d","value":2464616}},"0b71660d22fe48c2ade053b405b29da7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71f3d04483d143e197fb24060e276d04","placeholder":"​","style":"IPY_MODEL_a0f01b7b366c4b1bbdf3f8b2ac519191","value":" 2.35M/2.35M [00:00&lt;00:00, 33.9MB/s]"}},"97368996b675473dac55d0782a80bef2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c2fbb665b1b40b1b6760c576aceeb16":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbb30ef6f5284415bb1a73fce3ccba23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cad992e8f4764cce89c7523a36f743c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9e8fc1669c02489eb3ab399da537b04d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71f3d04483d143e197fb24060e276d04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0f01b7b366c4b1bbdf3f8b2ac519191":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe4372ac29c04d0dabdea7f0be95a6b0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_34c578370f204b8ca27415c2cc87b066","IPY_MODEL_d12fe568157e42dca0ca52be23d456c1","IPY_MODEL_41142207aca34cb380dca8a2158f1a19"],"layout":"IPY_MODEL_fee57ad07e95465182eb6724cfbd4731"}},"34c578370f204b8ca27415c2cc87b066":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2196d9c5df604a1198fd172d60016cdc","placeholder":"​","style":"IPY_MODEL_941db153172d4744b4c1757fa4c871ba","value":"100%"}},"d12fe568157e42dca0ca52be23d456c1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_49eb567e4fec41e1bc218074655bcd40","max":3911,"min":0,"orientation":"horizontal","style":"IPY_MODEL_75bb6db35a844cd295f0b5bf87e5fc87","value":3911}},"41142207aca34cb380dca8a2158f1a19":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e4f67557b2a54b84b9cce58bb18ce5a1","placeholder":"​","style":"IPY_MODEL_296049ceaf1347309036c56f87dc9490","value":" 3911/3911 [00:04&lt;00:00, 834.91it/s]"}},"fee57ad07e95465182eb6724cfbd4731":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2196d9c5df604a1198fd172d60016cdc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"941db153172d4744b4c1757fa4c871ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"49eb567e4fec41e1bc218074655bcd40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75bb6db35a844cd295f0b5bf87e5fc87":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e4f67557b2a54b84b9cce58bb18ce5a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"296049ceaf1347309036c56f87dc9490":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d42e8ebe3d6745bc95fdce7a47863020":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_727a9524d87546a7ac04533a907dc881","IPY_MODEL_17d24812066f439fad49a1723e918f26","IPY_MODEL_cb5e9ba57c2447bfa0e02c88f4e3d4fa"],"layout":"IPY_MODEL_73a6c566db9c4f2b842c0915890324f8"}},"727a9524d87546a7ac04533a907dc881":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5284b5f50a694ecc903e0b3bbf12731c","placeholder":"​","style":"IPY_MODEL_1f1da28d4f3b40359f2df04e1e2643fe","value":"Downloading: 100%"}},"17d24812066f439fad49a1723e918f26":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_882a8341586b4c85bb3ecec49a41e494","max":371146213,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bc76904ee1b04a3fba5782579ab4bf7f","value":371146213}},"cb5e9ba57c2447bfa0e02c88f4e3d4fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_07b09498b31d4c5f92b30ca4fd294d48","placeholder":"​","style":"IPY_MODEL_3a21254af5e64078853b337a241e133e","value":" 354M/354M [00:05&lt;00:00, 63.3MB/s]"}},"73a6c566db9c4f2b842c0915890324f8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5284b5f50a694ecc903e0b3bbf12731c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f1da28d4f3b40359f2df04e1e2643fe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"882a8341586b4c85bb3ecec49a41e494":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bc76904ee1b04a3fba5782579ab4bf7f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"07b09498b31d4c5f92b30ca4fd294d48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a21254af5e64078853b337a241e133e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}